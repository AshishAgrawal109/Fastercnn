{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representing text as vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"the cat sat on the mat\", \"the dog ate my homework\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ultimately our input data - the data which feeds into the model is a sentence, \n",
    "and our aim here is to represent this sentence on a word level or a character level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### one-hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get vocabulary\n",
    "index = 0\n",
    "vocab = dict()\n",
    "for sentence in sentences:\n",
    "    for word in sentence.split(\" \"):\n",
    "        if vocab.get(word) == None:\n",
    "            vocab[word] = index\n",
    "            index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 0,\n",
       " 'cat': 1,\n",
       " 'sat': 2,\n",
       " 'on': 3,\n",
       " 'mat': 4,\n",
       " 'dog': 5,\n",
       " 'ate': 6,\n",
       " 'my': 7,\n",
       " 'homework': 8}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 1, 0, 0, 0, 0]])\n",
      "torch.Size([6, 9])\n",
      "\n",
      "tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 1]])\n",
      "torch.Size([5, 9])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentences:\n",
    "    sentence = sentence.split()\n",
    "    sentence_length = len(sentence)\n",
    "    indices = torch.tensor([vocab[word] for word in sentence], dtype=torch.int64)\n",
    "    indices.unsqueeze_(1)\n",
    "    sentence_matrix = torch.zeros(sentence_length, vector_size, dtype=torch.int64)\n",
    "    sentence_matrix.scatter_(1, indices, 1)\n",
    "    \n",
    "    print(sentence_matrix)\n",
    "    print(sentence_matrix.shape)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Batching the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "         [0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "         [0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
      "         [0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
      "         [1, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "\n",
      "        [[1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "         [0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
      "         [0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0, 1]]])\n",
      "torch.Size([2, 5, 9])\n"
     ]
    }
   ],
   "source": [
    "# generally all the sentences should be of same length to batch the data\n",
    "max_sentence_length = 5 # i.e 5 words per sentence at max\n",
    "result = torch.zeros((len(sentences), max_sentence_length, vector_size), # no. of sentences,\n",
    "                    dtype=torch.int64)                                   # no. of words in a sentence\n",
    "                                                                         # size of the word vector\n",
    "                                                                       \n",
    "        \n",
    "for i, sentence in enumerate(sentences):\n",
    "    sentence = sentence.split()\n",
    "    sentence = sentence[:max_sentence_length] # clipping the sentence to max_sentence_length\n",
    "    for j, word in enumerate(sentence):\n",
    "        index = vocab.get(word)\n",
    "        result[i, j, index] = 1\n",
    "        \n",
    "\n",
    "print(result)\n",
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Character level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]]])\n",
      "torch.Size([2, 50, 128])\n"
     ]
    }
   ],
   "source": [
    "max_sentence_length = 50 # i.e no more than 50 charcters per sentence\n",
    "vector_size = 128 # ascii characters\n",
    "result = torch.zeros((len(sentences), max_sentence_length, vector_size), # no. of sentences,\n",
    "                     dtype=torch.int64)                                  # no. of characters in each sentence\n",
    "                                                                         # size of each character vector\n",
    "    \n",
    "for i, sentence in enumerate(sentences):\n",
    "    for j, character in enumerate(sentence):\n",
    "        if character not in string.printable:\n",
    "            continue\n",
    "        index = ord(character)\n",
    "        result[i, j, index] = 1\n",
    "        \n",
    "print(result)\n",
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Word Level one hot hashing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we don't create the vocabulary in the beginning.\n",
    "We run every word through a hash function and map it to a number between 1 and dimensionality.\n",
    "but the problem here is that every word will not have a unique hash value which might confuse the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]]])\n",
      "torch.Size([2, 10, 1000])\n"
     ]
    }
   ],
   "source": [
    "max_sentence_length = 10 # maximum of 10 words per sentence\n",
    "dimensionality = 1000 # size of word vector\n",
    "\n",
    "result = torch.zeros((len(sentences), max_sentence_length, dimensionality), # no. of sentences,\n",
    "                    dtype=torch.int64)                                      # no. of words in a sentence\n",
    "                                                                            # size of the word vector\n",
    "                                                                       \n",
    "        \n",
    "for i, sentence in enumerate(sentences):\n",
    "    sentence = sentence.split()\n",
    "    sentence = sentence[:max_sentence_length] # clipping the sentence to max_sentence_length\n",
    "    for j, word in enumerate(sentence):\n",
    "        index = abs(hash(word)) % dimensionality\n",
    "        result[i, j, index] = 1\n",
    "        \n",
    "print(result)\n",
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Embedding layer is just like a normal linear layer whose weights are updated during training,\n",
    "except that the weights represent word vectors and can be accessed via index of the word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = nn.Embedding(10, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 128])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get vector of word whose index is 1\n",
    "embedding_layer(torch.tensor([1])).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### N-Gram language modeling"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Here, given a context or history of two previous words, we try to predict the next word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_SIZE = 2\n",
    "EMBEDDING_DIM = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use Shakespeare Sonnet 2\n",
    "test_sentence = \"\"\"When forty winters shall besiege thy brow,\n",
    "And dig deep trenches in thy beauty's field,\n",
    "Thy youth's proud livery so gazed on now,\n",
    "Will be a totter'd weed of small worth held:\n",
    "Then being asked, where all thy beauty lies,\n",
    "Where all the treasure of thy lusty days;\n",
    "To say, within thine own deep sunken eyes,\n",
    "Were an all-eating shame, and thriftless praise.\n",
    "How much more praise deserv'd thy beauty's use,\n",
    "If thou couldst answer 'This fair child of mine\n",
    "Shall sum my count, and make my old excuse,'\n",
    "Proving his beauty by succession thine!\n",
    "This were to be new made when thou art old,\n",
    "And see thy blood warm when thou feel'st it cold.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = WordPunctTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = tokenizer.tokenize(test_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams = [([words[i], words[i+1]], words[i+2]) for i in range(len(words)-2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['When', 'forty'], 'winters'),\n",
       " (['forty', 'winters'], 'shall'),\n",
       " (['winters', 'shall'], 'besiege'),\n",
       " (['shall', 'besiege'], 'thy'),\n",
       " (['besiege', 'thy'], 'brow'),\n",
       " (['thy', 'brow'], ',')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigrams[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = {word: i for i, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Building the model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "If you think about it, it's just a category prediction problem where the number of categories = vocab size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramLanguageModeler(nn.Module):\n",
    "    def __init__(self, CONTEXT_SIZE, EMBEDDING_DIM, VOCAB_SIZE):\n",
    "        super().__init__()\n",
    "        self.CONTEXT_SIZE = CONTEXT_SIZE\n",
    "        self.EMBEDDING_DIM = EMBEDDING_DIM\n",
    "        self.embeddings = nn.Embedding(VOCAB_SIZE, EMBEDDING_DIM)\n",
    "        self.linear1 = nn.Linear(CONTEXT_SIZE*EMBEDDING_DIM, 128)\n",
    "        self.linear2 = nn.Linear(128, VOCAB_SIZE)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        if inputs.shape[0] != self.CONTEXT_SIZE:\n",
    "            raise ValueError(f\"The context size should be of size {self.CONTEXT_SIZE}\")\n",
    "        word_vectors = self.embeddings(inputs)\n",
    "        # we flatten the inputs before passing onto the linear layer\n",
    "        word_vectors = word_vectors.view(-1, self.CONTEXT_SIZE * self.EMBEDDING_DIM)\n",
    "        out = F.relu(self.linear1(word_vectors))\n",
    "        logits = self.linear2(out)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NGramLanguageModeler(CONTEXT_SIZE, EMBEDDING_DIM, VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_list = []\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50/50 [00:14<00:00,  3.53it/s]\n"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(n_epochs)):\n",
    "    loss_per_epoch = 0\n",
    "    for context, target in trigrams:\n",
    "        context_indices = torch.tensor([word2idx[word] for word in context], dtype=torch.int64)\n",
    "        out = model(context_indices)\n",
    "        loss = loss_fn(out, torch.tensor([word2idx[target]], dtype=torch.int64))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_per_epoch += loss.item()\n",
    "    loss_list.append(loss_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1fc294c9df0>]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAms0lEQVR4nO3dd3hUVf7H8fc3CQkQaiSwSAcpIt2AlNAEaYsK2FBXXRsiSFH8rbrV3XV11xUQG4rYFRRYQYqgiBJApCTSm3SJtCAiCtK/vz8y7AZFCaTMZObzep48c++5ZybfIw8frmfOvdfcHRERCS9RwS5ARERyn8JdRCQMKdxFRMKQwl1EJAwp3EVEwlBMsAsAKFOmjFetWjXYZYiIFChpaWl73D3xdMdCItyrVq1KampqsMsQESlQzGzrzx3TtIyISBhSuIuIhKFsTcuYWSlgNFAPcOA2YDBQO9ClFLDP3RuZWVVgDbAucGyBu/fNtYpFROSMsjvnPgKY4e5Xm1ksUNTdrzt50MyGAt9m6b/R3RvlXpkiInI2zhjuZlYCaAP8FsDdjwBHshw34Frg0rwpUUREzlZ25tyrAxnAK2a2xMxGm1l8luOtgV3uvj5LW7VA3xQza326DzWzPmaWamapGRkZ5z4CERH5ieyEewzQBBjp7o2BA8CDWY5fD4zNsr8DqBzoex8wJnD2fwp3H+XuSe6elJh42mWaIiJyjrIT7ulAursvDOxPIDPsMbMYoBfwzsnO7n7Y3b8ObKcBG4FauVn0SYeOHufhyavYe+DImTuLiESQM4a7u+8EtpnZyZUxHYDVge2OwFp3Tz/Z38wSzSw6sF0dqAlsytWqA5anf8uYRV9y+dPzWLX92zO/QUQkQmR3nfsA4C0zWw40Ah4NtPfm1CkZyPzydbmZLSPzLL+vu+/NhVp/olm1BMbf1YLjJ5yrRs5nyrLtefFrREQKHAuFJzElJSV5Tm4/sPu7Q/R783NSt35D37Y1+L/OtYmOslysUEQk9JhZmrsnne5YWFyhWrZ4Ycbc2ZwbLqnM8ykbue3VxXx78GiwyxIRCZqwCHeA2JgoHu1Zn3/0rMf8jXu48tl5rN/1XbDLEhEJirAJ95NuvKQKY+5szveHj9Pj2U+ZsXJHsEsSEcl3YRfuAE2rJjB1QDI1yxWn75uf8/iMtRw/EfzvFkRE8ktYhjvAr0oW5p27mnN9s0o8N3sjt766mH0HtR5eRCJD2IY7QFxMNI/1asCjPevz2cY9XPHMp6zZsT/YZYmI5LmwDveTbrikMm/3acHhY8fp9dx8Jms9vIiEuYgId4CLq5RmyoBk6lUowcCxS/j71NUcPX4i2GWJiOSJiAl3+N96+N+2rMpL8zZz4+iFZHx3ONhliYjkuogKd4BC0VE8fMVFPHldI5an76P703NJ25ond0cQEQmaiAv3k3o0rsC7d7ciLiaa3qMW8PpnWwiFWzGIiOSGiA13gLrnl2DKPckkX1CGP7+3iiHjlvHDkePBLktEJMciOtwBShYtxEu3NGVwx5pMXPoVPZ/7lC17DgS7LBGRHIn4cAeIijIGd6zFy79tyo5vD3H50/P4cNXOYJclInLOFO5ZtK9dlqkDkqlaJp4+b6TxrxlrOablkiJSACncf6RSQlHG923B9c0qM3L2Rm5+eRF7vtdySREpWBTup1G4UDSP9arPv69uQNrWb+j+1DwtlxSRAkXh/guuSarEu/1aEhsTxXUvLOCleZu1XFJECgSF+xlcdH5JpgxIpn2dsvx96mr6j/mc7w7pKU8iEtqyFe5mVsrMJpjZWjNbY2YtzOxhM/vKzJYGfrpl6f+QmW0ws3Vm1jnvys8fJYsUYtRNF/P7bnX4YNUurnjmU9bu1N0lRSR0ZffMfQQww93rAA2BNYH24e7eKPDzPoCZ1QV6AxcBXYDnzCw6l+vOd2ZGnzY1GHtncw4cPkaPZz9lQlp6sMsSETmtM4a7mZUA2gAvAbj7EXff9wtvuRJ4290Pu/tmYAPQLBdqDQnNqiUwdWAyjSuV5v7xy3hgwnIOHdVVrSISWrJz5l4dyABeMbMlZjbazOIDx+4xs+Vm9rKZlQ60VQC2ZXl/eqDtFGbWx8xSzSw1IyMjJ2PId2WLF+aN25txT/sLeCd1Gz2e/ZRNGd8HuywRkf/KTrjHAE2Ake7eGDgAPAiMBGoAjYAdwNBAfzvNZ/xkiYm7j3L3JHdPSkxMPIfSgysmOor7O9fm1Vubsmt/5lWtU/QQEBEJEdkJ93Qg3d0XBvYnAE3cfZe7H3f3E8CL/G/qJR2olOX9FYGwTb12tcsybWBr6pQvwYCxS/jjpBWaphGRoDtjuLv7TmCbmdUONHUAVptZ+SzdegIrA9uTgd5mFmdm1YCawKJcrDnknF+qCG/3aU6fNtV5c8GXXP38fLZ+rZuPiUjwZHe1zADgLTNbTuY0zKPA42a2ItDWHrgXwN1XAeOA1cAMoL+7h/2pbKHoKH7f7UJevDmJL78+SPen5vH+ih3BLktEIpSFwhWXSUlJnpqaGuwycs22vQe5Z+wSlm3bx80tqvD7bhdSuFCBXw0qIiHGzNLcPel0x3SFah6olFCU8Xe14I7karz+2VauGjlf94gXkXylcM8jsTFR/LF7XV68OYn0b36g+9PzmLo8bL9XFpEQo3DPY5fVLce0gcnULFeMe8ZoNY2I5A+Fez6oWLoo4+5qwV2B1TQ9nv2UDbt10ZOI5B2Fez4pFB3FQ90u5JVbm7L7u8Nc/vQ83ZtGRPKMwj2fta9dlumDWtOwUknuH7+M+95ZyoHDx4JdloiEGYV7EJQrUZi37mjO4I41mbT0Ky5/eh6rtn8b7LJEJIwo3IMkOsoY3LEWY+5szoEjx+j53Hxem79FT3oSkVyhcA+y5tXPY/qgNrS+oAx/mbyKO19P45sDR4JdlogUcAr3EJAQH8voW5L4c/e6zPkig64j5rJg09fBLktECjCFe4gwM25Lrsa7/VpSJDaaG15cwLCZX3Ds+IlglyYiBZDCPcTUq1CSqQOS6dm4Ik/NWs/1Ly4g/ZuDwS5LRAoYhXsIio+LYei1DXnyukas2fEdXUfMZdpy3WFSRLJP4R7CejSuwLSByVRPLEb/MZ/zuwnLtCZeRLJF4R7iqpwXz4S+Lejfvgbj09K5/Ol5rPxKa+JF5Jcp3AuAQtFR/F/nOoy5ozkHjxyn53OfMmrORk6c0Jp4ETk9hXsB0qLGeUwf1JpL65Tl0ffXcvPLi9i1/1CwyxKREKRwL2BKx8fy/G8u5tGe9Unb+g2dn5zDB6t2BrssEQkxCvcCyMy44ZLKTB2YTKXSRbnrjTQeencFB4/oy1YRyZStcDezUmY2wczWmtkaM2thZv8O7C83s4lmVirQt6qZ/WBmSwM/z+fpCCJYjcRi/OfultzdrgZvL/6S7k/NY0W6vmwVkeyfuY8AZrh7HaAhsAaYCdRz9wbAF8BDWfpvdPdGgZ++uVqxnCI2JooHumR+2frD0cwvW5+bvYHj+rJVJKKdMdzNrATQBngJwN2PuPs+d//Q3U/OAywAKuZdmXImLWqcx4xBbeh80a94fMY6rh+1gG17dWWrSKTKzpl7dSADeMXMlpjZaDOL/1Gf24DpWfarBfqmmFnr032omfUxs1QzS83IyDi36uUUJYsW4pkbGjPs2oas3rGfbiPm8u7n6bqNsEgEyk64xwBNgJHu3hg4ADx48qCZ/QE4BrwVaNoBVA70vQ8YEzj7P4W7j3L3JHdPSkxMzOEw5CQzo1eTikwf1Jo65Ytz37hl3DN2CfsO6jbCIpEkO+GeDqS7+8LA/gQywx4zuwXoDtzogdNDdz/s7l8HttOAjUCt3C5cflmlhKK83acFv+tSmw9W7qTLk3P5dMOeYJclIvnkjOHu7juBbWZWO9DUAVhtZl2AB4Ar3P2/k7tmlmhm0YHt6kBNYFOuVy5nFB1l9Gt3ARP7tSI+LpobRy/kr1NWcejo8WCXJiJ5LCab/QYAb5lZLJlBfSuwGIgDZpoZwILAypg2wN/M7BhwHOjr7ntzvXLJtvoVSzJ1QGv+OX0Nr3y6hbnr9/DkdY2oV6FksEsTkTxiofBlW1JSkqempga7jIgwd30G949fxtffH2Fwx5r0bVuDmGhdyyZSEJlZmrsnne6Y/lZHmNY1E/lgcBu61i/PEx9+wTUvfMaWPQeCXZaI5DKFewQqVTSWp69vzIjejdi4+3u6jpjLmwu2asmkSBhRuEewKxtV4IN725BUtTR/nLSSW15ZzM5vdZdJkXCgcI9w5UsW4fXbmvH3HvVYvHkvnYanMGnJVzqLFyngFO6CmXFT8ypMH9SamuWKM/idpfQf8zl7D+jCJ5GCSuEu/1W1TDzj7mrBA13q8NHq3XQaPoeZq3cFuywROQcKdzlFdJRxd7saTB7QisTicdz5eir3jVvKtz8cDXZpInIWFO5yWnV+VYL3+rdi4KUX8N7S7XQePofZ63YHuywRySaFu/ys2Jgo7utUm4n9WlK8cAy/fWUxD727nO8P64lPIqFO4S5n1KBiKaYMSOauttV5Z/E2Og+fw3zdhEwkpCncJVsKF4rmoa4XMr5vS2Jjorhh9EL+OGkFB3QWLxKSFO5yVi6uUpr3B7bm9uRqvLXwSzo/OYf5G3UWLxJqFO5y1orERvOn7nUZd1cLYqKMG15cyJ8mrdRZvEgIUbjLOWtaNYHpg9pwW6tqvLlwq87iRUKIwl1ypEhsNH++/NSz+D9MXKEVNSJBpnCXXHHyLP6O5GqMWfQlnYfPYc4XevC5SLAo3CXXFImN5o/d6zKhb0sKF4ri5pcX8cCE5ew/pKtbRfKbwl1y3cVVSjNtYGv6tq3B+LRtdBo2h0/W6upWkfyUrXA3s1JmNsHM1prZGjNrYWYJZjbTzNYHXktn6f+QmW0ws3Vm1jnvypdQVbhQNA92rcPEfq0oUSSGW19dzL3vLOUb3WlSJF9k98x9BDDD3esADYE1wIPALHevCcwK7GNmdYHewEVAF+A5M4vO7cKlYGhYKfPq1oEdajJl2XYuG57CtOU7dL94kTx2xnA3sxJAG+AlAHc/4u77gCuB1wLdXgN6BLavBN5298PuvhnYADTL3bKlIImLiea+y2oxZUAy5UsWof+Yz+n7Zhq79+upTyJ5JTtn7tWBDOAVM1tiZqPNLB4o5+47AAKvZQP9KwDbsrw/PdAmEe7C8iWY2K8lD3atw+x1GXQclsK41G06ixfJA9kJ9xigCTDS3RsDBwhMwfwMO03bT/72mlkfM0s1s9SMDC2ZixQx0VH0bVuD6YNaU+dXJfjdhOXc/PIitu09GOzSRMJKdsI9HUh394WB/Qlkhv0uMysPEHjdnaV/pSzvrwhs//GHuvsod09y96TExMRzrV8KqOqJxXi7T3P+3qMeS77cR6fhcxg9dxPHT+gsXiQ3nDHc3X0nsM3MageaOgCrgcnALYG2W4D3AtuTgd5mFmdm1YCawKJcrVrCQlRU5rNbP7y3DS1rnMcj09bQa+R81u7cH+zSRAo8y858p5k1AkYDscAm4FYy/2EYB1QGvgSucfe9gf5/AG4DjgGD3X36L31+UlKSp6amnvsopMBzd6Ys38FfJ6/i2x+O0q9dDfpfegFxMVpoJfJzzCzN3ZNOeywUvsxSuMtJew8c4ZGpq3l3yVfUSIznn1c1oGnVhGCXJRKSfincdYWqhJSE+FiGXdeI125rxqGjJ7jm+c/4w8QVuoWByFlSuEtIalsrkQ/vbcPtydUYu+hLLhuWwoyVO4NdlkiBoXCXkBUfF8OfutdlYr9WJMTH0ffNNO56I5VduvhJ5IwU7hLyGlYqxeR7WvG7LrUzL34amsIbC7ZyQssmRX6Wwl0KhELRUfRrdwEfDG5Dg0ol+dOklVzzwmd8seu7YJcmEpIU7lKgVC0Tz5u3X8LQaxqyKeN7fv3UXIZ+uI5DR48HuzSRkKJwlwLHzLjq4orMGtKOyxuez9Mfb6DriLl6fqtIFgp3KbAS4mMZdm0j3rz9Eo6fcG54cSH3j1/GXt0zXkThLgVfcs0yfHhvG/q1q8GkJV/RYehsJqSl626TEtEU7hIWCheK5ndd6jBtYGuqJxbj/vHLuOHFhWzK+D7YpYkEhcJdwkrtXxVn/F0t+EfPeqzc/i1dRsxlxEfrOXxMX7hKZFG4S9iJijJuvKQKs4a0pVPdcgz/6Au6jpjLZxu/DnZpIvlG4S5hq2zxwjxzQxNevbUpR4+f4PoXF3DfuKV8/f3hYJcmkucU7hL22tUuy4eD29K/fQ2mLNtOh2EpvLP4S13hKmFN4S4RoUhsNP/XuQ7vD2xNrbLFeeA/K7hu1Ges26krXCU8KdwlotQsV5y3+zTn8asasH535hWuj01fw8Ejx4JdmkiuUrhLxImKMq5tWomPh7SjV5MKvJCyicuGzeGj1buCXZpIrlG4S8RKiI/l8asbMr5vC+Ljornj9VT6vJ7KV/t+CHZpIjmmcJeI17RqAtMGtubBrnWYu34PHYem8ELKRo4ePxHs0kTOWbbC3cy2mNkKM1tqZqmBtncC+0sDx5cG2qua2Q9Zjj2fh/WL5IpC0VH0bVuDmfe1odUFZXhs+lp+/dRcFm7S2ngpmGLOom97d//vbffc/bqT22Y2FPg2S9+N7t4o5+WJ5K+KpYsy+pYkZq7excOTV3HdqAVc1aQiD3WrQ5liccEuTyTbcjwtY2YGXAuMzXk5IqHhsrrlmHlf5s3IJi/7ig5DU3hroZ7+JAVHdsPdgQ/NLM3M+vzoWGtgl7uvz9JWzcyWmFmKmbU+3QeaWR8zSzWz1IyMjHMoXSRvFY2N4Xdd6jB9UGsuLF+cP0xcSc+R81mR/u2Z3ywSZJad26Ka2fnuvt3MygIzgQHuPidwbCSwwd2HBvbjgGLu/rWZXQxMAi5y9/0/9/lJSUmempqa89GI5BF3Z9LSr/jHtLXsPXCY3zSvwpBOtSlZpFCwS5MIZmZp7p50umPZOnN39+2B193ARKBZ4INjgF7AO1n6Hnb3rwPbacBGoFZOBiASbGZGz8YVmTWkLTe3qMqbC7bSYehsJi7RfeMlNJ0x3M0s3syKn9wGOgErA4c7AmvdPT1L/0Qziw5sVwdqAptyu3CRYChZpBAPX3ERk+9JpmLpotz7zjJ6j1qgB3VLyMnOmXs5YJ6ZLQMWAdPcfUbgWG9++kVqG2B5oP8EoK+7782tgkVCQb0KJXn37pY81qs+63Z9R7cRc3n0/TV8f1i3MZDQkK0597ymOXcpyPYeOMLjM9by9uJtlCsRxx9/XZfuDcqTuZBMJO/keM5dRH5eQnws/7yqAe/2a0mZYnEMGLuEm15axIbdesSfBI/CXSSXNKlcmsn3JPP3Ky9iefo+uo6Yw79mrNUdJyUoFO4iuSg6yripRVU+vr8dVzSswMjZG+k4NIX3V+zQqhrJVwp3kTxQplgcQ69tyIS+LShZNJZ+b33OzS8vYmOGpmokfyjcRfJQUtUEptzTiocvr8vSbfvo8uQcHtdUjeQDhbtIHouJjuK3rarx8ZDMqZrnAlM10zVVI3lI4S6STxKLZ07VjO/bghJFCnG3pmokDyncRfJZ06oJTB2QfMpUzT+nr+WALoCSXKRwFwmCH0/VPJ+ykY7DUpi2XFM1kjsU7iJBdHKq5j93t6B00Vj6j/mc37y0kA27da8ayRmFu0gIuLhKAlMGJPO3Ky9iRfq3dHlS96qRnFG4i4SI6Cjj5hZV+eT+dlzVpCKj5myiw9DZvLf0K03VyFlTuIuEmPOKxfGvqxswsV9LyhYvzKC3l9J71ALW7dRUjWSfwl0kRDWuXJpJ/Vvxj571Mm8r/NRc/jplFfsPHQ12aVIAKNxFQlh0lHHjJVX4ZEg7rmtaiVfnb+HSJ2YzIS1dD+uWX6RwFykASsfH8mjP+rzXvxUVSxfl/vHLuOaFz1i1XQ/rltNTuIsUIA0qluLdu1vy+NUN2LLnAJc/PY8/TVrJvoNHgl2ahBiFu0gBExVlXJtUiY+HtOOm5lV4a+FW2j8xm7GLvtRUjfyXwl2kgCpZtBB/vbIeUwe0pmbZ4jz07gp6PvcpS7ftC3ZpEgKyFe5mtsXMVpjZUjNLDbQ9bGZfBdqWmlm3LP0fMrMNZrbOzDrnVfEiAnXPL8E7dzVnRO9G7Pj2ED2e/ZQHJizn6+8PB7s0CaKYs+jb3t33/KhtuLs/kbXBzOoCvYGLgPOBj8yslrsfz1mpIvJzzIwrG1Wgw4XleGrWel6et5npK3cwpFNtbrykMjHR+p/0SJMXf+JXAm+7+2F33wxsAJrlwe8RkR8pFhfD77tdyIzBrWlQsRR/mbyK7k/PY9HmvcEuTfJZdsPdgQ/NLM3M+mRpv8fMlpvZy2ZWOtBWAdiWpU96oO0UZtbHzFLNLDUjI+OciheR07ugbHHeuL0ZI29swv4fjnLtC58x6O0l7Np/KNilST7Jbri3cvcmQFegv5m1AUYCNYBGwA5gaKCvneb9P/kK391HuXuSuyclJiaedeEi8svMjK71yzNrSDsGXHoB01fu5NInZvNCykaOHDsR7PIkj2Ur3N19e+B1NzARaObuu9z9uLufAF7kf1Mv6UClLG+vCGzPvZJF5GwUiY1mSKfazLy3DS1qnMdj09fSZcQc5q7X/zGHszOGu5nFm1nxk9tAJ2ClmZXP0q0nsDKwPRnobWZxZlYNqAksyt2yReRsVTkvntG3NOXl3yZx/IRz00uL6PtGGunfHAx2aZIHsrNaphww0cxO9h/j7jPM7A0za0TmlMsW4C4Ad19lZuOA1cAxoL9WyoiEjkvrlKNljTK8NG8zT3+8nk+G7qZ/+wvo06Y6hQtFB7s8ySUWCveJTkpK8tTU1GCXIRJxvtr3A49OW8O0FTuonFCUP3evS4cLyxI4mZMQZ2Zp7p50umNa/CoSwSqUKsKzNzZhzB2XEBcTxR2vp3Lbq4vZvOdAsEuTHFK4iwgtLyjD+4Na88dfX8jiLd/QefgcHp+xlgN6zF+BpXAXEQAKRUdxR+vqfHx/W7o3LM9zszfSYWgKU5Zt12P+CiCFu4icomzxwgy7thET+rYgIT6WAWOXcP2LesxfQaNwF5HTSqqawJQByfy9Rz3W7Mh8zN/fpqzWY/4KCIW7iPys6CjjpuZV+OT+zMf8vTJ/M5c+MZvxqdt07/gQp3AXkTNKCDzmb3L/ZColFOX/JiznqufnsyJdj/kLVQp3Ecm2+hVL8p++LXnimoZs23uQK56dx+8nruCbA3rMX6hRuIvIWYmKMq6+uCIf39+O21pV453F22g/dDZvLtjKcU3VhAyFu4ickxKFC/Gn7nWZPqg1F/6qBH+ctJIrnplH2lbdOz4UKNxFJEdqlSvOmDsv4ZkbGvP190e4auRnDBm3jN3f6d7xwaRwF5EcMzO6NzifWUPacne7Gkxe9hUdnkhh9NxNHD2ue8cHg8JdRHJNfFwMD3SpwweD29CkSmkembaGbiPmMn/jjx+/LHlN4S4iua56YjFevbUpL96cxKFjx7nhxYX0H/M52/f9EOzSIobCXUTyhJlxWd1yzLy3Lfd2rMVHq3fRYWgKz36ygcPH9IiHvKZwF5E8VbhQNIM61uSj+9rSumYZ/v3BOro8OZfZ63YHu7SwpnAXkXxRKaEoo25O4rXbMh+3/NtXFnPn66ls26vH/OUFhbuI5Ku2tRKZMbg1D3Spw6cb9tBxWArDZ37BoaOaqslNCncRyXdxMdHc3a4Gs4a05bK65Rgxaz2XDU9h5updund8LslWuJvZFjNbYWZLzSw10PZvM1trZsvNbKKZlQq0VzWzHwJ9l5rZ83lYv4gUYOVLFuGZG5ow5s5LKBwTzZ2vp3Lrq4vZosf85djZnLm3d/dGWR7GOhOo5+4NgC+Ah7L03Rjo28jd++ZWsSISnlrW+N9j/lK3fEOn4XN44oN1HDyix/ydq3OelnH3D9395H/5BUDF3ClJRCLRKY/5a1CeZz7ZQMehKUxfsUNTNecgu+HuwIdmlmZmfU5z/DZgepb9ama2xMxSzKz16T7QzPqYWaqZpWZkZJxl2SISrsoWL8yw6xoxvm8LShaN5e63PufmlxexYff3wS6tQLHs/ItoZue7+3YzK0vmdMwAd58TOPYHIAno5e5uZnFAMXf/2swuBiYBF7n7/p/7/KSkJE9NTc2F4YhIODl2/ARjFn0ZmKI5zu3J1RjQoSbF4mKCXVpIMLO0LFPlp8jWmbu7bw+87gYmAs0CH3wL0B240QP/Srj7YXf/OrCdBmwEauV0ECISeWKio7i5RVU+vr8dvZpU4IU5m+gwdDaTl23XVM0ZnDHczSzezIqf3AY6ASvNrAvwAHCFux/M0j/RzKID29WBmsCmvCheRCJDmWJxPH51Q97t15LE4nEMHLuE619cwBe7vgt2aSErO2fu5YB5ZrYMWARMc/cZwDNAcWDmj5Y8tgGWB/pPAPq6u+7eLyI51qRyad7rn8wjPeqxdud3dB0xl0emrua7Q0eDXVrIydace17TnLuInK29B47w7w/W8fbiLylTLI7fd6tDj0YVMLNgl5ZvcjznLiISahLiY3msV30m9WvF+SULc+87y7juhQWs2fGzazciisJdRAq0hpVKMbFfK/7Zqz7rd39H96fn8dcpq9gf4VM1CncRKfCioozezSrzyf3t6N20Eq/O38KlT6Twn7T0iF1Vo3AXkbBRqmgs/+hZn8n9k6lYughDxi/j2hc+Y/X2yJuqUbiLSNipX7Ek797dkn9dVZ+NGQfo/vRcHp68im9/iJypGoW7iISlqCjjuqaV+XhIW264pDKvfbaFDkNnR8xUjcJdRMJaqaKxPNKjPlPuSaZSQtGImapRuItIRKhXoST/6duSx69qcMpUTbiuqlG4i0jEiIoyrm1aiY+HtOXGS6rw2mfhu6pG4S4iEadU0Vj+3qNeYKrmf6tqwukCKIW7iESsn07VzONvU1aHxVSNwl1EIlrWqZrrm1Xilfmb6TA0hUlLvirQUzUKdxER/req5r3+rTi/VBEGv7OU60YtYN3OgnlbYYW7iEgWDSqWYuLdLXmsV32+2PUd3Z7KvK3w94cL1sO6Fe4iIj8SFWVc36wynwxpx7VJFXnp080F7glQCncRkZ9ROj6Wx3o14N27W1K2eGEGjl3CjaMXsmF36E/VKNxFRM6gceXSTOrfikd61GPV9v10eXIuj01fw4EQnqpRuIuIZEN0lPGb5lX4eEhbejauwAspm7hsWArTV+wIyamabIW7mW0xsxWBZ6WmBtoSzGymma0PvJbO0v8hM9tgZuvMrHNeFS8ikt/OKxbHv69pyIS+LShRpBB3v/U5t7yymM17DgS7tFOczZl7e3dvlOV5fQ8Cs9y9JjArsI+Z1QV6AxcBXYDnzCw6F2sWEQm6pKoJTB2QzJ+71+Xzrd/Qefgchn24jkNHjwe7NCBn0zJXAq8Ftl8DemRpf9vdD7v7ZmAD0CwHv0dEJCTFREdxW3I1Ph7Slm71f8VTH2/gsuEpzFqzK9ilZTvcHfjQzNLMrE+grZy77wAIvJYNtFcAtmV5b3qgTUQkLJUtUZgnezdm7J3NiYuJ5vbXUrnz9VTSvzkYtJqyG+6t3L0J0BXob2ZtfqGvnabtJ982mFkfM0s1s9SMjIxsliEiErpa1DiP9we25sGudZi3fg8dh6Xw7CcbOHLsRL7Xkq1wd/ftgdfdwEQyp1l2mVl5gMDr7kD3dKBSlrdXBLaf5jNHuXuSuyclJiae+whEREJIbEwUfdvW4KMhbWlXqyz//mAdXUbMYf6GPflaxxnD3czizaz4yW2gE7ASmAzcEuh2C/BeYHsy0NvM4sysGlATWJTbhYuIhLIKpYrw/E0X88qtTTl23Llh9EIGjl3C7v2H8uX3x2SjTzlgopmd7D/G3WeY2WJgnJndDnwJXAPg7qvMbBywGjgG9Hf30Pj6WEQkn7WvXZYW957HyNkbGTl7I5+s3c19nWpxU/MqxETn3aVGFgqL75OSkjw1NTXYZYiI5KnNew7wl8mrmPNFBnXLl+CRnvVoUrn0md/4M8wsLcvy9FPoClURkXxSrUw8r93alOdubMLeA0fo9dx8Hpm6Ok9+V3amZUREJJeYGd3ql6dNrURGfPQFlROK5snvUbiLiARBsbgY/vDrunn2+ZqWEREJQwp3EZEwpHAXEQlDCncRkTCkcBcRCUMKdxGRMKRwFxEJQwp3EZEwFBL3ljGzDGBrDj6iDJC/99MMDRp3ZNG4I0t2xl3F3U97z/SQCPecMrPUn7t5TjjTuCOLxh1ZcjpuTcuIiIQhhbuISBgKl3AfFewCgkTjjiwad2TJ0bjDYs5dREROFS5n7iIikoXCXUQkDBXocDezLma2zsw2mNmDwa4nr5jZy2a228xWZmlLMLOZZrY+8HruD2IMUWZWycw+MbM1ZrbKzAYF2sN67GZW2MwWmdmywLj/GmgP63GfZGbRZrbEzKYG9iNl3FvMbIWZLTWz1EDbOY+9wIa7mUUDzwJdgbrA9WaWd481Ca5XgS4/ansQmOXuNYFZgf1wcwwY4u4XAs2B/oE/43Af+2HgUndvCDQCuphZc8J/3CcNAtZk2Y+UcQO0d/dGWda3n/PYC2y4A82ADe6+yd2PAG8DVwa5pjzh7nOAvT9qvhJ4LbD9GtAjP2vKD+6+w90/D2x/R+Zf+AqE+dg90/eB3UKBHyfMxw1gZhWBXwOjszSH/bh/wTmPvSCHewVgW5b99EBbpCjn7jsgMwSBskGuJ0+ZWVWgMbCQCBh7YGpiKbAbmOnuETFu4Engd8CJLG2RMG7I/Af8QzNLM7M+gbZzHntBfkC2naZN6zrDkJkVA/4DDHb3/Wan+6MPL+5+HGhkZqWAiWZWL8gl5Tkz6w7sdvc0M2sX5HKCoZW7bzezssBMM1ubkw8ryGfu6UClLPsVge1BqiUYdplZeYDA6+4g15MnzKwQmcH+lru/G2iOiLEDuPs+YDaZ37mE+7hbAVeY2RYyp1kvNbM3Cf9xA+Du2wOvu4GJZE49n/PYC3K4LwZqmlk1M4sFegOTg1xTfpoM3BLYvgV4L4i15AnLPEV/CVjj7sOyHArrsZtZYuCMHTMrAnQE1hLm43b3h9y9ortXJfPv88fu/hvCfNwAZhZvZsVPbgOdgJXkYOwF+gpVM+tG5hxdNPCyu/8juBXlDTMbC7Qj8xagu4C/AJOAcUBl4EvgGnf/8ZeuBZqZJQNzgRX8bw7292TOu4ft2M2sAZlfnkWTeQI2zt3/ZmbnEcbjziowLXO/u3ePhHGbWXUyz9Yhc7p8jLv/IydjL9DhLiIip1eQp2VERORnKNxFRMKQwl1EJAwp3EVEwpDCXUQkDCncRUTCkMJdRCQM/T/UDH3xn9FtiQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(list(range(n_epochs)), loss_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple RNN with pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_vector_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = nn.Embedding(1000, input_vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequence = torch.tensor([1, 4, 12, 18, 230, 111, 11], dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating an RNN from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size = 64\n",
    "state = torch.zeros(state_size) # initializing the hidden state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wi = torch.randn((state_size, input_vector_size)) # transformation matrix of input xi\n",
    "Ws = torch.randn((state_size, state_size)) # transformation matrix of state\n",
    "b = torch.randn(state_size) # bias "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 128]), torch.Size([64, 64]), torch.Size([64]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Wi.shape, Ws.shape, b.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_tensor = embed(test_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_t = state\n",
    "for x in sequence_tensor:\n",
    "    output_t = torch.tanh(torch.matmul(Ws, state_t) + torch.matmul(Wi, x) + b)\n",
    "    state_t = output_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pytorch RNN Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = nn.RNN(input_vector_size, state_size, batch_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We'll compare pytorch rnn output and our output and check if they are same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using weights from pytorch's rnn layer\n",
    "Wi = rnn.weight_ih_l0\n",
    "bi = rnn.bias_ih_l0\n",
    "Wh = rnn.weight_hh_l0\n",
    "bh = rnn.bias_hh_l0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 128]),\n",
       " torch.Size([64]),\n",
       " torch.Size([64, 64]),\n",
       " torch.Size([64]))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Wi.shape, bi.shape, Wh.shape, bh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "h0 = state # initializing hidden state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### First let's test our custom rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_list = []\n",
    "state_t = state\n",
    "for x in sequence_tensor:\n",
    "    output_t = torch.tanh(torch.matmul(Wi, x) + bi + torch.matmul(Wh, state_t) +  bh)\n",
    "    state_t = output_t\n",
    "    states_list.append(state_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.3575,  0.8866,  0.3297, -0.1152,  0.7982,  0.9475, -0.8738, -0.2656,\n",
       "         0.3211, -0.4832,  0.0184,  0.6564, -0.9244, -0.3799, -0.6355, -0.5357,\n",
       "         0.3366,  0.4906,  0.8238, -0.4333, -0.6566, -0.1396, -0.2119, -0.1178,\n",
       "        -0.1760, -0.3913,  0.2043,  0.0870,  0.7140, -0.6581, -0.7373, -0.4226,\n",
       "        -0.1222,  0.4654,  0.3471, -0.5335,  0.2161, -0.0403, -0.4448,  0.6491,\n",
       "        -0.3490, -0.0057,  0.5984,  0.2798, -0.3458,  0.0521, -0.1211,  0.6397,\n",
       "         0.1103,  0.9152,  0.2929,  0.1615, -0.5475, -0.6152,  0.0250, -0.3444,\n",
       "         0.5643, -0.0167,  0.3407, -0.3758,  0.5159,  0.6705,  0.6898, -0.5436],\n",
       "       grad_fn=<TanhBackward>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_t # final output of the last timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([ 4.6093e-02, -8.0820e-02,  7.2737e-02,  9.0263e-01,  2.2952e-01,\n",
       "         -1.9151e-01,  2.4947e-01, -8.8030e-01, -9.6883e-02,  7.7205e-01,\n",
       "         -2.0517e-01,  7.9555e-01,  3.5019e-01,  9.5798e-03, -3.3854e-04,\n",
       "         -3.3213e-03,  4.0920e-01, -4.4092e-01, -1.8821e-01, -3.3963e-01,\n",
       "          4.5386e-02,  5.9333e-02, -7.9663e-01,  7.3187e-01,  1.4824e-01,\n",
       "          6.8807e-01,  2.6872e-01,  1.5863e-01,  3.8996e-01, -1.0974e-01,\n",
       "         -4.3455e-01, -2.6538e-01, -2.4851e-01, -8.4637e-01,  2.2002e-02,\n",
       "         -1.8195e-01, -9.1377e-01, -9.9144e-02,  5.1035e-01,  4.1960e-01,\n",
       "         -3.7715e-01, -4.8304e-01, -2.3012e-02, -5.5634e-01, -5.8089e-01,\n",
       "          2.8184e-01,  9.5949e-02, -8.5539e-01,  8.4059e-01, -6.4788e-02,\n",
       "          9.7776e-01, -3.9081e-01,  7.4812e-01, -6.3848e-01, -9.0568e-02,\n",
       "         -4.0149e-02, -5.7260e-01,  6.2972e-02,  2.8534e-01,  1.8689e-02,\n",
       "          8.5492e-01,  7.6568e-01,  7.8325e-01,  3.2216e-01],\n",
       "        grad_fn=<TanhBackward>),\n",
       " tensor([ 0.4378, -0.6279,  0.9178,  0.7069,  0.2676, -0.4074,  0.9138,  0.4449,\n",
       "         -0.0326,  0.0615,  0.1006,  0.9641,  0.0671, -0.1435,  0.6354,  0.6126,\n",
       "         -0.6355, -0.8039, -0.7131, -0.0688,  0.8431, -0.1806, -0.6549, -0.3841,\n",
       "         -0.3423,  0.6101, -0.5060,  0.4505, -0.0744, -0.9510,  0.3976,  0.4608,\n",
       "         -0.7488,  0.1980,  0.2099, -0.9352,  0.9672,  0.7986,  0.7279,  0.3399,\n",
       "          0.9832, -0.0096,  0.1584, -0.2229, -0.2278, -0.6628, -0.9830, -0.8466,\n",
       "          0.2830, -0.7463, -0.8176, -0.5280, -0.6601, -0.5146, -0.7663, -0.3393,\n",
       "          0.6773,  0.8146,  0.9365,  0.1404, -0.6874,  0.2787, -0.6573,  0.0984],\n",
       "        grad_fn=<TanhBackward>),\n",
       " tensor([ 0.2128,  0.3576,  0.6664, -0.3869,  0.4440, -0.0827,  0.4800, -0.7314,\n",
       "         -0.8118,  0.3916, -0.7788,  0.7014,  0.8865, -0.6531, -0.1033,  0.7683,\n",
       "          0.3680, -0.7828,  0.7262,  0.5570,  0.9723,  0.2866, -0.5996,  0.3883,\n",
       "          0.6382,  0.0224,  0.7836,  0.3705,  0.0873, -0.5440, -0.4244, -0.6680,\n",
       "         -0.3118, -0.7696, -0.4105,  0.6969, -0.8088, -0.7431,  0.6595,  0.6705,\n",
       "         -0.1121, -0.8576, -0.9691,  0.6311,  0.8804, -0.8508, -0.8187, -0.2912,\n",
       "          0.3398, -0.5474,  0.6459, -0.0077,  0.5315, -0.1478,  0.0895,  0.0129,\n",
       "         -0.7898,  0.0044,  0.2314, -0.3212,  0.4568, -0.5811,  0.5005, -0.5686],\n",
       "        grad_fn=<TanhBackward>),\n",
       " tensor([ 0.9054,  0.2075, -0.7479,  0.2814, -0.9163, -0.7372, -0.9686, -0.7262,\n",
       "         -0.1559,  0.3275,  0.0494, -0.4729,  0.2739,  0.5008,  0.8248,  0.8126,\n",
       "          0.2572, -0.1793,  0.8126,  0.5127,  0.2963, -0.0595, -0.8505, -0.1815,\n",
       "         -0.7867, -0.5056,  0.8811, -0.7119, -0.3084, -0.8526,  0.4590,  0.0183,\n",
       "         -0.7153, -0.1553, -0.3138,  0.6363,  0.7291,  0.3072,  0.1373, -0.5269,\n",
       "          0.5453, -0.2381, -0.4991,  0.2967,  0.7640, -0.8606, -0.0430,  0.3122,\n",
       "         -0.3458, -0.1751,  0.4586,  0.2486, -0.1542,  0.0802,  0.1694,  0.8916,\n",
       "         -0.2881, -0.2159,  0.5889,  0.4247,  0.2951, -0.6106, -0.0722, -0.6633],\n",
       "        grad_fn=<TanhBackward>),\n",
       " tensor([ 0.5475,  0.2927, -0.5233,  0.4113, -0.8470, -0.8659,  0.0316,  0.9407,\n",
       "         -0.7207,  0.5462,  0.7987,  0.6751,  0.6146,  0.5155,  0.6937, -0.3926,\n",
       "          0.4608, -0.6217,  0.7694,  0.2909, -0.3373,  0.2021,  0.8249, -0.8083,\n",
       "         -0.1710, -0.3420,  0.0062, -0.9670, -0.0761, -0.9518,  0.7985, -0.8436,\n",
       "         -0.1755,  0.6079,  0.4079, -0.4474, -0.5397, -0.3980, -0.7665, -0.4996,\n",
       "          0.2817, -0.5641, -0.1183, -0.1125, -0.6334, -0.4905,  0.5309, -0.0797,\n",
       "          0.8202,  0.4134,  0.7094, -0.1310, -0.6950,  0.8332,  0.9065,  0.8695,\n",
       "          0.1433,  0.4867,  0.4884,  0.9744, -0.6609, -0.9409, -0.0970, -0.4333],\n",
       "        grad_fn=<TanhBackward>),\n",
       " tensor([-9.3474e-01, -5.2974e-01,  6.9282e-01, -7.0506e-02,  8.1007e-01,\n",
       "         -9.2578e-01, -5.8174e-01,  6.7074e-02,  1.1894e-02,  4.5635e-01,\n",
       "          5.8468e-01,  1.7361e-01,  9.5975e-01,  4.6556e-02,  1.3897e-01,\n",
       "         -3.0606e-01,  6.6268e-01, -4.6422e-04,  8.5957e-01, -8.8922e-01,\n",
       "         -8.6477e-01,  1.4755e-01,  3.6155e-02, -2.9249e-01, -1.6765e-01,\n",
       "         -9.3808e-01, -3.3410e-01, -7.8707e-01,  6.9216e-01,  2.8912e-01,\n",
       "          3.5109e-01,  1.6803e-01, -7.1404e-01, -2.4631e-01,  5.9839e-01,\n",
       "          8.7694e-01, -9.3037e-01, -4.7246e-01, -7.6003e-01, -6.4195e-01,\n",
       "          4.0698e-01, -1.2589e-01, -6.3642e-01, -3.9067e-01, -9.3628e-01,\n",
       "          3.3167e-01, -2.0777e-03, -4.2962e-01,  8.3362e-02,  9.5204e-01,\n",
       "          2.4723e-01,  4.8997e-01, -4.4978e-01,  4.9881e-01, -9.9415e-01,\n",
       "          2.7595e-01, -8.5366e-01, -3.7793e-01,  6.2052e-01,  6.7720e-01,\n",
       "         -7.4015e-01,  4.2172e-01, -8.7370e-01,  8.0448e-01],\n",
       "        grad_fn=<TanhBackward>),\n",
       " tensor([-0.3575,  0.8866,  0.3297, -0.1152,  0.7982,  0.9475, -0.8738, -0.2656,\n",
       "          0.3211, -0.4832,  0.0184,  0.6564, -0.9244, -0.3799, -0.6355, -0.5357,\n",
       "          0.3366,  0.4906,  0.8238, -0.4333, -0.6566, -0.1396, -0.2119, -0.1178,\n",
       "         -0.1760, -0.3913,  0.2043,  0.0870,  0.7140, -0.6581, -0.7373, -0.4226,\n",
       "         -0.1222,  0.4654,  0.3471, -0.5335,  0.2161, -0.0403, -0.4448,  0.6491,\n",
       "         -0.3490, -0.0057,  0.5984,  0.2798, -0.3458,  0.0521, -0.1211,  0.6397,\n",
       "          0.1103,  0.9152,  0.2929,  0.1615, -0.5475, -0.6152,  0.0250, -0.3444,\n",
       "          0.5643, -0.0167,  0.3407, -0.3758,  0.5159,  0.6705,  0.6898, -0.5436],\n",
       "        grad_fn=<TanhBackward>)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states_list # a list containing intermediate states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now let's test the same with pytorch's rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "h0 = torch.zeros((1, 1, state_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 64])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_tensor_batch = sequence_tensor.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = rnn(sequence_tensor_batch, h0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 4.6093e-02, -8.0820e-02,  7.2737e-02,  9.0263e-01,  2.2952e-01,\n",
       "          -1.9151e-01,  2.4947e-01, -8.8030e-01, -9.6883e-02,  7.7205e-01,\n",
       "          -2.0517e-01,  7.9555e-01,  3.5019e-01,  9.5797e-03, -3.3852e-04,\n",
       "          -3.3213e-03,  4.0920e-01, -4.4092e-01, -1.8821e-01, -3.3963e-01,\n",
       "           4.5386e-02,  5.9333e-02, -7.9663e-01,  7.3187e-01,  1.4824e-01,\n",
       "           6.8807e-01,  2.6872e-01,  1.5863e-01,  3.8996e-01, -1.0974e-01,\n",
       "          -4.3455e-01, -2.6538e-01, -2.4851e-01, -8.4637e-01,  2.2002e-02,\n",
       "          -1.8195e-01, -9.1377e-01, -9.9144e-02,  5.1035e-01,  4.1960e-01,\n",
       "          -3.7715e-01, -4.8304e-01, -2.3012e-02, -5.5634e-01, -5.8089e-01,\n",
       "           2.8184e-01,  9.5949e-02, -8.5539e-01,  8.4059e-01, -6.4788e-02,\n",
       "           9.7776e-01, -3.9081e-01,  7.4812e-01, -6.3848e-01, -9.0568e-02,\n",
       "          -4.0149e-02, -5.7260e-01,  6.2971e-02,  2.8534e-01,  1.8689e-02,\n",
       "           8.5492e-01,  7.6568e-01,  7.8325e-01,  3.2216e-01],\n",
       "         [ 4.3776e-01, -6.2786e-01,  9.1776e-01,  7.0687e-01,  2.6760e-01,\n",
       "          -4.0743e-01,  9.1381e-01,  4.4493e-01, -3.2627e-02,  6.1522e-02,\n",
       "           1.0059e-01,  9.6406e-01,  6.7109e-02, -1.4353e-01,  6.3542e-01,\n",
       "           6.1255e-01, -6.3547e-01, -8.0391e-01, -7.1313e-01, -6.8850e-02,\n",
       "           8.4313e-01, -1.8057e-01, -6.5485e-01, -3.8412e-01, -3.4225e-01,\n",
       "           6.1007e-01, -5.0597e-01,  4.5055e-01, -7.4413e-02, -9.5096e-01,\n",
       "           3.9758e-01,  4.6084e-01, -7.4881e-01,  1.9801e-01,  2.0987e-01,\n",
       "          -9.3524e-01,  9.6724e-01,  7.9860e-01,  7.2791e-01,  3.3991e-01,\n",
       "           9.8316e-01, -9.6226e-03,  1.5844e-01, -2.2290e-01, -2.2782e-01,\n",
       "          -6.6283e-01, -9.8304e-01, -8.4660e-01,  2.8302e-01, -7.4629e-01,\n",
       "          -8.1765e-01, -5.2804e-01, -6.6009e-01, -5.1462e-01, -7.6629e-01,\n",
       "          -3.3934e-01,  6.7725e-01,  8.1461e-01,  9.3645e-01,  1.4040e-01,\n",
       "          -6.8743e-01,  2.7869e-01, -6.5727e-01,  9.8366e-02],\n",
       "         [ 2.1281e-01,  3.5761e-01,  6.6639e-01, -3.8688e-01,  4.4402e-01,\n",
       "          -8.2739e-02,  4.7999e-01, -7.3141e-01, -8.1180e-01,  3.9161e-01,\n",
       "          -7.7883e-01,  7.0138e-01,  8.8649e-01, -6.5314e-01, -1.0333e-01,\n",
       "           7.6825e-01,  3.6802e-01, -7.8285e-01,  7.2621e-01,  5.5701e-01,\n",
       "           9.7228e-01,  2.8657e-01, -5.9960e-01,  3.8826e-01,  6.3824e-01,\n",
       "           2.2421e-02,  7.8363e-01,  3.7053e-01,  8.7278e-02, -5.4399e-01,\n",
       "          -4.2439e-01, -6.6803e-01, -3.1176e-01, -7.6965e-01, -4.1050e-01,\n",
       "           6.9690e-01, -8.0878e-01, -7.4311e-01,  6.5955e-01,  6.7050e-01,\n",
       "          -1.1207e-01, -8.5757e-01, -9.6913e-01,  6.3111e-01,  8.8036e-01,\n",
       "          -8.5082e-01, -8.1872e-01, -2.9118e-01,  3.3978e-01, -5.4738e-01,\n",
       "           6.4586e-01, -7.6553e-03,  5.3152e-01, -1.4778e-01,  8.9494e-02,\n",
       "           1.2894e-02, -7.8984e-01,  4.3820e-03,  2.3137e-01, -3.2119e-01,\n",
       "           4.5682e-01, -5.8105e-01,  5.0054e-01, -5.6860e-01],\n",
       "         [ 9.0545e-01,  2.0747e-01, -7.4795e-01,  2.8139e-01, -9.1633e-01,\n",
       "          -7.3724e-01, -9.6864e-01, -7.2621e-01, -1.5586e-01,  3.2753e-01,\n",
       "           4.9373e-02, -4.7292e-01,  2.7387e-01,  5.0085e-01,  8.2476e-01,\n",
       "           8.1264e-01,  2.5719e-01, -1.7934e-01,  8.1264e-01,  5.1273e-01,\n",
       "           2.9634e-01, -5.9549e-02, -8.5051e-01, -1.8150e-01, -7.8668e-01,\n",
       "          -5.0556e-01,  8.8108e-01, -7.1187e-01, -3.0836e-01, -8.5259e-01,\n",
       "           4.5905e-01,  1.8276e-02, -7.1526e-01, -1.5528e-01, -3.1381e-01,\n",
       "           6.3631e-01,  7.2905e-01,  3.0723e-01,  1.3731e-01, -5.2689e-01,\n",
       "           5.4531e-01, -2.3808e-01, -4.9915e-01,  2.9667e-01,  7.6397e-01,\n",
       "          -8.6056e-01, -4.3012e-02,  3.1221e-01, -3.4580e-01, -1.7510e-01,\n",
       "           4.5863e-01,  2.4855e-01, -1.5421e-01,  8.0235e-02,  1.6938e-01,\n",
       "           8.9155e-01, -2.8807e-01, -2.1592e-01,  5.8891e-01,  4.2470e-01,\n",
       "           2.9509e-01, -6.1057e-01, -7.2218e-02, -6.6327e-01],\n",
       "         [ 5.4747e-01,  2.9269e-01, -5.2328e-01,  4.1128e-01, -8.4703e-01,\n",
       "          -8.6594e-01,  3.1630e-02,  9.4071e-01, -7.2068e-01,  5.4618e-01,\n",
       "           7.9873e-01,  6.7508e-01,  6.1459e-01,  5.1552e-01,  6.9370e-01,\n",
       "          -3.9263e-01,  4.6083e-01, -6.2174e-01,  7.6939e-01,  2.9093e-01,\n",
       "          -3.3729e-01,  2.0213e-01,  8.2493e-01, -8.0831e-01, -1.7099e-01,\n",
       "          -3.4203e-01,  6.2466e-03, -9.6699e-01, -7.6106e-02, -9.5178e-01,\n",
       "           7.9849e-01, -8.4363e-01, -1.7545e-01,  6.0791e-01,  4.0786e-01,\n",
       "          -4.4744e-01, -5.3966e-01, -3.9801e-01, -7.6651e-01, -4.9957e-01,\n",
       "           2.8166e-01, -5.6410e-01, -1.1834e-01, -1.1248e-01, -6.3338e-01,\n",
       "          -4.9053e-01,  5.3088e-01, -7.9708e-02,  8.2023e-01,  4.1341e-01,\n",
       "           7.0941e-01, -1.3098e-01, -6.9495e-01,  8.3321e-01,  9.0653e-01,\n",
       "           8.6950e-01,  1.4332e-01,  4.8669e-01,  4.8840e-01,  9.7441e-01,\n",
       "          -6.6093e-01, -9.4093e-01, -9.6954e-02, -4.3331e-01],\n",
       "         [-9.3474e-01, -5.2974e-01,  6.9282e-01, -7.0506e-02,  8.1007e-01,\n",
       "          -9.2578e-01, -5.8174e-01,  6.7073e-02,  1.1894e-02,  4.5636e-01,\n",
       "           5.8468e-01,  1.7361e-01,  9.5975e-01,  4.6556e-02,  1.3897e-01,\n",
       "          -3.0606e-01,  6.6268e-01, -4.6426e-04,  8.5957e-01, -8.8922e-01,\n",
       "          -8.6477e-01,  1.4755e-01,  3.6154e-02, -2.9249e-01, -1.6765e-01,\n",
       "          -9.3808e-01, -3.3410e-01, -7.8707e-01,  6.9216e-01,  2.8912e-01,\n",
       "           3.5109e-01,  1.6803e-01, -7.1404e-01, -2.4631e-01,  5.9839e-01,\n",
       "           8.7694e-01, -9.3037e-01, -4.7246e-01, -7.6003e-01, -6.4195e-01,\n",
       "           4.0698e-01, -1.2589e-01, -6.3642e-01, -3.9067e-01, -9.3628e-01,\n",
       "           3.3167e-01, -2.0776e-03, -4.2962e-01,  8.3362e-02,  9.5204e-01,\n",
       "           2.4723e-01,  4.8997e-01, -4.4978e-01,  4.9881e-01, -9.9415e-01,\n",
       "           2.7595e-01, -8.5366e-01, -3.7793e-01,  6.2052e-01,  6.7720e-01,\n",
       "          -7.4015e-01,  4.2172e-01, -8.7370e-01,  8.0448e-01],\n",
       "         [-3.5745e-01,  8.8655e-01,  3.2970e-01, -1.1519e-01,  7.9825e-01,\n",
       "           9.4752e-01, -8.7381e-01, -2.6559e-01,  3.2113e-01, -4.8320e-01,\n",
       "           1.8396e-02,  6.5640e-01, -9.2441e-01, -3.7991e-01, -6.3554e-01,\n",
       "          -5.3567e-01,  3.3663e-01,  4.9065e-01,  8.2379e-01, -4.3333e-01,\n",
       "          -6.5659e-01, -1.3955e-01, -2.1195e-01, -1.1780e-01, -1.7602e-01,\n",
       "          -3.9129e-01,  2.0428e-01,  8.6971e-02,  7.1403e-01, -6.5806e-01,\n",
       "          -7.3730e-01, -4.2264e-01, -1.2222e-01,  4.6542e-01,  3.4714e-01,\n",
       "          -5.3348e-01,  2.1611e-01, -4.0323e-02, -4.4475e-01,  6.4912e-01,\n",
       "          -3.4897e-01, -5.6796e-03,  5.9838e-01,  2.7976e-01, -3.4585e-01,\n",
       "           5.2098e-02, -1.2111e-01,  6.3967e-01,  1.1035e-01,  9.1521e-01,\n",
       "           2.9293e-01,  1.6149e-01, -5.4746e-01, -6.1518e-01,  2.4987e-02,\n",
       "          -3.4445e-01,  5.6426e-01, -1.6696e-02,  3.4074e-01, -3.7583e-01,\n",
       "           5.1585e-01,  6.7055e-01,  6.8982e-01, -5.4355e-01]]],\n",
       "       grad_fn=<TransposeBackward1>)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3575,  0.8866,  0.3297, -0.1152,  0.7982,  0.9475, -0.8738,\n",
       "          -0.2656,  0.3211, -0.4832,  0.0184,  0.6564, -0.9244, -0.3799,\n",
       "          -0.6355, -0.5357,  0.3366,  0.4906,  0.8238, -0.4333, -0.6566,\n",
       "          -0.1396, -0.2119, -0.1178, -0.1760, -0.3913,  0.2043,  0.0870,\n",
       "           0.7140, -0.6581, -0.7373, -0.4226, -0.1222,  0.4654,  0.3471,\n",
       "          -0.5335,  0.2161, -0.0403, -0.4448,  0.6491, -0.3490, -0.0057,\n",
       "           0.5984,  0.2798, -0.3458,  0.0521, -0.1211,  0.6397,  0.1103,\n",
       "           0.9152,  0.2929,  0.1615, -0.5475, -0.6152,  0.0250, -0.3444,\n",
       "           0.5643, -0.0167,  0.3407, -0.3758,  0.5159,  0.6705,  0.6898,\n",
       "          -0.5436]]], grad_fn=<StackBackward>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Voila! Both the intermediate hidden states and the final output is the exact same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Let's do the same comparision with stacked (2 layer) RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_rnn = nn.RNN(input_vector_size, state_size, batch_first=True, num_layers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(128, 64, num_layers=2, batch_first=True)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Custom RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer 1\n",
    "Wi0 = stacked_rnn.weight_ih_l0\n",
    "bi0 = stacked_rnn.bias_ih_l0\n",
    "Wh0 = stacked_rnn.weight_hh_l0\n",
    "bh0 = stacked_rnn.bias_hh_l0\n",
    "\n",
    "# layer 2\n",
    "Wi1 = stacked_rnn.weight_ih_l1\n",
    "bi1 = stacked_rnn.bias_ih_l1\n",
    "Wh1 = stacked_rnn.weight_hh_l1\n",
    "bh1 = stacked_rnn.bias_hh_l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 128]) torch.Size([64]) torch.Size([64, 64]) torch.Size([64])\n",
      "torch.Size([64, 64]) torch.Size([64]) torch.Size([64, 64]) torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "print(Wi0.shape, bi0.shape, Wh0.shape, bh0.shape)\n",
    "print(Wi1.shape, bi1.shape, Wh1.shape, bh1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "h0 = torch.zeros(state_size)\n",
    "h1 = torch.zeros(state_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let's only compare the final output state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in sequence_tensor:\n",
    "    output_t0 = torch.tanh(torch.matmul(Wi0, x) + bi0 + torch.matmul(Wh0, h0) +  bh0)\n",
    "    h0 = output_t0\n",
    "    output_t1 = torch.tanh(torch.matmul(Wi1, output_t0) + bi1 + torch.matmul(Wh1, h1) +  bh1)\n",
    "    h1 = output_t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2928,  0.9150,  0.8000, -0.7299, -0.4685,  0.6698, -0.4187,  0.0011,\n",
       "         0.9215, -0.0619,  0.6369, -0.1371, -0.2154,  0.1984,  0.1148, -0.7858,\n",
       "         0.5273, -0.9107,  0.8165,  0.6592,  0.6275, -0.5804, -0.0106, -0.5003,\n",
       "         0.6993,  0.7250,  0.4690,  0.2746, -0.4140, -0.3932,  0.8941, -0.0061,\n",
       "         0.3116, -0.5476, -0.7164, -0.4358, -0.5323,  0.6103, -0.6076,  0.7009,\n",
       "         0.1427, -0.5602,  0.0411, -0.1266, -0.3412,  0.2330, -0.8007,  0.9833,\n",
       "        -0.5356, -0.6134, -0.9708, -0.7977, -0.8324, -0.9692, -0.2822, -0.2769,\n",
       "        -0.5987,  0.2714,  0.1375, -0.2299,  0.6119,  0.6106,  0.4026, -0.3283],\n",
       "       grad_fn=<TanhBackward>)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1628,  0.2336, -0.1487, -0.1180,  0.2257,  0.4696,  0.2923, -0.7042,\n",
       "         0.6253,  0.2631,  0.1130, -0.2642,  0.0966, -0.5539,  0.4591, -0.0130,\n",
       "         0.1238,  0.1137,  0.4038, -0.4220,  0.2644, -0.0608,  0.1855, -0.4629,\n",
       "         0.4423, -0.3561,  0.5318,  0.2474,  0.4449,  0.4060, -0.0224,  0.4130,\n",
       "         0.5069, -0.6645, -0.1503,  0.0956,  0.5251,  0.3742, -0.1306,  0.2174,\n",
       "        -0.4255,  0.5023,  0.4641, -0.0851,  0.6065, -0.1540, -0.2152, -0.3892,\n",
       "         0.2905,  0.3147, -0.0247, -0.4305, -0.3932,  0.1342,  0.3941, -0.0287,\n",
       "        -0.4227, -0.0377, -0.2807, -0.0042,  0.4724,  0.0741,  0.0124,  0.2642],\n",
       "       grad_fn=<TanhBackward>)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_t1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pytorch's stacked RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing two hidden states: one for each layer\n",
    "h = torch.zeros((2, 1, state_size)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 64])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_tensor_batch = sequence_tensor.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = stacked_rnn(sequence_tensor_batch, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2928,  0.9150,  0.8000, -0.7299, -0.4685,  0.6698, -0.4187,\n",
       "           0.0011,  0.9215, -0.0619,  0.6369, -0.1371, -0.2154,  0.1984,\n",
       "           0.1148, -0.7858,  0.5273, -0.9107,  0.8165,  0.6592,  0.6275,\n",
       "          -0.5804, -0.0106, -0.5003,  0.6993,  0.7250,  0.4690,  0.2746,\n",
       "          -0.4140, -0.3932,  0.8941, -0.0061,  0.3116, -0.5476, -0.7164,\n",
       "          -0.4358, -0.5323,  0.6103, -0.6076,  0.7009,  0.1427, -0.5602,\n",
       "           0.0411, -0.1266, -0.3412,  0.2330, -0.8007,  0.9833, -0.5356,\n",
       "          -0.6134, -0.9708, -0.7977, -0.8324, -0.9692, -0.2822, -0.2769,\n",
       "          -0.5987,  0.2714,  0.1375, -0.2299,  0.6119,  0.6106,  0.4026,\n",
       "          -0.3283]],\n",
       "\n",
       "        [[ 0.1628,  0.2336, -0.1487, -0.1180,  0.2257,  0.4696,  0.2923,\n",
       "          -0.7042,  0.6253,  0.2631,  0.1130, -0.2642,  0.0966, -0.5539,\n",
       "           0.4591, -0.0130,  0.1238,  0.1137,  0.4038, -0.4220,  0.2644,\n",
       "          -0.0608,  0.1855, -0.4629,  0.4423, -0.3561,  0.5318,  0.2474,\n",
       "           0.4449,  0.4060, -0.0224,  0.4130,  0.5069, -0.6645, -0.1503,\n",
       "           0.0956,  0.5251,  0.3742, -0.1306,  0.2174, -0.4255,  0.5023,\n",
       "           0.4641, -0.0851,  0.6065, -0.1540, -0.2152, -0.3892,  0.2905,\n",
       "           0.3147, -0.0247, -0.4305, -0.3932,  0.1342,  0.3941, -0.0287,\n",
       "          -0.4227, -0.0377, -0.2807, -0.0042,  0.4724,  0.0741,  0.0124,\n",
       "           0.2642]]], grad_fn=<StackBackward>)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Voila! Both are the same again!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidirectional RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's do the same with a bidirectional rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pytorch's bidirectional RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "bidirectional_rnn = nn.RNN(input_vector_size, state_size, batch_first=True, bidirectional=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = torch.zeros((2, 1, state_size)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_tensor_batch = sequence_tensor.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = bidirectional_rnn(sequence_tensor_batch, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2584,  0.3041, -0.3862,  0.4029,  0.6885,  0.8432, -0.6351,\n",
       "          -0.5460,  0.0953, -0.4156,  0.1068,  0.2245, -0.5007, -0.1374,\n",
       "           0.5629, -0.8426, -0.2664, -0.4181, -0.3821,  0.2238,  0.7070,\n",
       "          -0.1425,  0.7286,  0.5817, -0.7461,  0.6266,  0.3602,  0.5267,\n",
       "          -0.8968,  0.3007,  0.3293, -0.8665,  0.9362, -0.5053, -0.9976,\n",
       "           0.3650, -0.6077,  0.2448,  0.7236, -0.8658, -0.6613,  0.3575,\n",
       "          -0.5928,  0.3083, -0.2982, -0.2150, -0.4062, -0.2687, -0.5029,\n",
       "           0.0946, -0.0984,  0.5487, -0.1824,  0.1004,  0.1028,  0.8650,\n",
       "          -0.5543, -0.2549,  0.8172, -0.3581, -0.4622, -0.0857, -0.9556,\n",
       "           0.0445]],\n",
       "\n",
       "        [[ 0.2547,  0.7344,  0.8434, -0.3580,  0.5297, -0.9322,  0.8004,\n",
       "           0.3339, -0.4490,  0.1905,  0.6011,  0.9659,  0.9464, -0.4746,\n",
       "           0.9557,  0.8940, -0.3241, -0.6548,  0.9255, -0.8389,  0.4709,\n",
       "           0.3344, -0.4005,  0.7088, -0.5764,  0.6074, -0.5086, -0.4841,\n",
       "          -0.8506,  0.1230, -0.5096, -0.8173, -0.4453, -0.2131,  0.6687,\n",
       "          -0.3004, -0.7540, -0.0385,  0.5183,  0.4360,  0.5561,  0.6738,\n",
       "          -0.4656, -0.6029,  0.7869,  0.9342,  0.3325,  0.3110, -0.1449,\n",
       "          -0.0705,  0.1397, -0.0889, -0.3431, -0.0939,  0.8416,  0.2498,\n",
       "           0.8542, -0.7149,  0.5641,  0.5308, -0.8786,  0.6440,  0.6129,\n",
       "          -0.3789]]], grad_fn=<StackBackward>)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Custom Bidirectional RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward\n",
    "Wif = bidirectional_rnn.weight_ih_l0\n",
    "bif = bidirectional_rnn.bias_ih_l0\n",
    "Whf = bidirectional_rnn.weight_hh_l0\n",
    "bhf = bidirectional_rnn.bias_hh_l0\n",
    "\n",
    "# layer 2\n",
    "Wib = bidirectional_rnn.weight_ih_l0_reverse\n",
    "bib = bidirectional_rnn.bias_ih_l0_reverse\n",
    "Whb = bidirectional_rnn.weight_hh_l0_reverse\n",
    "bhb = bidirectional_rnn.bias_hh_l0_reverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 128]) torch.Size([64]) torch.Size([64, 64]) torch.Size([64])\n",
      "torch.Size([64, 128]) torch.Size([64]) torch.Size([64, 64]) torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "print(Wif.shape, bif.shape, Whf.shape, bhf.shape)\n",
    "print(Wib.shape, bib.shape, Whb.shape, bhb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf = torch.zeros(state_size)\n",
    "hb = torch.zeros(state_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(sequence_tensor)\n",
    "for i in range(n):\n",
    "    xf = sequence_tensor[i]\n",
    "    xb = sequence_tensor[n-i-1]\n",
    "    output_f = torch.tanh(torch.matmul(Wif, xf) + bif + torch.matmul(Whf, hf) +  bhf)\n",
    "    hf = output_f\n",
    "    output_b = torch.tanh(torch.matmul(Wib, xb) + bib + torch.matmul(Whb, hb) +  bhb)\n",
    "    hb = output_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2584,  0.3041, -0.3862,  0.4029,  0.6885,  0.8432, -0.6351, -0.5460,\n",
       "          0.0953, -0.4156,  0.1068,  0.2245, -0.5007, -0.1374,  0.5629, -0.8426,\n",
       "         -0.2664, -0.4181, -0.3821,  0.2238,  0.7070, -0.1425,  0.7286,  0.5817,\n",
       "         -0.7461,  0.6266,  0.3602,  0.5267, -0.8968,  0.3007,  0.3293, -0.8665,\n",
       "          0.9362, -0.5053, -0.9976,  0.3650, -0.6077,  0.2448,  0.7236, -0.8658,\n",
       "         -0.6613,  0.3575, -0.5928,  0.3083, -0.2982, -0.2150, -0.4062, -0.2687,\n",
       "         -0.5029,  0.0946, -0.0984,  0.5487, -0.1824,  0.1004,  0.1028,  0.8650,\n",
       "         -0.5543, -0.2549,  0.8172, -0.3581, -0.4622, -0.0857, -0.9556,  0.0445],\n",
       "        [ 0.2547,  0.7344,  0.8434, -0.3580,  0.5297, -0.9322,  0.8004,  0.3339,\n",
       "         -0.4490,  0.1905,  0.6011,  0.9659,  0.9464, -0.4746,  0.9557,  0.8940,\n",
       "         -0.3241, -0.6548,  0.9255, -0.8389,  0.4709,  0.3344, -0.4005,  0.7088,\n",
       "         -0.5764,  0.6074, -0.5086, -0.4841, -0.8506,  0.1230, -0.5096, -0.8173,\n",
       "         -0.4453, -0.2131,  0.6687, -0.3004, -0.7540, -0.0385,  0.5183,  0.4360,\n",
       "          0.5561,  0.6738, -0.4656, -0.6029,  0.7869,  0.9342,  0.3325,  0.3110,\n",
       "         -0.1449, -0.0705,  0.1397, -0.0889, -0.3431, -0.0939,  0.8416,  0.2498,\n",
       "          0.8542, -0.7149,  0.5641,  0.5308, -0.8786,  0.6440,  0.6129, -0.3789]],\n",
       "       grad_fn=<StackBackward>)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack((output_f, output_b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Same Answer Again! yayy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
