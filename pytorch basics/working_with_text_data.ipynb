{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representing text as vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"the cat sat on the mat\", \"the dog ate my homework\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ultimately our input data - the data which feeds into the model is a sentence, \n",
    "and our aim here is to represent this sentence on a word level or a character level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### one-hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get vocabulary\n",
    "index = 0\n",
    "vocab = dict()\n",
    "for sentence in sentences:\n",
    "    for word in sentence.split(\" \"):\n",
    "        if vocab.get(word) == None:\n",
    "            vocab[word] = index\n",
    "            index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 0,\n",
       " 'cat': 1,\n",
       " 'sat': 2,\n",
       " 'on': 3,\n",
       " 'mat': 4,\n",
       " 'dog': 5,\n",
       " 'ate': 6,\n",
       " 'my': 7,\n",
       " 'homework': 8}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 1, 0, 0, 0, 0]])\n",
      "torch.Size([6, 9])\n",
      "\n",
      "tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 1]])\n",
      "torch.Size([5, 9])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentences:\n",
    "    sentence = sentence.split()\n",
    "    sentence_length = len(sentence)\n",
    "    indices = torch.tensor([vocab[word] for word in sentence], dtype=torch.int64)\n",
    "    indices.unsqueeze_(1)\n",
    "    sentence_matrix = torch.zeros(sentence_length, vector_size, dtype=torch.int64)\n",
    "    sentence_matrix.scatter_(1, indices, 1)\n",
    "    \n",
    "    print(sentence_matrix)\n",
    "    print(sentence_matrix.shape)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Batching the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "         [0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "         [0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
      "         [0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
      "         [1, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "\n",
      "        [[1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "         [0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
      "         [0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0, 1]]])\n",
      "torch.Size([2, 5, 9])\n"
     ]
    }
   ],
   "source": [
    "# generally all the sentences should be of same length to batch the data\n",
    "max_sentence_length = 5 # i.e 5 words per sentence at max\n",
    "result = torch.zeros((len(sentences), max_sentence_length, vector_size), # no. of sentences,\n",
    "                    dtype=torch.int64)                                   # no. of words in a sentence\n",
    "                                                                         # size of the word vector\n",
    "                                                                       \n",
    "        \n",
    "for i, sentence in enumerate(sentences):\n",
    "    sentence = sentence.split()\n",
    "    sentence = sentence[:max_sentence_length] # clipping the sentence to max_sentence_length\n",
    "    for j, word in enumerate(sentence):\n",
    "        index = vocab.get(word)\n",
    "        result[i, j, index] = 1\n",
    "        \n",
    "\n",
    "print(result)\n",
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Character level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]]])\n",
      "torch.Size([2, 50, 128])\n"
     ]
    }
   ],
   "source": [
    "max_sentence_length = 50 # i.e no more than 50 charcters per sentence\n",
    "vector_size = 128 # ascii characters\n",
    "result = torch.zeros((len(sentences), max_sentence_length, vector_size), # no. of sentences,\n",
    "                     dtype=torch.int64)                                  # no. of characters in each sentence\n",
    "                                                                         # size of each character vector\n",
    "    \n",
    "for i, sentence in enumerate(sentences):\n",
    "    for j, character in enumerate(sentence):\n",
    "        if character not in string.printable:\n",
    "            continue\n",
    "        index = ord(character)\n",
    "        result[i, j, index] = 1\n",
    "        \n",
    "print(result)\n",
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Word Level one hot hashing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we don't create the vocabulary in the beginning.\n",
    "We run every word through a hash function and map it to a number between 1 and dimensionality.\n",
    "but the problem here is that every word will not have a unique hash value which might confuse the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]]])\n",
      "torch.Size([2, 10, 1000])\n"
     ]
    }
   ],
   "source": [
    "max_sentence_length = 10 # maximum of 10 words per sentence\n",
    "dimensionality = 1000 # size of word vector\n",
    "\n",
    "result = torch.zeros((len(sentences), max_sentence_length, dimensionality), # no. of sentences,\n",
    "                    dtype=torch.int64)                                      # no. of words in a sentence\n",
    "                                                                            # size of the word vector\n",
    "                                                                       \n",
    "        \n",
    "for i, sentence in enumerate(sentences):\n",
    "    sentence = sentence.split()\n",
    "    sentence = sentence[:max_sentence_length] # clipping the sentence to max_sentence_length\n",
    "    for j, word in enumerate(sentence):\n",
    "        index = abs(hash(word)) % dimensionality\n",
    "        result[i, j, index] = 1\n",
    "        \n",
    "print(result)\n",
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Embedding layer is just like a normal linear layer whose weights are updated during training,\n",
    "except that the weights represent word vectors and can be accessed via index of the word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = nn.Embedding(10, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 128])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get vector of word whose index is 1\n",
    "embedding_layer(torch.tensor([1])).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### N-Gram language modeling"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Here, given a context or history of two previous words, we try to predict the next word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_SIZE = 2\n",
    "EMBEDDING_DIM = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use Shakespeare Sonnet 2\n",
    "test_sentence = \"\"\"When forty winters shall besiege thy brow,\n",
    "And dig deep trenches in thy beauty's field,\n",
    "Thy youth's proud livery so gazed on now,\n",
    "Will be a totter'd weed of small worth held:\n",
    "Then being asked, where all thy beauty lies,\n",
    "Where all the treasure of thy lusty days;\n",
    "To say, within thine own deep sunken eyes,\n",
    "Were an all-eating shame, and thriftless praise.\n",
    "How much more praise deserv'd thy beauty's use,\n",
    "If thou couldst answer 'This fair child of mine\n",
    "Shall sum my count, and make my old excuse,'\n",
    "Proving his beauty by succession thine!\n",
    "This were to be new made when thou art old,\n",
    "And see thy blood warm when thou feel'st it cold.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = WordPunctTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = tokenizer.tokenize(test_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams = [([words[i], words[i+1]], words[i+2]) for i in range(len(words)-2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['When', 'forty'], 'winters'),\n",
       " (['forty', 'winters'], 'shall'),\n",
       " (['winters', 'shall'], 'besiege'),\n",
       " (['shall', 'besiege'], 'thy'),\n",
       " (['besiege', 'thy'], 'brow'),\n",
       " (['thy', 'brow'], ',')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigrams[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = {word: i for i, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Building the model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "If you think about it, it's just a category prediction problem where the number of categories = vocab size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramLanguageModeler(nn.Module):\n",
    "    def __init__(self, CONTEXT_SIZE, EMBEDDING_DIM, VOCAB_SIZE):\n",
    "        super().__init__()\n",
    "        self.CONTEXT_SIZE = CONTEXT_SIZE\n",
    "        self.EMBEDDING_DIM = EMBEDDING_DIM\n",
    "        self.embeddings = nn.Embedding(VOCAB_SIZE, EMBEDDING_DIM)\n",
    "        self.linear1 = nn.Linear(CONTEXT_SIZE*EMBEDDING_DIM, 128)\n",
    "        self.linear2 = nn.Linear(128, VOCAB_SIZE)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        if inputs.shape[0] != self.CONTEXT_SIZE:\n",
    "            raise ValueError(f\"The context size should be of size {self.CONTEXT_SIZE}\")\n",
    "        word_vectors = self.embeddings(inputs)\n",
    "        # we flatten the inputs before passing onto the linear layer\n",
    "        word_vectors = word_vectors.view(-1, self.CONTEXT_SIZE * self.EMBEDDING_DIM)\n",
    "        out = F.relu(self.linear1(word_vectors))\n",
    "        logits = self.linear2(out)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NGramLanguageModeler(CONTEXT_SIZE, EMBEDDING_DIM, VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_list = []\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50/50 [00:04<00:00, 11.18it/s]\n"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(n_epochs)):\n",
    "    loss_per_epoch = 0\n",
    "    for context, target in trigrams:\n",
    "        context_indices = torch.tensor([word2idx[word] for word in context], dtype=torch.int64)\n",
    "        out = model(context_indices)\n",
    "        loss = loss_fn(out, torch.tensor([word2idx[target]], dtype=torch.int64))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_per_epoch += loss.item()\n",
    "    loss_list.append(loss_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2343389baf0>]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAmTklEQVR4nO3dd3gVZfr/8fedhN5CCUhvIggIAbL0BBQUBAHha4FV14aIy1JEV2V1XXW/u/v7utIVFAuiAooIiIUsbek1offeW0C6EASe3x8ZdqMGOJCEyTn5vK6LK3NmnnNyP5fycXzOzD3mnENEREJLmN8FiIhIxlO4i4iEIIW7iEgIUriLiIQghbuISAiK8LsAgGLFirkKFSr4XYaISFBJTEw87JyLSutYlgj3ChUqkJCQ4HcZIiJBxcx2Xu6YlmVEREKQwl1EJAQp3EVEQpDCXUQkBCncRURCkMJdRCQEKdxFREJQQOFuZpFmNt7MNpjZejNrZGZfmNkK788OM1vhja1gZmdSHXs3s4o/+9MFXpu8liOnkjPrV4iIBKVAb2IaDMQ75+4zs5xAXufcg5cOmll/4Hiq8Vudc9EZV2baVu4+xpglu5i69gDvPlKPWmUiM/tXiogEhaueuZtZQSAO+BDAOXfOOXcs1XEDHgDGZlKNl9WgUlHGd2+EmXHfuwsZt3T3jS5BRCRLCmRZphKQBIw0s+Vm9oGZ5Ut1PBY46JzbnGpfRW/sbDOLTetDzaybmSWYWUJSUtJ1T6BWmUi+6dmU+hWK8MJXq+g3YTXJ5y9c9+eJiISCQMI9AqgLDHfO1QFOAy+lOt6Fn5+17wfKeWP7AmO8s/+fcc6NcM7FOOdioqLS7HsTsCL5cjLqifo807wyY5fs4oH3FrHv2Jl0faaISDALJNz3AHucc4u91+NJCXvMLALoBHxxabBzLtk5d8TbTgS2ArdkZNFpCQ8zXmxdjXcfrsuWgydpN3QeC7YezuxfKyKSJV013J1zB4DdZlbV29UCWOdttwQ2OOf2XBpvZlFmFu5tVwKqANsytOoraF2zJF//oSmF8+Xk4Q8WM3zWVvQQcBHJbgK9zr0nMNrMVgHRwN+9/Z359RepccAqM1tJyll+d+fcDxlQa8BuLp6fST2acHfNkvxf/Aae/jSRE2d/upEliIj4yrLCWW1MTIzLjH7uzjk+nLedf0zZQLkieRn+cF2q3fSr5X8RkaBkZonOuZi0joX0HapmRtfYSox9qiGnks/T8Z0FTFq+1++yREQyXUiH+yX1Kxbhu55Nua10Ifp8sYJXv16jyyVFJKRli3AHKF4wN6OfakDXphX5ZOFOHnhvEXt1uaSIhKhsE+4AOcLDeOWe6rz7cF22HjrFPUPmMnvT9d9AJSKSVWWrcL+kdc2SfNOzKSUK5uaxkUsYMG0TFy76/8WyiEhGyZbhDlCxWD4m/r4JneqUYciMzTw2cgk/nD7nd1kiIhki24Y7QJ6c4bx1fy3+0ek2Fm//gbZD5pK486jfZYmIpFu2DndIuVyyS/1yTHimMRHhxoPvLeSDudt0V6uIBLVsH+6X1CxdiG97xnJHteL873freeazZbqrVUSClsI9lUJ5cvDeI/V4pe2tTF9/kHZD57Fm7/Grv1FEJItRuP/CpbtaP+/WkOSfLtJp+ALGLN6lZRoRCSoK98uIqVCE73o1pUHFIvxp4mqe/WIFp5PP+12WiEhAFO5XUDR/Lj5+vD5977yFySv30e7teWw4cMLvskRErkrhfhXhYUavFlX4rGsDTpw5z73vzGfc0t1aphGRLE3hHqDGlYvxfe+m1C1XmBe+WsVzX67kx3NaphGRrEnhfg2KF8jNp082oHeLKkxcvpf2b89n08GTfpclIvIrCvdrFB5mPHvnLXz2ZAOO/XiO9m/P0zKNiGQ5Cvfr1OTmYnzfK5Y6ZVOWafqOW6mraUQky1C4p0Pxgrn5rGsDnm15C1+v2Eu7t+exfr+uphER/ync0yk8zOjdsgqjuzbk5NnzdHhnvm56EhHfBRTuZhZpZuPNbIOZrTezRmb2mpntNbMV3p82qcb3M7MtZrbRzFplXvlZR6PKRZnSO/Y/Nz31HLuck+pNIyI+CfTMfTAQ75yrBtQG1nv7Bzrnor0/3wOYWXWgM1ADaA0MM7PwDK47SyqWPxejHq/PH1tVZcqaA7QdMo9Ve475XZaIZENXDXczKwjEAR8COOfOOeeOXeEtHYDPnXPJzrntwBagfgbUGhTCwowet9/MF90acv7CRf5n+AK1EBaRGy6QM/dKQBIw0syWm9kHZpbPO/YHM1tlZh+ZWWFvX2lgd6r37/H2/YyZdTOzBDNLSEoKveeYxlQowve9Y2leNaWFcNdRCXrSk4jcMIGEewRQFxjunKsDnAZeAoYDlYFoYD/Q3xtvaXzGr05bnXMjnHMxzrmYqKio6yg964vMm5MRj9TjtXbVmbv5MG0Gz2XxtiN+lyUi2UAg4b4H2OOcW+y9Hg/Udc4ddM5dcM5dBN7nv0sve4Cyqd5fBtiXUQUHGzPjsSYVmfD7xuTJGU6X9xcxaLoeyC0imeuq4e6cOwDsNrOq3q4WwDozK5lqWEdgjbc9GehsZrnMrCJQBViSgTUHpZqlC/FNz6bcG12aQdM30+X9Rew7dsbvskQkRAV6tUxPYLSZrSJlGebvwJtmttrbdzvwLIBzbi0wDlgHxAM9nHMXMrrwYJQ/VwQDHoxmwAO1WbP3OG2GzGXq2gN+lyUiIciywlUcMTExLiEhwe8ybqjth0/Tc+wy1uw9waONytOvza3kzpEtrhgVkQxiZonOuZi0jukOVZ9ULJaPr55pzBNNKjJq4U7ufWc+Ww6pw6SIZAyFu49yRYTzarvqfPRYDIdOJnPP0HmMXaLWBSKSfgr3LOCOaiWI7x1LvfKF6TdhNT3GLOP4j2pdICLXT+GeRRQvmJtPn2jAi62rMXXtQe4ePIelO37wuywRCVIK9ywkLMx4pnllxj/TmBwRYTz43kIGTtvE+QsX/S5NRIKMwj0Lii4bybc9m9IhujSDZ6RcE7/n6I9+lyUiQUThnkUVyJ2DgQ9GM/DB2qzff5K7B8/l21XZ9kZfEblGCvcsrmOdMnzfK5bKUfn5w5jl/PFLPc5PRK5O4R4EyhXNy5fdG9HzjpsZv2wPbYfMZeXuY36XJSJZmMI9SOQID+O5u6ry+VMNOXc+pU/8sFlb1IBMRNKkcA8yDSoVZUrvOFrVuIk34zfy0AdqQCYiv6ZwD0KF8ubg7d/W4Z/31WL1nuO0HjSH71bt97ssEclCFO5Bysy4P6Ys3/WKpVJUfnqMWcbzX67klL5sFREU7kGvQrF8fNm9Eb3uuJkJy/bQZvBclu066ndZIuIzhXsIyBEeRt+7qvLF0424cNFx/7sLGTRdd7aKZGcK9xDymwpFmNInlva1SzFo+mbuf28hO4+c9rssEfGBwj3EFPTubB3SpQ5bD52izeC5jEvYrTbCItmMwj1Eta9divg+cdQqE8kL41fxzGfLOHr6nN9licgNonAPYaUi8zC6awP+1KYaMzYcpNWgOczelOR3WSJyAyjcQ1xYmNEtrjKTejQhMm8OHv1oCX/5eg1nzumZ5SKhLKBwN7NIMxtvZhvMbL2ZNTKzf3qvV5nZRDOL9MZWMLMzZrbC+/Nups5AAlKjVCEm/6Hpf57Zes/Quazec9zvskQkkwR65j4YiHfOVQNqA+uBaUBN51wtYBPQL9X4rc65aO9P9wytWK5b7hwpz2z97MkGnE6+QMdh83l75mZdMikSgq4a7mZWEIgDPgRwzp1zzh1zzk11zl26HXIRUCbzypSM1LRKMeL7xNK65k28NXUTD45YpEsmRUJMIGfulYAkYKSZLTezD8ws3y/GPAFMSfW6ojd2tpnFpvWhZtbNzBLMLCEpSV/y3WiReXPy9m/rMrhzNJsOpjwMZOySXbpkUiREBBLuEUBdYLhzrg5wGnjp0kEzexk4D4z2du0Hynlj+wJjvLP/n3HOjXDOxTjnYqKiotI5DbleHaJL868+cUSXjaTfhNV0HZXAoZNn/S5LRNIpkHDfA+xxzi32Xo8nJewxs0eBe4CHnHfK55xLds4d8bYTga3ALRlduGScUpF5+OzJBrx6T3XmbTlMq4FziF+jLpMiweyq4e6cOwDsNrOq3q4WwDozaw28CLR3zv3n6c1mFmVm4d52JaAKsC3DK5cMFRZmPNG0It/1akqZwnnp/tky+o5bwYmzP/ldmohch4gAx/UERptZTlKC+nFgKZALmGZmAIu8K2PigDfM7DxwAejunPshwyuXTHFz8QJM+H1jhs7YzDuztrJo6xHeur82jW8u5ndpInINLCt8gRYTE+MSEhL8LkN+Yfmuozw3biXbDp/mscYVeLF1NfLkDPe7LBHxmFmicy4mrWO6Q1Uuq065wnzXK5bHGlfg4wU7aDtkLsvVK14kKCjc5Yry5AzntfY1GN21AWd/usD/DF9A/6kbOXdeNz6JZGUKdwlIk5uLEf9sHB3rlGHozC3c+858Nhw44XdZInIZCncJWMHcOej/QG3ee6Qeh06epd3QeQybtUXtC0SyIIW7XLNWNW7iX33iaHlrCd6M38j97y1kW9Ipv8sSkVQU7nJdiubPxbCHUtoXbEs6TZshc/lo3nYuXvT/6isRUbhLOpgZHaJLM+3ZOBpXLsYb366jy/uL2HXkx6u/WUQylcJd0q14wdx8+GgMb/5PLdbtO0HrwXP4dNFOncWL+EjhLhnCzHjgN2WJfzaOeuUL8+dJa3jko8XsOaqzeBE/KNwlQ5WOzMMnT9Tn7x1vY8WuY7QeNJfP1UpY5IZTuEuGMzN+26Ac8X3iuK10IV6asJpHRy5l37Ezfpcmkm0o3CXTlC2Sl9FdG/BGhxos3f4DrQbO4YulOosXuREU7pKpwsKM3zWqwL/6xFGjdEFe/Epn8SI3gsJdbohyRfMypmtD3uhQg4QdP3DXwDlaixfJRAp3uWEuncXH946jZumCvDRhNb/7aAl7dRYvkuEU7nLDXTqL/2uHGiTuPMpdA2bzma6LF8lQCnfxRViY8Yi3Fl+nXGFembSGhz5YrLtbRTKIwl18VbZIXj59sj7/6HQbq/cep9WgOXw8Xz1qRNJL4S6+MzO61C/H1GfjaFCpCK99s47OIxap06RIOijcJcsoFZmHkY/9hrfur82GAye4e/Bc3pu9Vf3iRa5DQOFuZpFmNt7MNpjZejNrZGZFzGyamW32fhZONb6fmW0xs41m1irzypdQY2bcV68M0/s2o3nVKP4xZQOdhi/QU59ErlGgZ+6DgXjnXDWgNrAeeAmY4ZyrAszwXmNm1YHOQA2gNTDMzMIzunAJbcUL5ubdh+vxzm/rsvfoGdoNncfAaZv07FaRAF013M2sIBAHfAjgnDvnnDsGdABGecNGAfd62x2Az51zyc657cAWoH7Gli3ZgZnRtlZJpvVtRtvbSjJ4xmbaDZ3Hit3H/C5NJMsL5My9EpAEjDSz5Wb2gZnlA0o45/YDeD+Le+NLA7tTvX+Pt+9nzKybmSWYWUJSUlK6JiGhrUi+nAzqXIePHovh+Jmf6DRsPn/9dh0/njvvd2kiWVYg4R4B1AWGO+fqAKfxlmAuw9LY96vr2pxzI5xzMc65mKioqICKleztjmolmNo3ji71y/HhvO20GjSHeZsP+12WSJYUSLjvAfY45xZ7r8eTEvYHzawkgPfzUKrxZVO9vwywL2PKleyuYO4c/K3jbXzerSERYWE8/OFi/vjlSo7/+JPfpYlkKVcNd+fcAWC3mVX1drUA1gGTgUe9fY8CX3vbk4HOZpbLzCoCVYAlGVq1ZHsNKxVlSu9YnmlemQnL99JiwGy+X71fjchEPBbIXwYziwY+AHIC24DHSfkPwzigHLALuN8594M3/mXgCeA80Mc5N+VKnx8TE+MSEhKufxaSra3Ze5wXv1rF2n0nuKt6Cd7oUJObCuX2uyyRTGdmic65mDSPZYUzHYW7pNf5Cxf5cN52BkzbRM7wMF68uxq/rV+OsLC0vgISCQ1XCnfdoSohISI8jKebVWbqs3HUKluIVyat4cERC9lySC0MJHtSuEtIKV80H5892YB/3leLTQdP0WbwXIbM2KybnyTbUbhLyDEz7o8py/S+zbirRgkGTNtE2yFzSdz5g9+lidwwCncJWVEFcvH2b+vy0WMx/HjuAve9u5BXJq3mxFldNimhT+EuIe+OaiWY+mwcjzeuyJjFu7hzwGzi1xzwuyyRTKVwl2whX64IXm1XnYm/b0KRfLno/lkiT32SwP7jen6rhCaFu2QrtctGMvkPTeh3dzXmbk6iZf/ZjJy/nQt68pOEGIW7ZDs5Ll022acZ9SoU4fVv1tFp2HzW7jvud2kiGUbhLtlWuaJ5GfX4bxjcOZq9x87Q/u35/P379eo2KSFB4S7ZmpnRIbo00/s244GYMoyYs407B8zh3xsOXf3NIlmYwl0EiMybk390qsW4pxuRJ2c4j3+8lN+PTuTgibN+lyZyXRTuIqnUr1iE73vF8sdWVZmx/hAt+s9m1IId+sJVgo7CXeQXckaE0eP2m5n6bBx1ykXyl8lr6TRsPmv26gtXCR4Kd5HLKF80H588Ud/7wvUs7d+ex1+/XcepZH3hKlmfwl3kCi594TrjuWZ0qV+Oj+Zvp2X/2cSv0YNBJGtTuIsEoFCelMf7ffVMYwrny0n3z5bx5KgEdv/wo9+liaRJ4S5yDeqWK8w3f2jCK21vZdG2I9w5cDbDZm1RS2HJchTuItcoIjyMrrGVmN63Gc1vKc6b8RtpO2Qui7Yd8bs0kf9QuItcp1KReXj3kXp8+GgMZ366QOcRi+g7bgWHTyX7XZqIwl0kvVrcWoJpzzajx+2V+WblPlr0n83oxTu5qGvjxUcBhbuZ7TCz1Wa2wswSvH1feK9XeMdXePsrmNmZVMfezcT6RbKEPDnD+WOrakzpHcutJQvw8sQ1dBy+QNfGi28irmHs7c65w5deOOcevLRtZv2B1P8Wb3XORae/PJHgcnPxAox9qiGTVuzlb9+tp/3b8/hdowr0vesWCubO4Xd5ko2ke1nGzAx4ABib/nJEgp+Z0bFOGWY815yHG5Zn1MId3PHWbCYt36tr4+WGCTTcHTDVzBLNrNsvjsUCB51zm1Ptq2hmy81stpnFpvWBZtbNzBLMLCEpKek6ShfJ2grlycEbHWoyuUdTSkfmps8XK+jy/iI2Hzzpd2mSDVggZxJmVso5t8/MigPTgJ7OuTneseHAFudcf+91LiC/c+6ImdUDJgE1nHMnLvf5MTExLiEhIf2zEcmiLlx0fL50F2/Gb+R08nmejK1IrzuqkC/XtayMivycmSU652LSOhbQmbtzbp/38xAwEajvfXAE0An4ItXYZOfcEW87EdgK3JKeCYgEu/Aw46EG5Zn5XDM61inNe7O3ceeA2UxZrTYGkjmuGu5mls/MClzaBu4C1niHWwIbnHN7Uo2PMrNwb7sSUAXYltGFiwSjovlz8c/7azO+eyMK5snBM6OX8ejIpWw/fNrv0iTEBHLmXgKYZ2YrgSXAd865eO9YZ379RWocsMobPx7o7pz7IaMKFgkFMRWK8G3Pprx6T3WW7TxKq4FzGDB1I2d/uuB3aRIiAlpzz2xac5fs7NCJs/zt+/V8vWIfZQrn4bV2NWhZvYTfZUkQSPeau4hknuIFczO4cx3GPtWQPDnC6fpJAk9+vJRdR9RxUq6fwl0ki2hUuSjf947l5TYpHSdbDpzNwGmbtFQj10XhLpKF5AgP46m4Ssx4rjmtatzE4BmbuXPgbGasP+h3aRJkFO4iWdBNhXIztEsdxjzVgFwR4Tw5Sks1cm0U7iJZWOPKxfi+Vyz97q7GQm+pZoCWaiQACneRLC5nRBhPN6vMzOea07rGTQyZsZmWA2Yzde0B3QAll6VwFwkSNxXKzZAuKVfV5M0ZTrdPE3n846Xs0A1QkgaFu0iQaVS5KN/1iuWVtreSsOModw2cw1v/2siZc1qqkf9SuIsEoRzec1xnPteMtrVK8va/t9BSvWokFYW7SBArXjA3Ax+MZtzTjSiQO4JnRi/jdx8tYcuhU36XJj5TuIuEgPoVU3rVvN6+Bit2H+PuwXP4x5T1nEo+73dp4hOFu0iIiAgP49HGFfj38825NzqlrXCL/rP4eoWeAJUdKdxFQkwxr63whN83pniB3PT+fAUPjljE+v2XfV6OhCCFu0iIqluuMJN6NOEfnW5j88GTtB0yl9cmr+X4mZ/8Lk1uAIW7SAgLDzO61C/Hv59vzkMNyvPJwh3c8dYsxi3dzcWLWqoJZQp3kWwgMm9O/npvTb7p2ZSKxfLxwler6Dh8ASt3H/O7NMkkCneRbKRGqUJ82b0RAx+szb5jZ7h32Hxe+moVR04l+12aZDCFu0g2Y2Z0rFOGmc81o2vTioxP3MPtb81i1IIdnL9w0e/yJIMo3EWyqQK5c/By2+rE94mlVplI/jJ5LfcMncfibUf8Lk0ygMJdJJu7uXgBPn2yPsMfqsvJs+d5cMQieo1dzoHjZ/0uTdIhoHA3sx1mttrMVphZgrfvNTPb6+1bYWZtUo3vZ2ZbzGyjmbXKrOJFJGOYGXffVpLpfZvRq0UV4tce4I7+sxg+ayvJ59WQLBhZIHeumdkOIMY5dzjVvteAU865t34xtjowFqgPlAKmA7c45y77b0hMTIxLSEi4nvpFJBPsOvIjf/1uHdPWHaRisXz8pV11mlct7ndZ8gtmluici0nrWGYsy3QAPnfOJTvntgNbSAl6EQkS5Yrm5f3fxfDx47/BgMdGLqXrqAQ95i+IBBruDphqZolm1i3V/j+Y2Soz+8jMCnv7SgO7U43Z4+37GTPrZmYJZpaQlJR0XcWLSOZqXrU48X3ieOnuaizcepiWA2fTf6p6xweDQMO9iXOuLnA30MPM4oDhQGUgGtgP9PfGWhrv/9Xaj3NuhHMuxjkXExUVdc2Fi8iNkTMijO7NKjPz+ea0qXkTQ2duoUX/WXy3Sr3js7KAwt05t8/7eQiYCNR3zh10zl1wzl0E3ue/Sy97gLKp3l4G2JdxJYuIH0oUzM2gznX4snsjCuXNSY8xy3jog8VsOnjS79IkDVcNdzPLZ2YFLm0DdwFrzKxkqmEdgTXe9mSgs5nlMrOKQBVgScaWLSJ++U2FlN7xf723Jmv3neDuwXN5/Rs1JMtqIgIYUwKYaGaXxo9xzsWb2admFk3KkssO4GkA59xaMxsHrAPOAz2udKWMiASf8DDjkYblaXtbSd6aupGPF+zgm5X7eKFVNe6rV4awsLRWZ+VGCuhSyMymSyFFgtuavcf5y+S1JO48Su2ykbzevgbRZSP9Livk3ehLIUUkm6lZuhDjuzdiwANeQ7J35vPi+FUcVkMy3yjcRSRDmBmd6qY0JOsWV4mvlqU0JBs5f7sakvlA4S4iGapA7hz8qc2txPeJJbpsJK9/s462Q+axcKsakt1ICncRyRQ3Fy/AJ0/U592H63H63Hm6vL+IHmOWse/YGb9LyxYU7iKSacyM1jVvYnrfZjzb8hamrztIi/6zeXvmZs7+pIvoMpPCXUQyXe4c4fRuWYXpfZvR7JYo3pq6ibsGzmHG+oN+lxayFO4icsOULZKXdx+px2dPNiBnRBhPjkrg8ZFL2H74tN+lhRyFu4jccE2rFGNK71heaXsrS3ccpdXAOfxf/AZOJ5/3u7SQoXAXEV/kCA+ja2wlZj7fjHa1SzF81lZa9J/N5JX71JAsAyjcRcRXxQvkpv8DtfnqmcZEFchFr7HL6TxiEev3n/C7tKCmcBeRLKFe+cJM6tGEv3e8jU0HT9J2yFz+8vUajv+ohmTXQ+EuIllGeJjx2wbl+PfzzXm4YXk+XbST2/vP4vMlu7h4UUs110LhLiJZTmTenLzRoSbf9GxK5ah8vDRhNR2HzWfF7mN+lxY0FO4ikmXVKFWIcU83YtCD0ew/fpZ735nPC+NXqiFZABTuIpKlmRn31inNzOeb83RcJSYs26uGZAFQuItIUMifK4J+bW4lvk+cGpIFQOEuIkHl5uL5/9OQ7FRySkOynmOXs/+4GpKlpnAXkaCTuiFZrxZV+NfaA7ToP5ths7aQfF4NyUDhLiJBLE/OcPreeQvTn21Gk5uL8Wb8RloPmsusjYf8Ls13CncRCXrliubl/d/F8PHjvwHgsZFLeeqTBHb/8KPPlfknoHA3sx1mttrMVphZgrfvn2a2wcxWmdlEM4v09lcwszPe2BVm9m4m1i8i8h/NqxYnvk8sL7auxvwth2kxYDYDp23Klr3jr+XM/XbnXHSqJ21PA2o652oBm4B+qcZu9cZGO+e6Z1SxIiJXkysinGeaV2bGc81oVeMmBs/YTMsBs/nX2gPZqiHZdS/LOOemOucu9edcBJTJmJJERNKvZKE8DO1Sh7FPNSRfzgie/jSRR0cuZWvSKb9LuyECDXcHTDWzRDPrlsbxJ4ApqV5XNLPlZjbbzGLT+kAz62ZmCWaWkJSUdI1li4gEplHlonzXqymv3lOd5TuP0nrQHP7flNDvHW+B/G+KmZVyzu0zs+KkLMf0dM7N8Y69DMQAnZxzzsxyAfmdc0fMrB4wCajhnLts/86YmBiXkJCQAdMREbm8pJPJ/F/8BsYn7uGmgrn5U9tbaVerJGbmd2nXxcwSUy2V/0xAZ+7OuX3ez0PARKC+98GPAvcADznvvxLOuWTn3BFvOxHYCtyS3kmIiKRXVIFcvHV/Su/4YgVy/qd3/MYDJ/0uLcNdNdzNLJ+ZFbi0DdwFrDGz1sCLQHvn3I+pxkeZWbi3XQmoAmzLjOJFRK5HvfKF+bpHU/7WsSYbD56kzZC5vP7NWk6cDZ3e8REBjCkBTPT+tyUCGOOcizezLUAuYJp3bJF3ZUwc8IaZnQcuAN2dcz9kSvUiItcpPMx4qEF52tQsyT+nbuTjBTv4ZuU+Xrr7VjrVKU1YWHAu1VwS0Jp7ZtOau4j4bdWeY7z69VpW7D5GvfKFeaNDDWqUKuR3WVeU7jV3EZFQV6tMJBOeacyb99Vix+HTtBs6jz9PCt7H/CncRUQ8YWHGAzFlmflccx5pWJ7Ri1Me8/fF0uB7zJ/CXUTkFwrlzcHrHWrybc9YKkfl48WvVtNx+AJW7Tnmd2kBU7iLiFxG9VIFGfd0IwY8UJu9R8/Q4Z35/Gniao6ePud3aVelcBcRuQIzo1PdMsx8vhmPN67IF0t3c3v/WYxevJMLWXipRuEuIhKAgrlz8Gq76nzfK5aqJQrw8sQ1dBw2nxW7j/ldWpoU7iIi16DqTQX4vFtDBneO5sDxs3QcNp+XvlrFkVPJfpf2Mwp3EZFrZGZ0iC7NzOeb81RsJcYn7uGO/rP5dOGOLLNUo3AXEblO+XNF8Kc2tzKldyzVSxbkz1+vpcM781i266jfpSncRUTSq0qJAox5qgFDu9Qh6WQynYYt4IXxK31dqlG4i4hkADOjXe1SzHiuOU/HVWLCsr3c/tYs35ZqFO4iIhkof64I+nlLNTVLF+LPX6+l/dvzSNx5Y5dqFO4iIpmgSokCjO6aslRz+FQy/zP8xi7VKNxFRDLJZZdqFmX+DVAKdxGRTJZ6qaZGqUL8edIaOrwzj+WZeFWNwl1E5Aa5dFXNkC51OHQimY7DFvC/367LlN8VyJOYREQkg5gZ7WuX4o5qxRk8fRNli+TNlN+jcBcR8UH+XBG83LZ6pn2+lmVEREKQwl1EJAQFFO5mtsPMVpvZCjNL8PYVMbNpZrbZ+1k41fh+ZrbFzDaaWavMKl5ERNJ2LWfutzvnolM9afslYIZzrgoww3uNmVUHOgM1gNbAMDMLz8CaRUTkKtKzLNMBGOVtjwLuTbX/c+dcsnNuO7AFqJ+O3yMiItco0HB3wFQzSzSzbt6+Es65/QDez+Le/tLA7lTv3ePt+xkz62ZmCWaWkJSUdH3Vi4hImgK9FLKJc26fmRUHppnZhiuMtTT2/eo+W+fcCGAEQExMTNbobi8iEiICOnN3zu3zfh4CJpKyzHLQzEoCeD8PecP3AGVTvb0MsC+jChYRkasz56580mxm+YAw59xJb3sa8AbQAjjinPt/ZvYSUMQ594KZ1QDGkPIfgFKkfNlaxTl34Qq/IwnYmY55FAMOp+P9wUrzzl407+wlkHmXd85FpXUgkGWZEsBEM7s0foxzLt7MlgLjzOxJYBdwP4Bzbq2ZjQPWAeeBHlcKdu89aRYXKDNLSHUVT7aheWcvmnf2kt55XzXcnXPbgNpp7D9Cytl7Wu/5G/C36y1KRETSR3eoioiEoFAJ9xF+F+ATzTt70byzl3TN+6pfqIqISPAJlTN3ERFJReEuIhKCgjrczay113lyi3etfUgys4/M7JCZrUm177JdOUOFmZU1s3+b2XozW2tmvb39IT13M8ttZkvMbKU379e9/SE970vMLNzMlpvZt97r7DLva+q+ezVBG+5ep8l3gLuB6kAXryNlKPqYlA6bqaXZlTPEnAeec87dCjQEenj/jEN97snAHc652kA00NrMGhL6876kN7A+1evsMm8IsPtuIII23Em5A3aLc26bc+4c8DkpHSlDjnNuDvDDL3ZfritnyHDO7XfOLfO2T5LyF740IT53l+KU9zKH98cR4vMGMLMyQFvgg1S7Q37eV3Ddcw/mcA+o+2QIu1xXzpBkZhWAOsBissHcvaWJFaT0bJrmnMsW8wYGAS8AF1Ptyw7zhmvrvntVwfyA7IC6T0rwM7P8wFdAH+fcCa8VRkjzWnZEm1kkKe0/avpcUqYzs3uAQ865RDNr7nM5friW7rtXFcxn7tm9++TlunKGFDPLQUqwj3bOTfB2Z4u5AzjnjgGzSPnOJdTn3QRob2Y7SFlmvcPMPiP05w1cc/fdqwrmcF8KVDGzimaWk5RH+032uaYbaTLwqLf9KPC1j7VkCks5Rf8QWO+cG5DqUEjP3cyivDN2zCwP0BLYQIjP2znXzzlXxjlXgZS/zzOdcw8T4vOGlO67Zlbg0jZwF7CGdMw9qO9QNbM2pKzRhQMfeQ3LQo6ZjQWak9IC9CDwF2ASMA4oh9eV0zn3yy9dg5qZNQXmAqv57xrsn0hZdw/ZuZtZLVK+PAsn5QRsnHPuDTMrSgjPOzVvWeZ559w92WHeZlaJlLN1+G/33b+lZ+5BHe4iIpK2YF6WERGRy1C4i4iEIIW7iEgIUriLiIQghbuISAhSuIuIhCCFu4hICPr/A2yuTrSV590AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(list(range(n_epochs)), loss_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple RNN with pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_vector_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = nn.Embedding(1000, input_vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequence = torch.tensor([1, 4, 12, 18, 230, 111, 11], dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating an RNN from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size = 64\n",
    "state = torch.zeros(state_size) # initializing the hidden state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wi = torch.randn((state_size, input_vector_size)) # transformation matrix of input xi\n",
    "Ws = torch.randn((state_size, state_size)) # transformation matrix of state\n",
    "b = torch.randn(state_size) # bias "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 128]), torch.Size([64, 64]), torch.Size([64]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Wi.shape, Ws.shape, b.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_tensor = embed(test_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_t = state\n",
    "for x in sequence_tensor:\n",
    "    output_t = torch.tanh(torch.matmul(Ws, state_t) + torch.matmul(Wi, x) + b)\n",
    "    state_t = output_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pytorch RNN Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = nn.RNN(input_vector_size, state_size, batch_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We'll compare pytorch rnn output and our output and check if they are same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using weights from pytorch's rnn layer\n",
    "Wi = rnn.weight_ih_l0\n",
    "bi = rnn.bias_ih_l0\n",
    "Wh = rnn.weight_hh_l0\n",
    "bh = rnn.bias_hh_l0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 128]),\n",
       " torch.Size([64]),\n",
       " torch.Size([64, 64]),\n",
       " torch.Size([64]))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Wi.shape, bi.shape, Wh.shape, bh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "h0 = state # initializing hidden state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### First let's test our custom rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_list = []\n",
    "state_t = state\n",
    "for x in sequence_tensor:\n",
    "    output_t = torch.tanh(torch.matmul(Wi, x) + bi + torch.matmul(Wh, state_t) +  bh)\n",
    "    state_t = output_t\n",
    "    states_list.append(state_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.5549,  0.8068,  0.7389, -0.5183,  0.9057, -0.7015, -0.5985, -0.3457,\n",
       "         0.2922,  0.9046,  0.0513,  0.5576, -0.5892, -0.1520,  0.8366,  0.1028,\n",
       "        -0.6041,  0.0480,  0.3224, -0.7450, -0.6623, -0.8614, -0.1351,  0.7046,\n",
       "        -0.5074, -0.8946,  0.9738, -0.2938,  0.5297,  0.1757, -0.7744, -0.0239,\n",
       "        -0.8277, -0.2251, -0.2563,  0.7254, -0.1132,  0.9066,  0.1143, -0.2577,\n",
       "         0.7632,  0.6573, -0.5984, -0.6507,  0.5404, -0.2503,  0.6906,  0.4851,\n",
       "        -0.1277, -0.2493, -0.7632,  0.3178,  0.4480,  0.8027, -0.5846, -0.8174,\n",
       "         0.1661,  0.2434, -0.8176,  0.2816, -0.1396, -0.5181,  0.6943, -0.6803],\n",
       "       grad_fn=<TanhBackward>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_t # final output of the last timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([ 0.1395,  0.9356,  0.0863, -0.2411, -0.6047,  0.7912,  0.0396, -0.5374,\n",
       "         -0.5039, -0.7114,  0.6759,  0.3823,  0.7369,  0.2703, -0.4207, -0.0780,\n",
       "          0.7747,  0.1778,  0.8449,  0.7537, -0.7819, -0.4907, -0.5894, -0.0015,\n",
       "         -0.0983,  0.2805, -0.1361,  0.7578,  0.2959, -0.5811,  0.7909,  0.2150,\n",
       "         -0.8929,  0.9694,  0.2769, -0.1751,  0.2835, -0.4321, -0.7670,  0.0440,\n",
       "         -0.2670, -0.1736, -0.6518, -0.2353, -0.1837, -0.3271,  0.2798,  0.6732,\n",
       "         -0.8962,  0.8046, -0.5638, -0.6226, -0.8434, -0.9279,  0.4406,  0.6008,\n",
       "         -0.9454, -0.2706,  0.1162, -0.2004,  0.0163,  0.6877, -0.1168, -0.8814],\n",
       "        grad_fn=<TanhBackward>),\n",
       " tensor([ 0.9854, -0.5265,  0.8053,  0.2958, -0.8472,  0.3888, -0.3276,  0.0700,\n",
       "          0.8150,  0.9106, -0.3973,  0.8119,  0.5252,  0.1653,  0.4559,  0.9212,\n",
       "          0.8079,  0.9578, -0.5743, -0.6430,  0.8470,  0.5389, -0.5913,  0.6171,\n",
       "          0.3691, -0.8232,  0.9012,  0.5869, -0.7986,  0.8912, -0.4070, -0.4244,\n",
       "          0.6345,  0.0227,  0.4036, -0.9283, -0.3158, -0.0158, -0.4701, -0.5338,\n",
       "          0.7261,  0.4970,  0.8598, -0.3090, -0.4279,  0.5346, -0.5722, -0.5163,\n",
       "          0.6854, -0.5046,  0.4585,  0.7487,  0.2127,  0.8419, -0.8187,  0.5603,\n",
       "         -0.3195,  0.7034,  0.1917,  0.7597,  0.7471,  0.3199,  0.5371, -0.8714],\n",
       "        grad_fn=<TanhBackward>),\n",
       " tensor([ 6.6730e-01, -5.3835e-01,  8.3080e-01,  4.4450e-01, -5.2244e-01,\n",
       "         -4.6605e-01, -2.1542e-01,  9.3641e-01, -2.2439e-01, -3.2340e-01,\n",
       "         -4.7701e-01,  3.2449e-01, -4.3021e-01,  7.6564e-01, -5.7898e-01,\n",
       "         -3.5501e-01, -2.0258e-02,  5.8871e-01, -8.0988e-01, -4.4189e-01,\n",
       "         -1.4982e-01,  2.2319e-01, -3.8250e-03,  6.7095e-01,  9.3201e-04,\n",
       "          2.2578e-01,  6.3075e-01,  4.0585e-01,  2.2122e-01, -5.3352e-01,\n",
       "          8.5084e-01, -9.1070e-02,  4.0846e-02,  7.6072e-01,  9.3988e-01,\n",
       "         -6.3194e-01, -4.7889e-01,  7.4234e-01,  3.7539e-01, -1.7493e-01,\n",
       "          8.8375e-01,  7.2965e-01, -2.2087e-01,  5.1127e-01, -1.3168e-01,\n",
       "         -9.5635e-01, -7.2142e-01,  4.6489e-01,  8.5566e-01,  7.8833e-01,\n",
       "          9.2372e-01,  9.9442e-02,  4.4079e-01,  5.8235e-01,  3.6011e-01,\n",
       "          6.4358e-01,  9.4456e-01,  6.5128e-01, -9.5349e-01,  3.5373e-01,\n",
       "          9.0539e-01,  7.1406e-01,  7.6703e-01, -8.1265e-01],\n",
       "        grad_fn=<TanhBackward>),\n",
       " tensor([ 0.0767,  0.3430, -0.6995, -0.8328, -0.5466,  0.1190, -0.1157, -0.8677,\n",
       "         -0.8850, -0.8742,  0.9135,  0.3776, -0.7774, -0.8225, -0.1038,  0.4489,\n",
       "         -0.5617,  0.6233,  0.0315,  0.2435, -0.5167, -0.9314,  0.2846,  0.8678,\n",
       "         -0.6315, -0.7175, -0.3460, -0.3654, -0.7199, -0.2419, -0.8133,  0.6201,\n",
       "         -0.0371,  0.8914,  0.3326, -0.0879,  0.1432, -0.8469, -0.6004, -0.9569,\n",
       "         -0.3994, -0.6742,  0.3978, -0.7444,  0.1539, -0.0667,  0.1848,  0.5303,\n",
       "          0.6453, -0.0249, -0.6885, -0.7052, -0.9793, -0.8372, -0.8917,  0.6356,\n",
       "          0.9167, -0.6841,  0.0881,  0.6303, -0.8417,  0.0389,  0.1804,  0.5301],\n",
       "        grad_fn=<TanhBackward>),\n",
       " tensor([ 0.2995, -0.2196, -0.2073,  0.6717, -0.8960, -0.7163,  0.0859, -0.7638,\n",
       "         -0.4556, -0.0122,  0.3372, -0.9803, -0.9701,  0.4434, -0.5396, -0.1310,\n",
       "          0.6071,  0.9230, -0.9920, -0.3315,  0.2983, -0.1202,  0.5443, -0.5503,\n",
       "         -0.7691,  0.5271,  0.7266,  0.6292,  0.8444, -0.9340, -0.9534, -0.1860,\n",
       "         -0.1616,  0.7497,  0.7151, -0.7066, -0.5957, -0.3533,  0.4299, -0.1216,\n",
       "          0.0331,  0.7938,  0.1534,  0.0901, -0.2731, -0.3255,  0.6970,  0.2155,\n",
       "          0.9336,  0.9724,  0.0969, -0.6411,  0.4811,  0.3353, -0.7845, -0.1071,\n",
       "          0.1051, -0.7225,  0.5392, -0.2865, -0.8095,  0.1859, -0.2024, -0.7058],\n",
       "        grad_fn=<TanhBackward>),\n",
       " tensor([-0.6984,  0.8929, -0.0364, -0.9728,  0.1318,  0.9769,  0.6520,  0.2328,\n",
       "         -0.1257, -0.4433, -0.4490,  0.2067,  0.1795,  0.0039,  0.3114,  0.7585,\n",
       "         -0.2478, -0.6069,  0.4320,  0.6706,  0.9096,  0.0640,  0.0058,  0.4082,\n",
       "         -0.1902, -0.4515,  0.8298, -0.3503, -0.2079,  0.8928,  0.4080, -0.3896,\n",
       "          0.6024,  0.8047,  0.1034,  0.4359,  0.7492, -0.7758, -0.7427, -0.5488,\n",
       "         -0.9607,  0.6041,  0.5508, -0.8284,  0.3674, -0.1906,  0.2048, -0.4475,\n",
       "         -0.3154,  0.8794,  0.7266, -0.3788,  0.3137, -0.3396, -0.9689,  0.8584,\n",
       "         -0.7688, -0.6858, -0.9628,  0.8991,  0.2212, -0.9857, -0.5563,  0.4988],\n",
       "        grad_fn=<TanhBackward>),\n",
       " tensor([ 0.5549,  0.8068,  0.7389, -0.5183,  0.9057, -0.7015, -0.5985, -0.3457,\n",
       "          0.2922,  0.9046,  0.0513,  0.5576, -0.5892, -0.1520,  0.8366,  0.1028,\n",
       "         -0.6041,  0.0480,  0.3224, -0.7450, -0.6623, -0.8614, -0.1351,  0.7046,\n",
       "         -0.5074, -0.8946,  0.9738, -0.2938,  0.5297,  0.1757, -0.7744, -0.0239,\n",
       "         -0.8277, -0.2251, -0.2563,  0.7254, -0.1132,  0.9066,  0.1143, -0.2577,\n",
       "          0.7632,  0.6573, -0.5984, -0.6507,  0.5404, -0.2503,  0.6906,  0.4851,\n",
       "         -0.1277, -0.2493, -0.7632,  0.3178,  0.4480,  0.8027, -0.5846, -0.8174,\n",
       "          0.1661,  0.2434, -0.8176,  0.2816, -0.1396, -0.5181,  0.6943, -0.6803],\n",
       "        grad_fn=<TanhBackward>)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states_list # a list containing intermediate states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now let's test the same with pytorch's rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "h0 = torch.zeros((1, 1, state_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 64])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_tensor_batch = sequence_tensor.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = rnn(sequence_tensor_batch, h0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.3946e-01,  9.3558e-01,  8.6308e-02, -2.4111e-01, -6.0473e-01,\n",
       "           7.9117e-01,  3.9635e-02, -5.3739e-01, -5.0388e-01, -7.1140e-01,\n",
       "           6.7591e-01,  3.8231e-01,  7.3693e-01,  2.7034e-01, -4.2068e-01,\n",
       "          -7.7975e-02,  7.7468e-01,  1.7776e-01,  8.4486e-01,  7.5375e-01,\n",
       "          -7.8194e-01, -4.9066e-01, -5.8937e-01, -1.5410e-03, -9.8270e-02,\n",
       "           2.8046e-01, -1.3606e-01,  7.5778e-01,  2.9595e-01, -5.8105e-01,\n",
       "           7.9091e-01,  2.1504e-01, -8.9293e-01,  9.6939e-01,  2.7694e-01,\n",
       "          -1.7514e-01,  2.8355e-01, -4.3210e-01, -7.6700e-01,  4.3962e-02,\n",
       "          -2.6700e-01, -1.7355e-01, -6.5176e-01, -2.3526e-01, -1.8370e-01,\n",
       "          -3.2710e-01,  2.7983e-01,  6.7319e-01, -8.9622e-01,  8.0461e-01,\n",
       "          -5.6385e-01, -6.2260e-01, -8.4339e-01, -9.2789e-01,  4.4061e-01,\n",
       "           6.0081e-01, -9.4543e-01, -2.7062e-01,  1.1615e-01, -2.0036e-01,\n",
       "           1.6323e-02,  6.8775e-01, -1.1679e-01, -8.8136e-01],\n",
       "         [ 9.8543e-01, -5.2649e-01,  8.0535e-01,  2.9583e-01, -8.4715e-01,\n",
       "           3.8879e-01, -3.2763e-01,  7.0043e-02,  8.1500e-01,  9.1059e-01,\n",
       "          -3.9733e-01,  8.1194e-01,  5.2523e-01,  1.6533e-01,  4.5594e-01,\n",
       "           9.2121e-01,  8.0792e-01,  9.5782e-01, -5.7434e-01, -6.4301e-01,\n",
       "           8.4703e-01,  5.3890e-01, -5.9134e-01,  6.1713e-01,  3.6905e-01,\n",
       "          -8.2317e-01,  9.0120e-01,  5.8691e-01, -7.9861e-01,  8.9117e-01,\n",
       "          -4.0695e-01, -4.2439e-01,  6.3445e-01,  2.2686e-02,  4.0360e-01,\n",
       "          -9.2831e-01, -3.1581e-01, -1.5760e-02, -4.7009e-01, -5.3379e-01,\n",
       "           7.2612e-01,  4.9705e-01,  8.5979e-01, -3.0901e-01, -4.2795e-01,\n",
       "           5.3459e-01, -5.7221e-01, -5.1630e-01,  6.8542e-01, -5.0458e-01,\n",
       "           4.5849e-01,  7.4866e-01,  2.1274e-01,  8.4191e-01, -8.1870e-01,\n",
       "           5.6033e-01, -3.1948e-01,  7.0339e-01,  1.9173e-01,  7.5971e-01,\n",
       "           7.4712e-01,  3.1992e-01,  5.3709e-01, -8.7136e-01],\n",
       "         [ 6.6730e-01, -5.3835e-01,  8.3080e-01,  4.4450e-01, -5.2244e-01,\n",
       "          -4.6605e-01, -2.1542e-01,  9.3641e-01, -2.2439e-01, -3.2340e-01,\n",
       "          -4.7701e-01,  3.2449e-01, -4.3021e-01,  7.6564e-01, -5.7898e-01,\n",
       "          -3.5501e-01, -2.0258e-02,  5.8871e-01, -8.0988e-01, -4.4189e-01,\n",
       "          -1.4982e-01,  2.2319e-01, -3.8250e-03,  6.7095e-01,  9.3198e-04,\n",
       "           2.2578e-01,  6.3075e-01,  4.0585e-01,  2.2122e-01, -5.3352e-01,\n",
       "           8.5084e-01, -9.1070e-02,  4.0846e-02,  7.6072e-01,  9.3988e-01,\n",
       "          -6.3194e-01, -4.7889e-01,  7.4234e-01,  3.7539e-01, -1.7493e-01,\n",
       "           8.8375e-01,  7.2965e-01, -2.2087e-01,  5.1127e-01, -1.3168e-01,\n",
       "          -9.5635e-01, -7.2142e-01,  4.6489e-01,  8.5566e-01,  7.8833e-01,\n",
       "           9.2372e-01,  9.9442e-02,  4.4079e-01,  5.8235e-01,  3.6011e-01,\n",
       "           6.4358e-01,  9.4456e-01,  6.5128e-01, -9.5349e-01,  3.5373e-01,\n",
       "           9.0539e-01,  7.1406e-01,  7.6703e-01, -8.1265e-01],\n",
       "         [ 7.6736e-02,  3.4302e-01, -6.9946e-01, -8.3282e-01, -5.4656e-01,\n",
       "           1.1897e-01, -1.1574e-01, -8.6771e-01, -8.8499e-01, -8.7418e-01,\n",
       "           9.1347e-01,  3.7760e-01, -7.7735e-01, -8.2253e-01, -1.0377e-01,\n",
       "           4.4887e-01, -5.6167e-01,  6.2335e-01,  3.1481e-02,  2.4353e-01,\n",
       "          -5.1672e-01, -9.3138e-01,  2.8463e-01,  8.6777e-01, -6.3150e-01,\n",
       "          -7.1750e-01, -3.4596e-01, -3.6542e-01, -7.1986e-01, -2.4190e-01,\n",
       "          -8.1326e-01,  6.2012e-01, -3.7115e-02,  8.9137e-01,  3.3261e-01,\n",
       "          -8.7934e-02,  1.4323e-01, -8.4690e-01, -6.0037e-01, -9.5689e-01,\n",
       "          -3.9938e-01, -6.7423e-01,  3.9780e-01, -7.4438e-01,  1.5393e-01,\n",
       "          -6.6733e-02,  1.8480e-01,  5.3034e-01,  6.4532e-01, -2.4915e-02,\n",
       "          -6.8853e-01, -7.0521e-01, -9.7929e-01, -8.3716e-01, -8.9174e-01,\n",
       "           6.3559e-01,  9.1667e-01, -6.8411e-01,  8.8097e-02,  6.3030e-01,\n",
       "          -8.4174e-01,  3.8863e-02,  1.8036e-01,  5.3013e-01],\n",
       "         [ 2.9946e-01, -2.1957e-01, -2.0727e-01,  6.7173e-01, -8.9605e-01,\n",
       "          -7.1633e-01,  8.5936e-02, -7.6378e-01, -4.5555e-01, -1.2242e-02,\n",
       "           3.3718e-01, -9.8034e-01, -9.7014e-01,  4.4342e-01, -5.3963e-01,\n",
       "          -1.3100e-01,  6.0713e-01,  9.2304e-01, -9.9200e-01, -3.3148e-01,\n",
       "           2.9828e-01, -1.2021e-01,  5.4430e-01, -5.5034e-01, -7.6910e-01,\n",
       "           5.2710e-01,  7.2665e-01,  6.2920e-01,  8.4441e-01, -9.3400e-01,\n",
       "          -9.5338e-01, -1.8599e-01, -1.6165e-01,  7.4969e-01,  7.1508e-01,\n",
       "          -7.0661e-01, -5.9571e-01, -3.5328e-01,  4.2987e-01, -1.2155e-01,\n",
       "           3.3125e-02,  7.9377e-01,  1.5335e-01,  9.0127e-02, -2.7311e-01,\n",
       "          -3.2545e-01,  6.9697e-01,  2.1551e-01,  9.3361e-01,  9.7245e-01,\n",
       "           9.6856e-02, -6.4106e-01,  4.8114e-01,  3.3529e-01, -7.8453e-01,\n",
       "          -1.0707e-01,  1.0515e-01, -7.2249e-01,  5.3916e-01, -2.8651e-01,\n",
       "          -8.0950e-01,  1.8594e-01, -2.0240e-01, -7.0576e-01],\n",
       "         [-6.9838e-01,  8.9287e-01, -3.6446e-02, -9.7283e-01,  1.3183e-01,\n",
       "           9.7692e-01,  6.5200e-01,  2.3283e-01, -1.2570e-01, -4.4330e-01,\n",
       "          -4.4901e-01,  2.0670e-01,  1.7950e-01,  3.8981e-03,  3.1144e-01,\n",
       "           7.5847e-01, -2.4782e-01, -6.0686e-01,  4.3205e-01,  6.7056e-01,\n",
       "           9.0957e-01,  6.3960e-02,  5.7793e-03,  4.0820e-01, -1.9021e-01,\n",
       "          -4.5151e-01,  8.2984e-01, -3.5026e-01, -2.0795e-01,  8.9277e-01,\n",
       "           4.0805e-01, -3.8957e-01,  6.0243e-01,  8.0466e-01,  1.0335e-01,\n",
       "           4.3590e-01,  7.4915e-01, -7.7576e-01, -7.4265e-01, -5.4882e-01,\n",
       "          -9.6070e-01,  6.0411e-01,  5.5080e-01, -8.2841e-01,  3.6736e-01,\n",
       "          -1.9060e-01,  2.0480e-01, -4.4751e-01, -3.1536e-01,  8.7939e-01,\n",
       "           7.2659e-01, -3.7881e-01,  3.1373e-01, -3.3962e-01, -9.6892e-01,\n",
       "           8.5843e-01, -7.6883e-01, -6.8581e-01, -9.6276e-01,  8.9910e-01,\n",
       "           2.2123e-01, -9.8572e-01, -5.5635e-01,  4.9883e-01],\n",
       "         [ 5.5492e-01,  8.0682e-01,  7.3893e-01, -5.1826e-01,  9.0571e-01,\n",
       "          -7.0150e-01, -5.9847e-01, -3.4571e-01,  2.9218e-01,  9.0461e-01,\n",
       "           5.1343e-02,  5.5761e-01, -5.8924e-01, -1.5203e-01,  8.3656e-01,\n",
       "           1.0277e-01, -6.0408e-01,  4.7992e-02,  3.2243e-01, -7.4499e-01,\n",
       "          -6.6229e-01, -8.6142e-01, -1.3512e-01,  7.0462e-01, -5.0736e-01,\n",
       "          -8.9457e-01,  9.7383e-01, -2.9382e-01,  5.2968e-01,  1.7570e-01,\n",
       "          -7.7442e-01, -2.3938e-02, -8.2770e-01, -2.2511e-01, -2.5631e-01,\n",
       "           7.2543e-01, -1.1323e-01,  9.0657e-01,  1.1425e-01, -2.5767e-01,\n",
       "           7.6325e-01,  6.5731e-01, -5.9844e-01, -6.5073e-01,  5.4044e-01,\n",
       "          -2.5031e-01,  6.9058e-01,  4.8510e-01, -1.2770e-01, -2.4933e-01,\n",
       "          -7.6321e-01,  3.1783e-01,  4.4802e-01,  8.0269e-01, -5.8458e-01,\n",
       "          -8.1735e-01,  1.6605e-01,  2.4340e-01, -8.1758e-01,  2.8156e-01,\n",
       "          -1.3962e-01, -5.1807e-01,  6.9427e-01, -6.8029e-01]]],\n",
       "       grad_fn=<TransposeBackward1>)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.5549,  0.8068,  0.7389, -0.5183,  0.9057, -0.7015, -0.5985,\n",
       "          -0.3457,  0.2922,  0.9046,  0.0513,  0.5576, -0.5892, -0.1520,\n",
       "           0.8366,  0.1028, -0.6041,  0.0480,  0.3224, -0.7450, -0.6623,\n",
       "          -0.8614, -0.1351,  0.7046, -0.5074, -0.8946,  0.9738, -0.2938,\n",
       "           0.5297,  0.1757, -0.7744, -0.0239, -0.8277, -0.2251, -0.2563,\n",
       "           0.7254, -0.1132,  0.9066,  0.1143, -0.2577,  0.7632,  0.6573,\n",
       "          -0.5984, -0.6507,  0.5404, -0.2503,  0.6906,  0.4851, -0.1277,\n",
       "          -0.2493, -0.7632,  0.3178,  0.4480,  0.8027, -0.5846, -0.8174,\n",
       "           0.1661,  0.2434, -0.8176,  0.2816, -0.1396, -0.5181,  0.6943,\n",
       "          -0.6803]]], grad_fn=<StackBackward>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Voila! Both the intermediate hidden states and the final output is the exact same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Let's do the same comparision with stacked (2 layer) RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_rnn = nn.RNN(input_vector_size, state_size, batch_first=True, num_layers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(128, 64, num_layers=2, batch_first=True)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Custom RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer 1\n",
    "Wi0 = stacked_rnn.weight_ih_l0\n",
    "bi0 = stacked_rnn.bias_ih_l0\n",
    "Wh0 = stacked_rnn.weight_hh_l0\n",
    "bh0 = stacked_rnn.bias_hh_l0\n",
    "\n",
    "# layer 2\n",
    "Wi1 = stacked_rnn.weight_ih_l1\n",
    "bi1 = stacked_rnn.bias_ih_l1\n",
    "Wh1 = stacked_rnn.weight_hh_l1\n",
    "bh1 = stacked_rnn.bias_hh_l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 128]) torch.Size([64]) torch.Size([64, 64]) torch.Size([64])\n",
      "torch.Size([64, 64]) torch.Size([64]) torch.Size([64, 64]) torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "print(Wi0.shape, bi0.shape, Wh0.shape, bh0.shape)\n",
    "print(Wi1.shape, bi1.shape, Wh1.shape, bh1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "h0 = torch.zeros(state_size)\n",
    "h1 = torch.zeros(state_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let's only compare the final output state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in sequence_tensor:\n",
    "    output_t0 = torch.tanh(torch.matmul(Wi0, x) + bi0 + torch.matmul(Wh0, h0) +  bh0)\n",
    "    h0 = output_t0\n",
    "    output_t1 = torch.tanh(torch.matmul(Wi1, output_t0) + bi1 + torch.matmul(Wh1, h1) +  bh1)\n",
    "    h1 = output_t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.9578,  0.7571, -0.8651,  0.1665, -0.8175,  0.3092,  0.7179, -0.7568,\n",
       "         0.4298,  0.3076,  0.6144, -0.0575,  0.9375,  0.9978, -0.0470,  0.7916,\n",
       "         0.1007, -0.3611,  0.4616,  0.8379, -0.5931,  0.4272,  0.3340,  0.8574,\n",
       "        -0.7391,  0.9032, -0.2280,  0.9635, -0.9190, -0.8977,  0.2729, -0.8435,\n",
       "         0.8402,  0.4345,  0.5399,  0.4688,  0.3675, -0.0247,  0.5308,  0.9949,\n",
       "         0.4029, -0.1893, -0.6556,  0.3070,  0.5281, -0.2568,  0.4367,  0.2242,\n",
       "         0.7138,  0.6359,  0.8539, -0.5524,  0.1838,  0.1632,  0.2134,  0.8493,\n",
       "        -0.6872,  0.8329,  0.4985,  0.5339, -0.9430,  0.2861,  0.1986,  0.1596],\n",
       "       grad_fn=<TanhBackward>)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.3800, -0.2105, -0.0488,  0.4090, -0.8054, -0.5435, -0.1308, -0.5152,\n",
       "        -0.4580,  0.7135,  0.5479,  0.4659, -0.4149, -0.0800,  0.2386, -0.4307,\n",
       "         0.3888, -0.0822, -0.0080, -0.2859, -0.5149,  0.2024,  0.1307, -0.0341,\n",
       "        -0.0572,  0.0569,  0.2115,  0.0829, -0.0316,  0.1362,  0.7339, -0.3099,\n",
       "         0.5562, -0.0174, -0.0894,  0.1531, -0.4546,  0.5583,  0.6179, -0.2832,\n",
       "        -0.3931, -0.3491, -0.6540,  0.5562,  0.3635,  0.4238,  0.4085,  0.6564,\n",
       "        -0.4809, -0.6669,  0.3754,  0.3388,  0.0602,  0.1189, -0.0230, -0.0062,\n",
       "        -0.0931, -0.5271, -0.7822,  0.4243,  0.2381, -0.5826, -0.4516,  0.1362],\n",
       "       grad_fn=<TanhBackward>)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_t1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pytorch's stacked RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing two hidden states: one for each layer\n",
    "h = torch.zeros((2, 1, state_size)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 64])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_tensor_batch = sequence_tensor.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = stacked_rnn(sequence_tensor_batch, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.9578,  0.7571, -0.8651,  0.1665, -0.8175,  0.3092,  0.7179,\n",
       "          -0.7568,  0.4298,  0.3076,  0.6144, -0.0575,  0.9375,  0.9978,\n",
       "          -0.0470,  0.7916,  0.1007, -0.3611,  0.4616,  0.8379, -0.5931,\n",
       "           0.4272,  0.3340,  0.8574, -0.7391,  0.9032, -0.2280,  0.9635,\n",
       "          -0.9190, -0.8977,  0.2729, -0.8435,  0.8402,  0.4345,  0.5399,\n",
       "           0.4688,  0.3675, -0.0247,  0.5308,  0.9949,  0.4029, -0.1893,\n",
       "          -0.6556,  0.3070,  0.5281, -0.2568,  0.4367,  0.2242,  0.7138,\n",
       "           0.6359,  0.8539, -0.5524,  0.1838,  0.1632,  0.2134,  0.8493,\n",
       "          -0.6872,  0.8329,  0.4985,  0.5339, -0.9430,  0.2861,  0.1986,\n",
       "           0.1596]],\n",
       "\n",
       "        [[ 0.3800, -0.2105, -0.0488,  0.4090, -0.8054, -0.5435, -0.1308,\n",
       "          -0.5152, -0.4580,  0.7135,  0.5479,  0.4659, -0.4149, -0.0800,\n",
       "           0.2386, -0.4307,  0.3888, -0.0822, -0.0080, -0.2859, -0.5149,\n",
       "           0.2024,  0.1307, -0.0341, -0.0572,  0.0569,  0.2115,  0.0829,\n",
       "          -0.0316,  0.1362,  0.7339, -0.3099,  0.5562, -0.0174, -0.0894,\n",
       "           0.1531, -0.4546,  0.5583,  0.6179, -0.2832, -0.3931, -0.3491,\n",
       "          -0.6540,  0.5562,  0.3635,  0.4238,  0.4085,  0.6564, -0.4809,\n",
       "          -0.6669,  0.3754,  0.3388,  0.0602,  0.1189, -0.0230, -0.0062,\n",
       "          -0.0931, -0.5271, -0.7822,  0.4243,  0.2381, -0.5826, -0.4516,\n",
       "           0.1362]]], grad_fn=<StackBackward>)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Voila! Both are the same again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
