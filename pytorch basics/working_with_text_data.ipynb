{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representing text as vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"the cat sat on the mat\", \"the dog ate my homework\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ultimately our input data - the data which feeds into the model is a sentence, \n",
    "and our aim here is to represent this sentence on a word level or a character level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### one-hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get vocabulary\n",
    "index = 0\n",
    "vocab = dict()\n",
    "for sentence in sentences:\n",
    "    for word in sentence.split(\" \"):\n",
    "        if vocab.get(word) == None:\n",
    "            vocab[word] = index\n",
    "            index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 0,\n",
       " 'cat': 1,\n",
       " 'sat': 2,\n",
       " 'on': 3,\n",
       " 'mat': 4,\n",
       " 'dog': 5,\n",
       " 'ate': 6,\n",
       " 'my': 7,\n",
       " 'homework': 8}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 1, 0, 0, 0, 0]])\n",
      "torch.Size([6, 9])\n",
      "\n",
      "tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 1]])\n",
      "torch.Size([5, 9])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentences:\n",
    "    sentence = sentence.split()\n",
    "    sentence_length = len(sentence)\n",
    "    indices = torch.tensor([vocab[word] for word in sentence], dtype=torch.int64)\n",
    "    indices.unsqueeze_(1)\n",
    "    sentence_matrix = torch.zeros(sentence_length, vector_size, dtype=torch.int64)\n",
    "    sentence_matrix.scatter_(1, indices, 1)\n",
    "    \n",
    "    print(sentence_matrix)\n",
    "    print(sentence_matrix.shape)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Batching the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "         [0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "         [0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
      "         [0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
      "         [1, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "\n",
      "        [[1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "         [0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
      "         [0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0, 1]]])\n",
      "torch.Size([2, 5, 9])\n"
     ]
    }
   ],
   "source": [
    "# generally all the sentences should be of same length to batch the data\n",
    "max_sentence_length = 5 # i.e 5 words per sentence at max\n",
    "result = torch.zeros((len(sentences), max_sentence_length, vector_size), # no. of sentences,\n",
    "                    dtype=torch.int64)                                   # no. of words in a sentence\n",
    "                                                                         # size of the word vector\n",
    "                                                                       \n",
    "        \n",
    "for i, sentence in enumerate(sentences):\n",
    "    sentence = sentence.split()\n",
    "    sentence = sentence[:max_sentence_length] # clipping the sentence to max_sentence_length\n",
    "    for j, word in enumerate(sentence):\n",
    "        index = vocab.get(word)\n",
    "        result[i, j, index] = 1\n",
    "        \n",
    "\n",
    "print(result)\n",
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Character level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]]])\n",
      "torch.Size([2, 50, 128])\n"
     ]
    }
   ],
   "source": [
    "max_sentence_length = 50 # i.e no more than 50 charcters per sentence\n",
    "vector_size = 128 # ascii characters\n",
    "result = torch.zeros((len(sentences), max_sentence_length, vector_size), # no. of sentences,\n",
    "                     dtype=torch.int64)                                  # no. of characters in each sentence\n",
    "                                                                         # size of each character vector\n",
    "    \n",
    "for i, sentence in enumerate(sentences):\n",
    "    for j, character in enumerate(sentence):\n",
    "        if character not in string.printable:\n",
    "            continue\n",
    "        index = ord(character)\n",
    "        result[i, j, index] = 1\n",
    "        \n",
    "print(result)\n",
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Word Level one hot hashing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we don't create the vocabulary in the beginning.\n",
    "We run every word through a hash function and map it to a number between 1 and dimensionality.\n",
    "but the problem here is that every word will not have a unique hash value which might confuse the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]]])\n",
      "torch.Size([2, 10, 1000])\n"
     ]
    }
   ],
   "source": [
    "max_sentence_length = 10 # maximum of 10 words per sentence\n",
    "dimensionality = 1000 # size of word vector\n",
    "\n",
    "result = torch.zeros((len(sentences), max_sentence_length, dimensionality), # no. of sentences,\n",
    "                    dtype=torch.int64)                                      # no. of words in a sentence\n",
    "                                                                            # size of the word vector\n",
    "                                                                       \n",
    "        \n",
    "for i, sentence in enumerate(sentences):\n",
    "    sentence = sentence.split()\n",
    "    sentence = sentence[:max_sentence_length] # clipping the sentence to max_sentence_length\n",
    "    for j, word in enumerate(sentence):\n",
    "        index = abs(hash(word)) % dimensionality\n",
    "        result[i, j, index] = 1\n",
    "        \n",
    "print(result)\n",
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Embedding layer is just like a normal linear layer whose weights are updated during training,\n",
    "except that the weights represent word vectors and can be accessed via index of the word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = nn.Embedding(10, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 128])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get vector of word whose index is 1\n",
    "embedding_layer(torch.tensor([1])).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### N-Gram language modeling"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Here, given a context or history of two previous words, we try to predict the next word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_SIZE = 2\n",
    "EMBEDDING_DIM = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use Shakespeare Sonnet 2\n",
    "test_sentence = \"\"\"When forty winters shall besiege thy brow,\n",
    "And dig deep trenches in thy beauty's field,\n",
    "Thy youth's proud livery so gazed on now,\n",
    "Will be a totter'd weed of small worth held:\n",
    "Then being asked, where all thy beauty lies,\n",
    "Where all the treasure of thy lusty days;\n",
    "To say, within thine own deep sunken eyes,\n",
    "Were an all-eating shame, and thriftless praise.\n",
    "How much more praise deserv'd thy beauty's use,\n",
    "If thou couldst answer 'This fair child of mine\n",
    "Shall sum my count, and make my old excuse,'\n",
    "Proving his beauty by succession thine!\n",
    "This were to be new made when thou art old,\n",
    "And see thy blood warm when thou feel'st it cold.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = WordPunctTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = tokenizer.tokenize(test_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams = [([words[i], words[i+1]], words[i+2]) for i in range(len(words)-2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['When', 'forty'], 'winters'),\n",
       " (['forty', 'winters'], 'shall'),\n",
       " (['winters', 'shall'], 'besiege'),\n",
       " (['shall', 'besiege'], 'thy'),\n",
       " (['besiege', 'thy'], 'brow'),\n",
       " (['thy', 'brow'], ',')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigrams[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = {word: i for i, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Building the model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "If you think about it, it's just a category prediction problem where the number of categories = vocab size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramLanguageModeler(nn.Module):\n",
    "    def __init__(self, CONTEXT_SIZE, EMBEDDING_DIM, VOCAB_SIZE):\n",
    "        super().__init__()\n",
    "        self.CONTEXT_SIZE = CONTEXT_SIZE\n",
    "        self.EMBEDDING_DIM = EMBEDDING_DIM\n",
    "        self.embeddings = nn.Embedding(VOCAB_SIZE, EMBEDDING_DIM)\n",
    "        self.linear1 = nn.Linear(CONTEXT_SIZE*EMBEDDING_DIM, 128)\n",
    "        self.linear2 = nn.Linear(128, VOCAB_SIZE)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        if inputs.shape[0] != self.CONTEXT_SIZE:\n",
    "            raise ValueError(f\"The context size should be of size {self.CONTEXT_SIZE}\")\n",
    "        word_vectors = self.embeddings(inputs)\n",
    "        # we flatten the inputs before passing onto the linear layer\n",
    "        word_vectors = word_vectors.view(-1, self.CONTEXT_SIZE * self.EMBEDDING_DIM)\n",
    "        out = F.relu(self.linear1(word_vectors))\n",
    "        logits = self.linear2(out)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NGramLanguageModeler(CONTEXT_SIZE, EMBEDDING_DIM, VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_list = []\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50/50 [00:04<00:00, 11.13it/s]\n"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(n_epochs)):\n",
    "    loss_per_epoch = 0\n",
    "    for context, target in trigrams:\n",
    "        context_indices = torch.tensor([word2idx[word] for word in context], dtype=torch.int64)\n",
    "        out = model(context_indices)\n",
    "        loss = loss_fn(out, torch.tensor([word2idx[target]], dtype=torch.int64))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_per_epoch += loss.item()\n",
    "    loss_list.append(loss_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x208af754220>]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(list(range(n_epochs)), loss_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple RNN with pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_vector_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = nn.Embedding(1000, input_vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequence = torch.tensor([1, 4, 12, 18, 230, 111, 11], dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating an RNN from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size = 64\n",
    "state = torch.zeros(state_size) # initializing the hidden state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wi = torch.randn((state_size, input_vector_size)) # transformation matrix of input xi\n",
    "Ws = torch.randn((state_size, state_size)) # transformation matrix of state\n",
    "b = torch.randn(state_size) # bias "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 128]), torch.Size([64, 64]), torch.Size([64]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Wi.shape, Ws.shape, b.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_tensor = embed(test_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_t = state\n",
    "for x in sequence_tensor:\n",
    "    output_t = torch.tanh(torch.matmul(Ws, state_t) + torch.matmul(Wi, x) + b)\n",
    "    state_t = output_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pytorch RNN Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = nn.RNN(input_vector_size, state_size, batch_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We'll compare pytorch rnn output and our output and check if they are same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using weights from pytorch's rnn layer\n",
    "Wi = rnn.weight_ih_l0\n",
    "bi = rnn.bias_ih_l0\n",
    "Wh = rnn.weight_hh_l0\n",
    "bh = rnn.bias_hh_l0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 128]),\n",
       " torch.Size([64]),\n",
       " torch.Size([64, 64]),\n",
       " torch.Size([64]))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Wi.shape, bi.shape, Wh.shape, bh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "h0 = state # initializing hidden state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### First let's test our custom rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_list = []\n",
    "state_t = state\n",
    "for x in sequence_tensor:\n",
    "    output_t = torch.tanh(torch.matmul(Wi, x) + bi + torch.matmul(Wh, state_t) +  bh)\n",
    "    state_t = output_t\n",
    "    states_list.append(state_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1538,  0.9347,  0.7296,  0.7717,  0.0053,  0.0352, -0.6093,  0.1462,\n",
       "        -0.6054, -0.8625, -0.7908,  0.9317, -0.7929, -0.1757,  0.8325,  0.4964,\n",
       "        -0.8150,  0.8474,  0.1338,  0.8385,  0.5589, -0.7716, -0.1686,  0.7162,\n",
       "        -0.9401, -0.8225, -0.6820,  0.4487,  0.2680,  0.2931, -0.5631,  0.7256,\n",
       "         0.8582,  0.7073, -0.3750, -0.3470, -0.2613, -0.4475,  0.3777, -0.2594,\n",
       "        -0.6955,  0.9514, -0.5481,  0.7562, -0.0182, -0.0740,  0.0812,  0.5066,\n",
       "        -0.1386,  0.5738, -0.3389, -0.6417,  0.8674,  0.4195, -0.8150, -0.3845,\n",
       "        -0.0085,  0.5888, -0.9176, -0.1380, -0.7844,  0.7922,  0.9049, -0.7361],\n",
       "       grad_fn=<TanhBackward>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_t # final output of the last timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([ 0.6902, -0.6426,  0.7828,  0.7758,  0.1088,  0.9638, -0.9412, -0.8770,\n",
       "         -0.3936, -0.1240, -0.0599,  0.9660,  0.3182, -0.8015, -0.3295, -0.1352,\n",
       "          0.1999, -0.4119, -0.5083,  0.8436,  0.6053,  0.4503, -0.2808,  0.7617,\n",
       "         -0.9603, -0.8197,  0.9601, -0.6824, -0.0191, -0.1181, -0.9241, -0.6260,\n",
       "          0.5252, -0.2527,  0.4631, -0.0237, -0.9413, -0.8928, -0.0671,  0.1388,\n",
       "         -0.6575,  0.3038,  0.0547,  0.2883, -0.7080, -0.8493,  0.4100,  0.4838,\n",
       "          0.2452, -0.3934,  0.1744, -0.6237, -0.7903,  0.1194, -0.0174, -0.9471,\n",
       "         -0.3420,  0.2886,  0.2486,  0.5731, -0.3336, -0.2695, -0.4414, -0.2608],\n",
       "        grad_fn=<TanhBackward>),\n",
       " tensor([ 0.3649,  0.7335,  0.6849, -0.3175, -0.7331, -0.6399, -0.5019,  0.2697,\n",
       "          0.9694, -0.2267, -0.1909, -0.7884, -0.2040,  0.9877,  0.1272, -0.1171,\n",
       "         -0.3121, -0.6973,  0.2080, -0.7877, -0.4868, -0.7071, -0.1992,  0.2603,\n",
       "          0.7421,  0.0181,  0.6090,  0.8957,  0.7196, -0.9174,  0.5055,  0.9671,\n",
       "          0.7301,  0.3850,  0.8765,  0.5651, -0.2275,  0.0524, -0.1979,  0.1871,\n",
       "          0.0865, -0.6154,  0.0188, -0.0240, -0.8591,  0.1066,  0.8226,  0.2821,\n",
       "         -0.0964, -0.8458,  0.1906,  0.1376,  0.6691, -0.3875,  0.8614,  0.5048,\n",
       "         -0.6859, -0.4128, -0.9233,  0.5121, -0.9109, -0.5113,  0.0702, -0.4305],\n",
       "        grad_fn=<TanhBackward>),\n",
       " tensor([-0.1792,  0.2503, -0.3461, -0.5725,  0.2383, -0.8712, -0.8482, -0.7012,\n",
       "         -0.6913,  0.3793,  0.5191,  0.0098, -0.2622,  0.5415, -0.2625, -0.5269,\n",
       "          0.5111,  0.2432,  0.1671, -0.3215, -0.0272,  0.6435,  0.2947,  0.3333,\n",
       "         -0.7911, -0.1584, -0.4875, -0.4019,  0.8892, -0.8148,  0.1259,  0.9682,\n",
       "         -0.7163, -0.5023, -0.1914,  0.6261,  0.7464,  0.6082,  0.5595,  0.7867,\n",
       "          0.2868,  0.7816,  0.5235, -0.2605,  0.1109,  0.6321,  0.8806, -0.2579,\n",
       "          0.1936,  0.7456,  0.7781, -0.1323, -0.3919, -0.6778, -0.0634, -0.2973,\n",
       "          0.7936, -0.2545,  0.1044, -0.1773,  0.6523,  0.5583, -0.4187,  0.3797],\n",
       "        grad_fn=<TanhBackward>),\n",
       " tensor([ 0.7049,  0.0881, -0.3299, -0.8336, -0.6621, -0.8821,  0.0704, -0.9009,\n",
       "          0.5658, -0.2228,  0.3351, -0.3068,  0.7841,  0.6958,  0.4184, -0.5345,\n",
       "          0.6127,  0.2487, -0.5443, -0.7270,  0.6407,  0.6534, -0.7185, -0.3919,\n",
       "         -0.9088, -0.4413, -0.6818,  0.0282, -0.1284, -0.3660,  0.0042,  0.3013,\n",
       "          0.0266, -0.9397, -0.7801, -0.2409,  0.2091, -0.8312, -0.2478,  0.8434,\n",
       "          0.3625,  0.4672,  0.7496,  0.6771, -0.1606,  0.8498,  0.5041,  0.5198,\n",
       "          0.8256,  0.4837, -0.8882, -0.2693, -0.8063, -0.7833, -0.5121,  0.9534,\n",
       "         -0.3291,  0.9141,  0.1188, -0.6712,  0.9390,  0.8924,  0.6841,  0.6805],\n",
       "        grad_fn=<TanhBackward>),\n",
       " tensor([ 0.3994,  0.9608, -0.4171,  0.6302,  0.3347,  0.7628,  0.5253, -0.9058,\n",
       "          0.6573, -0.3977,  0.8268,  0.8071, -0.4610,  0.1869,  0.5182, -0.6973,\n",
       "          0.5883, -0.7601,  0.6858,  0.6569, -0.5781,  0.1065, -0.2754, -0.8198,\n",
       "         -0.9742, -0.4454, -0.3526,  0.6489, -0.4141, -0.0738, -0.6521, -0.7185,\n",
       "         -0.2708,  0.1197,  0.5288, -0.1408, -0.6231,  0.3174,  0.5722,  0.4176,\n",
       "         -0.7736, -0.4456,  0.5983,  0.4739, -0.4358,  0.4858,  0.8483,  0.5701,\n",
       "          0.5943,  0.8726, -0.7164, -0.8497,  0.9579,  0.6629,  0.0803, -0.9810,\n",
       "          0.2215, -0.7778,  0.8670, -0.5533, -0.7613,  0.9606,  0.1141,  0.3865],\n",
       "        grad_fn=<TanhBackward>),\n",
       " tensor([ 9.5619e-01, -4.7351e-01,  3.4209e-01,  9.4596e-01,  5.4753e-01,\n",
       "          4.8100e-01,  1.8348e-02,  5.8643e-01,  9.8321e-01, -6.3301e-01,\n",
       "          5.3264e-01,  5.0265e-02, -2.6600e-01, -3.6843e-01,  8.0700e-01,\n",
       "          9.5043e-01,  3.5350e-01,  5.4900e-01, -3.6972e-01,  9.2740e-04,\n",
       "         -9.7415e-01, -2.4693e-01, -1.2662e-02, -8.0467e-01, -3.5146e-01,\n",
       "          3.3540e-01,  2.0857e-01, -9.9553e-01, -8.8355e-02, -9.6984e-01,\n",
       "          6.1916e-01, -2.5973e-01,  1.9970e-01, -2.6882e-01, -4.2229e-01,\n",
       "         -4.9154e-01,  3.4822e-01, -4.7259e-01,  8.9977e-01, -4.8687e-01,\n",
       "          8.3577e-01, -7.1840e-01,  5.3044e-01,  3.4717e-01,  1.9878e-01,\n",
       "          8.9973e-01, -3.9984e-01, -3.0934e-01,  8.2844e-01,  1.7665e-01,\n",
       "         -9.3239e-01, -5.0813e-01, -5.4819e-02, -6.3926e-01, -5.4625e-01,\n",
       "         -6.2807e-02,  1.3378e-01, -8.9398e-02, -5.9889e-01, -7.7928e-01,\n",
       "         -6.2541e-01, -8.3325e-01,  1.6513e-01,  6.9423e-01],\n",
       "        grad_fn=<TanhBackward>),\n",
       " tensor([-0.1538,  0.9347,  0.7296,  0.7717,  0.0053,  0.0352, -0.6093,  0.1462,\n",
       "         -0.6054, -0.8625, -0.7908,  0.9317, -0.7929, -0.1757,  0.8325,  0.4964,\n",
       "         -0.8150,  0.8474,  0.1338,  0.8385,  0.5589, -0.7716, -0.1686,  0.7162,\n",
       "         -0.9401, -0.8225, -0.6820,  0.4487,  0.2680,  0.2931, -0.5631,  0.7256,\n",
       "          0.8582,  0.7073, -0.3750, -0.3470, -0.2613, -0.4475,  0.3777, -0.2594,\n",
       "         -0.6955,  0.9514, -0.5481,  0.7562, -0.0182, -0.0740,  0.0812,  0.5066,\n",
       "         -0.1386,  0.5738, -0.3389, -0.6417,  0.8674,  0.4195, -0.8150, -0.3845,\n",
       "         -0.0085,  0.5888, -0.9176, -0.1380, -0.7844,  0.7922,  0.9049, -0.7361],\n",
       "        grad_fn=<TanhBackward>)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states_list # a list containing intermediate states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now let's test the same with pytorch's rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "h0 = torch.zeros((1, 1, state_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 64])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_tensor_batch = sequence_tensor.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = rnn(sequence_tensor_batch, h0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 6.9023e-01, -6.4258e-01,  7.8281e-01,  7.7581e-01,  1.0875e-01,\n",
       "           9.6378e-01, -9.4120e-01, -8.7700e-01, -3.9362e-01, -1.2398e-01,\n",
       "          -5.9866e-02,  9.6596e-01,  3.1819e-01, -8.0149e-01, -3.2952e-01,\n",
       "          -1.3519e-01,  1.9985e-01, -4.1187e-01, -5.0832e-01,  8.4365e-01,\n",
       "           6.0534e-01,  4.5029e-01, -2.8083e-01,  7.6172e-01, -9.6030e-01,\n",
       "          -8.1974e-01,  9.6014e-01, -6.8243e-01, -1.9146e-02, -1.1808e-01,\n",
       "          -9.2405e-01, -6.2597e-01,  5.2519e-01, -2.5274e-01,  4.6310e-01,\n",
       "          -2.3658e-02, -9.4129e-01, -8.9276e-01, -6.7115e-02,  1.3884e-01,\n",
       "          -6.5753e-01,  3.0383e-01,  5.4706e-02,  2.8832e-01, -7.0805e-01,\n",
       "          -8.4933e-01,  4.1005e-01,  4.8375e-01,  2.4520e-01, -3.9339e-01,\n",
       "           1.7442e-01, -6.2375e-01, -7.9031e-01,  1.1945e-01, -1.7373e-02,\n",
       "          -9.4712e-01, -3.4205e-01,  2.8865e-01,  2.4859e-01,  5.7308e-01,\n",
       "          -3.3358e-01, -2.6950e-01, -4.4139e-01, -2.6081e-01],\n",
       "         [ 3.6492e-01,  7.3348e-01,  6.8494e-01, -3.1746e-01, -7.3313e-01,\n",
       "          -6.3991e-01, -5.0186e-01,  2.6970e-01,  9.6944e-01, -2.2673e-01,\n",
       "          -1.9086e-01, -7.8843e-01, -2.0396e-01,  9.8766e-01,  1.2722e-01,\n",
       "          -1.1710e-01, -3.1214e-01, -6.9729e-01,  2.0801e-01, -7.8770e-01,\n",
       "          -4.8684e-01, -7.0713e-01, -1.9925e-01,  2.6026e-01,  7.4206e-01,\n",
       "           1.8128e-02,  6.0898e-01,  8.9566e-01,  7.1963e-01, -9.1742e-01,\n",
       "           5.0551e-01,  9.6708e-01,  7.3014e-01,  3.8495e-01,  8.7655e-01,\n",
       "           5.6513e-01, -2.2753e-01,  5.2411e-02, -1.9794e-01,  1.8709e-01,\n",
       "           8.6549e-02, -6.1538e-01,  1.8785e-02, -2.3972e-02, -8.5914e-01,\n",
       "           1.0661e-01,  8.2264e-01,  2.8212e-01, -9.6356e-02, -8.4583e-01,\n",
       "           1.9056e-01,  1.3765e-01,  6.6907e-01, -3.8751e-01,  8.6137e-01,\n",
       "           5.0482e-01, -6.8592e-01, -4.1281e-01, -9.2331e-01,  5.1214e-01,\n",
       "          -9.1086e-01, -5.1126e-01,  7.0176e-02, -4.3046e-01],\n",
       "         [-1.7924e-01,  2.5030e-01, -3.4609e-01, -5.7254e-01,  2.3831e-01,\n",
       "          -8.7122e-01, -8.4821e-01, -7.0117e-01, -6.9130e-01,  3.7929e-01,\n",
       "           5.1914e-01,  9.8466e-03, -2.6223e-01,  5.4154e-01, -2.6249e-01,\n",
       "          -5.2688e-01,  5.1112e-01,  2.4323e-01,  1.6708e-01, -3.2154e-01,\n",
       "          -2.7152e-02,  6.4352e-01,  2.9470e-01,  3.3334e-01, -7.9108e-01,\n",
       "          -1.5844e-01, -4.8751e-01, -4.0194e-01,  8.8924e-01, -8.1485e-01,\n",
       "           1.2594e-01,  9.6821e-01, -7.1631e-01, -5.0227e-01, -1.9136e-01,\n",
       "           6.2605e-01,  7.4638e-01,  6.0819e-01,  5.5947e-01,  7.8672e-01,\n",
       "           2.8677e-01,  7.8164e-01,  5.2351e-01, -2.6051e-01,  1.1092e-01,\n",
       "           6.3210e-01,  8.8056e-01, -2.5790e-01,  1.9358e-01,  7.4564e-01,\n",
       "           7.7806e-01, -1.3228e-01, -3.9194e-01, -6.7781e-01, -6.3386e-02,\n",
       "          -2.9726e-01,  7.9358e-01, -2.5454e-01,  1.0443e-01, -1.7726e-01,\n",
       "           6.5229e-01,  5.5832e-01, -4.1873e-01,  3.7971e-01],\n",
       "         [ 7.0489e-01,  8.8071e-02, -3.2991e-01, -8.3362e-01, -6.6211e-01,\n",
       "          -8.8208e-01,  7.0435e-02, -9.0092e-01,  5.6584e-01, -2.2276e-01,\n",
       "           3.3509e-01, -3.0676e-01,  7.8410e-01,  6.9585e-01,  4.1843e-01,\n",
       "          -5.3455e-01,  6.1272e-01,  2.4871e-01, -5.4433e-01, -7.2698e-01,\n",
       "           6.4068e-01,  6.5338e-01, -7.1850e-01, -3.9190e-01, -9.0878e-01,\n",
       "          -4.4127e-01, -6.8177e-01,  2.8207e-02, -1.2839e-01, -3.6596e-01,\n",
       "           4.2267e-03,  3.0127e-01,  2.6597e-02, -9.3969e-01, -7.8006e-01,\n",
       "          -2.4091e-01,  2.0905e-01, -8.3119e-01, -2.4778e-01,  8.4339e-01,\n",
       "           3.6255e-01,  4.6717e-01,  7.4955e-01,  6.7713e-01, -1.6056e-01,\n",
       "           8.4977e-01,  5.0414e-01,  5.1981e-01,  8.2555e-01,  4.8370e-01,\n",
       "          -8.8819e-01, -2.6932e-01, -8.0625e-01, -7.8334e-01, -5.1213e-01,\n",
       "           9.5345e-01, -3.2907e-01,  9.1412e-01,  1.1883e-01, -6.7120e-01,\n",
       "           9.3900e-01,  8.9237e-01,  6.8413e-01,  6.8053e-01],\n",
       "         [ 3.9943e-01,  9.6078e-01, -4.1709e-01,  6.3023e-01,  3.3471e-01,\n",
       "           7.6280e-01,  5.2526e-01, -9.0578e-01,  6.5734e-01, -3.9766e-01,\n",
       "           8.2676e-01,  8.0712e-01, -4.6101e-01,  1.8693e-01,  5.1819e-01,\n",
       "          -6.9733e-01,  5.8830e-01, -7.6008e-01,  6.8583e-01,  6.5687e-01,\n",
       "          -5.7808e-01,  1.0654e-01, -2.7540e-01, -8.1981e-01, -9.7424e-01,\n",
       "          -4.4536e-01, -3.5264e-01,  6.4891e-01, -4.1408e-01, -7.3798e-02,\n",
       "          -6.5210e-01, -7.1847e-01, -2.7077e-01,  1.1966e-01,  5.2881e-01,\n",
       "          -1.4084e-01, -6.2310e-01,  3.1735e-01,  5.7225e-01,  4.1760e-01,\n",
       "          -7.7360e-01, -4.4558e-01,  5.9832e-01,  4.7390e-01, -4.3584e-01,\n",
       "           4.8581e-01,  8.4826e-01,  5.7010e-01,  5.9435e-01,  8.7259e-01,\n",
       "          -7.1642e-01, -8.4968e-01,  9.5789e-01,  6.6289e-01,  8.0286e-02,\n",
       "          -9.8099e-01,  2.2154e-01, -7.7782e-01,  8.6702e-01, -5.5326e-01,\n",
       "          -7.6126e-01,  9.6056e-01,  1.1413e-01,  3.8654e-01],\n",
       "         [ 9.5619e-01, -4.7351e-01,  3.4209e-01,  9.4596e-01,  5.4753e-01,\n",
       "           4.8100e-01,  1.8348e-02,  5.8643e-01,  9.8321e-01, -6.3301e-01,\n",
       "           5.3264e-01,  5.0265e-02, -2.6600e-01, -3.6843e-01,  8.0700e-01,\n",
       "           9.5043e-01,  3.5350e-01,  5.4900e-01, -3.6972e-01,  9.2748e-04,\n",
       "          -9.7415e-01, -2.4693e-01, -1.2662e-02, -8.0467e-01, -3.5146e-01,\n",
       "           3.3540e-01,  2.0857e-01, -9.9553e-01, -8.8355e-02, -9.6984e-01,\n",
       "           6.1916e-01, -2.5973e-01,  1.9970e-01, -2.6882e-01, -4.2229e-01,\n",
       "          -4.9154e-01,  3.4822e-01, -4.7259e-01,  8.9977e-01, -4.8687e-01,\n",
       "           8.3577e-01, -7.1840e-01,  5.3044e-01,  3.4717e-01,  1.9878e-01,\n",
       "           8.9973e-01, -3.9984e-01, -3.0934e-01,  8.2844e-01,  1.7665e-01,\n",
       "          -9.3239e-01, -5.0813e-01, -5.4819e-02, -6.3926e-01, -5.4625e-01,\n",
       "          -6.2807e-02,  1.3378e-01, -8.9398e-02, -5.9889e-01, -7.7928e-01,\n",
       "          -6.2541e-01, -8.3325e-01,  1.6513e-01,  6.9423e-01],\n",
       "         [-1.5376e-01,  9.3470e-01,  7.2960e-01,  7.7173e-01,  5.3097e-03,\n",
       "           3.5160e-02, -6.0933e-01,  1.4624e-01, -6.0539e-01, -8.6247e-01,\n",
       "          -7.9082e-01,  9.3171e-01, -7.9294e-01, -1.7569e-01,  8.3250e-01,\n",
       "           4.9641e-01, -8.1500e-01,  8.4740e-01,  1.3380e-01,  8.3853e-01,\n",
       "           5.5895e-01, -7.7157e-01, -1.6861e-01,  7.1621e-01, -9.4006e-01,\n",
       "          -8.2249e-01, -6.8201e-01,  4.4870e-01,  2.6801e-01,  2.9309e-01,\n",
       "          -5.6308e-01,  7.2561e-01,  8.5821e-01,  7.0731e-01, -3.7498e-01,\n",
       "          -3.4698e-01, -2.6134e-01, -4.4746e-01,  3.7775e-01, -2.5936e-01,\n",
       "          -6.9553e-01,  9.5139e-01, -5.4808e-01,  7.5616e-01, -1.8204e-02,\n",
       "          -7.4003e-02,  8.1223e-02,  5.0659e-01, -1.3855e-01,  5.7383e-01,\n",
       "          -3.3890e-01, -6.4166e-01,  8.6738e-01,  4.1952e-01, -8.1499e-01,\n",
       "          -3.8454e-01, -8.5364e-03,  5.8875e-01, -9.1759e-01, -1.3796e-01,\n",
       "          -7.8441e-01,  7.9223e-01,  9.0489e-01, -7.3606e-01]]],\n",
       "       grad_fn=<TransposeBackward1>)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1538,  0.9347,  0.7296,  0.7717,  0.0053,  0.0352, -0.6093,\n",
       "           0.1462, -0.6054, -0.8625, -0.7908,  0.9317, -0.7929, -0.1757,\n",
       "           0.8325,  0.4964, -0.8150,  0.8474,  0.1338,  0.8385,  0.5589,\n",
       "          -0.7716, -0.1686,  0.7162, -0.9401, -0.8225, -0.6820,  0.4487,\n",
       "           0.2680,  0.2931, -0.5631,  0.7256,  0.8582,  0.7073, -0.3750,\n",
       "          -0.3470, -0.2613, -0.4475,  0.3777, -0.2594, -0.6955,  0.9514,\n",
       "          -0.5481,  0.7562, -0.0182, -0.0740,  0.0812,  0.5066, -0.1386,\n",
       "           0.5738, -0.3389, -0.6417,  0.8674,  0.4195, -0.8150, -0.3845,\n",
       "          -0.0085,  0.5888, -0.9176, -0.1380, -0.7844,  0.7922,  0.9049,\n",
       "          -0.7361]]], grad_fn=<StackBackward>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Voila! Both the intermediate hidden states and the final output is the exact same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Let's do the same comparision with stacked (2 layer) RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_rnn = nn.RNN(input_vector_size, state_size, batch_first=True, num_layers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(128, 64, num_layers=2, batch_first=True)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Custom RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer 1\n",
    "Wi0 = stacked_rnn.weight_ih_l0\n",
    "bi0 = stacked_rnn.bias_ih_l0\n",
    "Wh0 = stacked_rnn.weight_hh_l0\n",
    "bh0 = stacked_rnn.bias_hh_l0\n",
    "\n",
    "# layer 2\n",
    "Wi1 = stacked_rnn.weight_ih_l1\n",
    "bi1 = stacked_rnn.bias_ih_l1\n",
    "Wh1 = stacked_rnn.weight_hh_l1\n",
    "bh1 = stacked_rnn.bias_hh_l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 128]) torch.Size([64]) torch.Size([64, 64]) torch.Size([64])\n",
      "torch.Size([64, 64]) torch.Size([64]) torch.Size([64, 64]) torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "print(Wi0.shape, bi0.shape, Wh0.shape, bh0.shape)\n",
    "print(Wi1.shape, bi1.shape, Wh1.shape, bh1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "h0 = torch.zeros(state_size)\n",
    "h1 = torch.zeros(state_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let's only compare the final output state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in sequence_tensor:\n",
    "    output_t0 = torch.tanh(torch.matmul(Wi0, x) + bi0 + torch.matmul(Wh0, h0) +  bh0)\n",
    "    h0 = output_t0\n",
    "    output_t1 = torch.tanh(torch.matmul(Wi1, output_t0) + bi1 + torch.matmul(Wh1, h1) +  bh1)\n",
    "    h1 = output_t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.3040, -0.7206,  0.4327, -0.3948,  0.7157, -0.8285,  0.1730,  0.3187,\n",
       "         0.8709, -0.9211, -0.1380,  0.5022,  0.6089,  0.8666, -0.8169,  0.3745,\n",
       "        -0.8762, -0.7781,  0.0329, -0.9537, -0.6276, -0.1819,  0.7403,  0.6341,\n",
       "         0.3235,  0.4908, -0.1424, -0.0802, -0.7671, -0.4847,  0.0573,  0.6085,\n",
       "         0.8735, -0.8274, -0.1856, -0.3978, -0.4590,  0.1027, -0.7695,  0.9689,\n",
       "        -0.0595,  0.7864, -0.8305, -0.7630, -0.6278,  0.2638,  0.8462, -0.5419,\n",
       "         0.6264, -0.6404,  0.9179, -0.3545, -0.8668, -0.6995, -0.2720, -0.1060,\n",
       "        -0.2285, -0.7063, -0.8376, -0.4023,  0.3161,  0.3398,  0.6905, -0.4741],\n",
       "       grad_fn=<TanhBackward>)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.6532, -0.2377,  0.5428, -0.1767, -0.0807, -0.3861, -0.3155, -0.7183,\n",
       "         0.7231, -0.0166,  0.5051, -0.1257, -0.5053,  0.1901, -0.3643, -0.0653,\n",
       "         0.0941, -0.3174, -0.2137, -0.5508, -0.2560, -0.3790,  0.3109, -0.0227,\n",
       "        -0.1050,  0.2032,  0.5722, -0.2836, -0.5619, -0.2735, -0.2176, -0.5239,\n",
       "         0.5023,  0.0414,  0.4168,  0.5851, -0.0250, -0.5020,  0.2633, -0.1788,\n",
       "         0.2757, -0.3850, -0.3017,  0.4231, -0.1724,  0.3517,  0.4816,  0.6312,\n",
       "        -0.1027,  0.1863, -0.1371,  0.3120,  0.1851, -0.5933,  0.2322,  0.1761,\n",
       "         0.4492,  0.0303, -0.3130, -0.1995,  0.3242, -0.3520,  0.0558, -0.0165],\n",
       "       grad_fn=<TanhBackward>)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_t1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pytorch's stacked RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing two hidden states: one for each layer\n",
    "h = torch.zeros((2, 1, state_size)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 64])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_tensor_batch = sequence_tensor.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = stacked_rnn(sequence_tensor_batch, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3040, -0.7206,  0.4327, -0.3948,  0.7157, -0.8285,  0.1730,\n",
       "           0.3187,  0.8709, -0.9211, -0.1380,  0.5022,  0.6089,  0.8666,\n",
       "          -0.8169,  0.3745, -0.8762, -0.7781,  0.0329, -0.9537, -0.6276,\n",
       "          -0.1819,  0.7403,  0.6341,  0.3235,  0.4908, -0.1424, -0.0802,\n",
       "          -0.7671, -0.4847,  0.0573,  0.6085,  0.8735, -0.8274, -0.1856,\n",
       "          -0.3978, -0.4590,  0.1027, -0.7695,  0.9689, -0.0595,  0.7864,\n",
       "          -0.8305, -0.7630, -0.6278,  0.2638,  0.8462, -0.5419,  0.6264,\n",
       "          -0.6404,  0.9179, -0.3545, -0.8668, -0.6995, -0.2720, -0.1060,\n",
       "          -0.2285, -0.7063, -0.8376, -0.4023,  0.3161,  0.3398,  0.6905,\n",
       "          -0.4741]],\n",
       "\n",
       "        [[-0.6532, -0.2377,  0.5428, -0.1767, -0.0807, -0.3861, -0.3155,\n",
       "          -0.7183,  0.7231, -0.0166,  0.5051, -0.1257, -0.5053,  0.1901,\n",
       "          -0.3643, -0.0653,  0.0941, -0.3174, -0.2137, -0.5508, -0.2560,\n",
       "          -0.3790,  0.3109, -0.0227, -0.1050,  0.2032,  0.5722, -0.2836,\n",
       "          -0.5619, -0.2735, -0.2176, -0.5239,  0.5023,  0.0414,  0.4168,\n",
       "           0.5851, -0.0250, -0.5020,  0.2633, -0.1788,  0.2757, -0.3850,\n",
       "          -0.3017,  0.4231, -0.1724,  0.3517,  0.4816,  0.6312, -0.1027,\n",
       "           0.1863, -0.1371,  0.3120,  0.1851, -0.5933,  0.2322,  0.1761,\n",
       "           0.4492,  0.0303, -0.3130, -0.1995,  0.3242, -0.3520,  0.0558,\n",
       "          -0.0165]]], grad_fn=<StackBackward>)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Voila! Both are the same again!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidirectional RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's do the same with a bidirectional rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pytorch's bidirectional RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "bidirectional_rnn = nn.RNN(input_vector_size, state_size, batch_first=True, bidirectional=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = torch.zeros((2, 1, state_size)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_tensor_batch = sequence_tensor.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = bidirectional_rnn(sequence_tensor_batch, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.6273, -0.1384,  0.0183,  0.2608,  0.5468,  0.4579, -0.6911,\n",
       "           0.3738, -0.3968, -0.7581,  0.8475, -0.5910,  0.3460,  0.8407,\n",
       "           0.9438,  0.4376, -0.9470, -0.6010,  0.3528, -0.3232, -0.7826,\n",
       "           0.1393, -0.7454, -0.0791, -0.0145,  0.5617,  0.6644, -0.3221,\n",
       "          -0.0165,  0.1839, -0.8301, -0.7987, -0.7383,  0.3258,  0.4239,\n",
       "          -0.8353,  0.5910, -0.5097,  0.1005, -0.4422,  0.7388,  0.0368,\n",
       "          -0.0994,  0.0161,  0.3081, -0.8939,  0.9890,  0.9477,  0.5823,\n",
       "           0.4370, -0.8723,  0.2667, -0.9634, -0.7353, -0.3673, -0.2008,\n",
       "           0.2295,  0.4515, -0.1674,  0.3181, -0.8245,  0.8870, -0.7443,\n",
       "           0.6709]],\n",
       "\n",
       "        [[ 0.4360,  0.5481, -0.2999,  0.3920, -0.5371,  0.5163,  0.5920,\n",
       "          -0.3078,  0.4497, -0.3756,  0.0920, -0.5040,  0.8217, -0.9783,\n",
       "           0.5015,  0.5806,  0.9161, -0.3670, -0.7906,  0.4319,  0.4613,\n",
       "          -0.4099, -0.1022,  0.2499,  0.7750,  0.4010, -0.9704,  0.1389,\n",
       "           0.3025, -0.5125,  0.0811,  0.3388, -0.3535,  0.4343, -0.8805,\n",
       "           0.9426,  0.5217,  0.8891, -0.6822,  0.7020,  0.2881,  0.6581,\n",
       "           0.7853, -0.0365,  0.0541,  0.5831,  0.0103, -0.1674, -0.6261,\n",
       "          -0.0289, -0.5388, -0.9817,  0.8637, -0.8958, -0.3379, -0.4929,\n",
       "           0.7838, -0.5963, -0.5520,  0.0209, -0.7612, -0.4005, -0.4104,\n",
       "           0.7948]]], grad_fn=<StackBackward>)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Custom Bidirectional RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward\n",
    "Wif = bidirectional_rnn.weight_ih_l0\n",
    "bif = bidirectional_rnn.bias_ih_l0\n",
    "Whf = bidirectional_rnn.weight_hh_l0\n",
    "bhf = bidirectional_rnn.bias_hh_l0\n",
    "\n",
    "# layer 2\n",
    "Wib = bidirectional_rnn.weight_ih_l0_reverse\n",
    "bib = bidirectional_rnn.bias_ih_l0_reverse\n",
    "Whb = bidirectional_rnn.weight_hh_l0_reverse\n",
    "bhb = bidirectional_rnn.bias_hh_l0_reverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 128]) torch.Size([64]) torch.Size([64, 64]) torch.Size([64])\n",
      "torch.Size([64, 128]) torch.Size([64]) torch.Size([64, 64]) torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "print(Wif.shape, bif.shape, Whf.shape, bhf.shape)\n",
    "print(Wib.shape, bib.shape, Whb.shape, bhb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf = torch.zeros(state_size)\n",
    "hb = torch.zeros(state_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(sequence_tensor)\n",
    "for i in range(n):\n",
    "    xf = sequence_tensor[i]\n",
    "    xb = sequence_tensor[n-i-1]\n",
    "    output_f = torch.tanh(torch.matmul(Wif, xf) + bif + torch.matmul(Whf, hf) +  bhf)\n",
    "    hf = output_f\n",
    "    output_b = torch.tanh(torch.matmul(Wib, xb) + bib + torch.matmul(Whb, hb) +  bhb)\n",
    "    hb = output_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6273, -0.1384,  0.0183,  0.2608,  0.5468,  0.4579, -0.6911,  0.3738,\n",
       "         -0.3968, -0.7581,  0.8475, -0.5910,  0.3460,  0.8407,  0.9438,  0.4376,\n",
       "         -0.9470, -0.6010,  0.3528, -0.3232, -0.7826,  0.1393, -0.7454, -0.0791,\n",
       "         -0.0145,  0.5617,  0.6644, -0.3221, -0.0165,  0.1839, -0.8301, -0.7987,\n",
       "         -0.7383,  0.3258,  0.4239, -0.8353,  0.5910, -0.5097,  0.1005, -0.4422,\n",
       "          0.7388,  0.0368, -0.0994,  0.0161,  0.3081, -0.8939,  0.9890,  0.9477,\n",
       "          0.5823,  0.4370, -0.8723,  0.2667, -0.9634, -0.7353, -0.3673, -0.2008,\n",
       "          0.2295,  0.4515, -0.1674,  0.3181, -0.8245,  0.8870, -0.7443,  0.6709],\n",
       "        [ 0.4360,  0.5481, -0.2999,  0.3920, -0.5371,  0.5163,  0.5920, -0.3078,\n",
       "          0.4497, -0.3756,  0.0920, -0.5040,  0.8217, -0.9783,  0.5015,  0.5806,\n",
       "          0.9161, -0.3670, -0.7906,  0.4319,  0.4613, -0.4099, -0.1022,  0.2499,\n",
       "          0.7750,  0.4010, -0.9704,  0.1389,  0.3025, -0.5125,  0.0811,  0.3388,\n",
       "         -0.3535,  0.4343, -0.8805,  0.9426,  0.5217,  0.8891, -0.6822,  0.7020,\n",
       "          0.2881,  0.6581,  0.7853, -0.0365,  0.0541,  0.5831,  0.0103, -0.1674,\n",
       "         -0.6261, -0.0289, -0.5388, -0.9817,  0.8637, -0.8958, -0.3379, -0.4929,\n",
       "          0.7838, -0.5963, -0.5520,  0.0209, -0.7612, -0.4005, -0.4104,  0.7948]],\n",
       "       grad_fn=<StackBackward>)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack((output_f, output_b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Same Answer Again! yayy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conv1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = nn.Embedding(100, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = embedding(torch.tensor([2, 34, 65, 53, 22, 76, 98, 29]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = len(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_batch = sequences.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 128])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1D = nn.Conv1d(sequence_length, 16, kernel_size=5, padding=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 8, 5])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1D.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = conv1D(sequences_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 128])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
