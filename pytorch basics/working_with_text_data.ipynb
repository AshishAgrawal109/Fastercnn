{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representing text as vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"the cat sat on the mat\", \"the dog ate my homework\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ultimately our input data - the data which feeds into the model is a sentence, \n",
    "and our aim here is to represent this sentence on a word level or a character level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### one-hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get vocabulary\n",
    "index = 0\n",
    "vocab = dict()\n",
    "for sentence in sentences:\n",
    "    for word in sentence.split(\" \"):\n",
    "        if vocab.get(word) == None:\n",
    "            vocab[word] = index\n",
    "            index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 0,\n",
       " 'cat': 1,\n",
       " 'sat': 2,\n",
       " 'on': 3,\n",
       " 'mat': 4,\n",
       " 'dog': 5,\n",
       " 'ate': 6,\n",
       " 'my': 7,\n",
       " 'homework': 8}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 1, 0, 0, 0, 0]])\n",
      "torch.Size([6, 9])\n",
      "\n",
      "tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 1]])\n",
      "torch.Size([5, 9])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentences:\n",
    "    sentence = sentence.split()\n",
    "    sentence_length = len(sentence)\n",
    "    indices = torch.tensor([vocab[word] for word in sentence], dtype=torch.int64)\n",
    "    indices.unsqueeze_(1)\n",
    "    sentence_matrix = torch.zeros(sentence_length, vector_size, dtype=torch.int64)\n",
    "    sentence_matrix.scatter_(1, indices, 1)\n",
    "    \n",
    "    print(sentence_matrix)\n",
    "    print(sentence_matrix.shape)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Batching the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "         [0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "         [0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
      "         [0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
      "         [1, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "\n",
      "        [[1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "         [0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
      "         [0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0, 1]]])\n",
      "torch.Size([2, 5, 9])\n"
     ]
    }
   ],
   "source": [
    "# generally all the sentences should be of same length to batch the data\n",
    "max_sentence_length = 5 # i.e 5 words per sentence at max\n",
    "result = torch.zeros((len(sentences), max_sentence_length, vector_size), # no. of sentences,\n",
    "                    dtype=torch.int64)                                   # no. of words in a sentence\n",
    "                                                                         # size of the word vector\n",
    "                                                                       \n",
    "        \n",
    "for i, sentence in enumerate(sentences):\n",
    "    sentence = sentence.split()\n",
    "    sentence = sentence[:max_sentence_length] # clipping the sentence to max_sentence_length\n",
    "    for j, word in enumerate(sentence):\n",
    "        index = vocab.get(word)\n",
    "        result[i, j, index] = 1\n",
    "        \n",
    "\n",
    "print(result)\n",
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Character level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]]])\n",
      "torch.Size([2, 50, 128])\n"
     ]
    }
   ],
   "source": [
    "max_sentence_length = 50 # i.e no more than 50 charcters per sentence\n",
    "vector_size = 128 # ascii characters\n",
    "result = torch.zeros((len(sentences), max_sentence_length, vector_size), # no. of sentences,\n",
    "                     dtype=torch.int64)                                  # no. of characters in each sentence\n",
    "                                                                         # size of each character vector\n",
    "    \n",
    "for i, sentence in enumerate(sentences):\n",
    "    for j, character in enumerate(sentence):\n",
    "        if character not in string.printable:\n",
    "            continue\n",
    "        index = ord(character)\n",
    "        result[i, j, index] = 1\n",
    "        \n",
    "print(result)\n",
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Word Level one hot hashing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we don't create the vocabulary in the beginning.\n",
    "We run every word through a hash function and map it to a number between 1 and dimensionality.\n",
    "but the problem here is that every word will not have a unique hash value which might confuse the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]]])\n",
      "torch.Size([2, 10, 1000])\n"
     ]
    }
   ],
   "source": [
    "max_sentence_length = 10 # maximum of 10 words per sentence\n",
    "dimensionality = 1000 # size of word vector\n",
    "\n",
    "result = torch.zeros((len(sentences), max_sentence_length, dimensionality), # no. of sentences,\n",
    "                    dtype=torch.int64)                                      # no. of words in a sentence\n",
    "                                                                            # size of the word vector\n",
    "                                                                       \n",
    "        \n",
    "for i, sentence in enumerate(sentences):\n",
    "    sentence = sentence.split()\n",
    "    sentence = sentence[:max_sentence_length] # clipping the sentence to max_sentence_length\n",
    "    for j, word in enumerate(sentence):\n",
    "        index = abs(hash(word)) % dimensionality\n",
    "        result[i, j, index] = 1\n",
    "        \n",
    "print(result)\n",
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Embedding layer is just like a normal linear layer whose weights are updated during training,\n",
    "except that the weights represent word vectors and can be accessed via index of the word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = nn.Embedding(10, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 128])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get vector of word whose index is 1\n",
    "embedding_layer(torch.tensor([1])).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### N-Gram language modeling"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Here, given a context or history of two previous words, we try to predict the next word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_SIZE = 2\n",
    "EMBEDDING_DIM = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use Shakespeare Sonnet 2\n",
    "test_sentence = \"\"\"When forty winters shall besiege thy brow,\n",
    "And dig deep trenches in thy beauty's field,\n",
    "Thy youth's proud livery so gazed on now,\n",
    "Will be a totter'd weed of small worth held:\n",
    "Then being asked, where all thy beauty lies,\n",
    "Where all the treasure of thy lusty days;\n",
    "To say, within thine own deep sunken eyes,\n",
    "Were an all-eating shame, and thriftless praise.\n",
    "How much more praise deserv'd thy beauty's use,\n",
    "If thou couldst answer 'This fair child of mine\n",
    "Shall sum my count, and make my old excuse,'\n",
    "Proving his beauty by succession thine!\n",
    "This were to be new made when thou art old,\n",
    "And see thy blood warm when thou feel'st it cold.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = WordPunctTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = tokenizer.tokenize(test_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams = [([words[i], words[i+1]], words[i+2]) for i in range(len(words)-2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['When', 'forty'], 'winters'),\n",
       " (['forty', 'winters'], 'shall'),\n",
       " (['winters', 'shall'], 'besiege'),\n",
       " (['shall', 'besiege'], 'thy'),\n",
       " (['besiege', 'thy'], 'brow'),\n",
       " (['thy', 'brow'], ',')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigrams[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = {word: i for i, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Building the model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "If you think about it, it's just a category prediction problem where the number of categories = vocab size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramLanguageModeler(nn.Module):\n",
    "    def __init__(self, CONTEXT_SIZE, EMBEDDING_DIM, VOCAB_SIZE):\n",
    "        super().__init__()\n",
    "        self.CONTEXT_SIZE = CONTEXT_SIZE\n",
    "        self.EMBEDDING_DIM = EMBEDDING_DIM\n",
    "        self.embeddings = nn.Embedding(VOCAB_SIZE, EMBEDDING_DIM)\n",
    "        self.linear1 = nn.Linear(CONTEXT_SIZE*EMBEDDING_DIM, 128)\n",
    "        self.linear2 = nn.Linear(128, VOCAB_SIZE)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        if inputs.shape[0] != self.CONTEXT_SIZE:\n",
    "            raise ValueError(f\"The context size should be of size {self.CONTEXT_SIZE}\")\n",
    "        word_vectors = self.embeddings(inputs)\n",
    "        # we flatten the inputs before passing onto the linear layer\n",
    "        word_vectors = word_vectors.view(-1, self.CONTEXT_SIZE * self.EMBEDDING_DIM)\n",
    "        out = F.relu(self.linear1(word_vectors))\n",
    "        logits = self.linear2(out)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NGramLanguageModeler(CONTEXT_SIZE, EMBEDDING_DIM, VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_list = []\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50/50 [00:03<00:00, 13.96it/s]\n"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(n_epochs)):\n",
    "    loss_per_epoch = 0\n",
    "    for context, target in trigrams:\n",
    "        context_indices = torch.tensor([word2idx[word] for word in context], dtype=torch.int64)\n",
    "        out = model(context_indices)\n",
    "        loss = loss_fn(out, torch.tensor([word2idx[target]], dtype=torch.int64))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_per_epoch += loss.item()\n",
    "    loss_list.append(loss_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x17a2341e820>]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(list(range(n_epochs)), loss_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Batching and training - !! Get back to it later !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(list(zip(*trigrams)), batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_list = []\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('all', 'couldst', ',', 'were', 'thou', 'and', 'held', 'new', \"'\", 'Shall', 'praise', 'now', 'And', \"'\", 'mine', 'shall'), ('thy', 'answer', 'And', 'to', 'couldst', 'make', ':', 'made', 'd', 'sum', '.', ',', 'see', 'd', 'Shall', 'besiege')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-101-d2a9ad3aa57e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcontext_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_batch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontext_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtarget_batch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCONTEXT_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(n_epochs)):\n",
    "    loss_per_epoch = 0\n",
    "    for data in dataloader:\n",
    "        for target_batch, context_batch in data:\n",
    "            batch_size = target_batch.shape[0]\n",
    "            inputs = torch.zeros((batch_size, CONTEXT_SIZE), dtype=torch.int64)\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                for j, word in enumerate(context_batch[i]):\n",
    "                    inputs[i, j] = word2idx[word]\n",
    "\n",
    "            targets = torch.zeros((batch_size, 1), dtype=torch.int64)\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                word = target_batch[i][0]\n",
    "                targets[i, 0] = word2idx[word]\n",
    "\n",
    "            out = model(inputs)\n",
    "            loss = loss_fn(out, targets)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        loss_per_epoch += loss.item()\n",
    "    loss_list.append(loss_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1],\n",
       "        [1, 1]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack((torch.tensor([1, 1], dtype=torch.int64), torch.tensor([1, 1], dtype=torch.int64)), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
