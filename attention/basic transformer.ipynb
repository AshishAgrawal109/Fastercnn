{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea7dac6f",
   "metadata": {},
   "source": [
    "## Basic Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "297db42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0f7fea",
   "metadata": {},
   "source": [
    "In this exercise, we give a network an input sequence of characters (e.g., **aabbccdd**), the network is trained to produce the same sequence in reverse order (**ddccbbaa**)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fae86b9",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed6a70f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataset of 500 examples using a small vocabulary\n",
    "# we will try training on sequences of length 10 and testing on sequences of length 15\n",
    "# this setup tests whether the model has actually learned an algorithm to reverse its input\n",
    "vocab = {'a': 0, 'b': 1, 'c':2, 'd':3, 'e':4, '<sos>':5, '<eos>':6}\n",
    "idx_to_w = dict((v, k) for (k,v) in vocab.items())\n",
    "train_seq_len = 10\n",
    "num_train_examples = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e62d133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate toy data\n",
    "train_inputs = torch.LongTensor(num_train_examples, train_seq_len).random_(0\n",
    ", len(vocab)-2) # random sequences\n",
    "inv_idx = torch.arange(train_seq_len-1, -1, -1).long()\n",
    "train_outputs = train_inputs[:, inv_idx] # outputs are just the reverse of the input\n",
    "sos_vec = torch.LongTensor(num_train_examples, 1)\n",
    "sos_vec[:] = vocab['<sos>']\n",
    "eos_vec = torch.LongTensor(num_train_examples, 1)\n",
    "eos_vec[:] = vocab['<eos>']\n",
    "train_encoder_input = torch.cat((train_inputs, eos_vec), 1)\n",
    "train_decoder_input = torch.cat((sos_vec, train_outputs), 1)\n",
    "train_targets = torch.cat((train_outputs, eos_vec), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af8839c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder input : e c a a e e b e a b <eos>\n",
      "decoder input: <sos> b a e b e e a a c e\n",
      "decoder target: b a e b e e a a c e <eos>\n"
     ]
    }
   ],
   "source": [
    "print('encoder input :', ' '.join([idx_to_w[w] for w in train_encoder_input[0].numpy()]))\n",
    "print('decoder input:', ' '.join([idx_to_w[w] for w in train_decoder_input[0].numpy()]))\n",
    "print('decoder target:', ' '.join([idx_to_w[w] for w in train_targets[0].numpy()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a86afc4",
   "metadata": {},
   "source": [
    "### Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9bbef16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(precision=2, sci_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "615a81ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class for our vanilla seq2seq\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, char_dim, hidden_size, vocab_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.char_dim = char_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        # character embeddings\n",
    "        self.char_embeds = nn.Embedding(vocab_size, char_dim)\n",
    "        \n",
    "        # position embeddings\n",
    "        self.pos_embeds = nn.Embedding(15, char_dim) # add these to enc/dec\n",
    "        \n",
    "        # decoder attention\n",
    "        self.query = nn.Linear(char_dim, hidden_size)\n",
    "        self.key = nn.Linear(char_dim, hidden_size)\n",
    "        self.value = nn.Linear(char_dim, hidden_size)\n",
    "        \n",
    "        # output layer (softmax will be applied after this)\n",
    "        self.cls = nn.Linear(hidden_size, vocab_size)\n",
    "    \n",
    "    # a vectorized way of computing self attention for all queries efficiently\n",
    "    def smart_unmasked_attn(self, qs, ks, vs):\n",
    "        # here, queries are decoder states, keys and values are encoder representations\n",
    "        scores = qs @ ks.t() # get all dot products at once, N x N\n",
    "        scores = F.softmax(scores, dim=1)\n",
    "        return scores @ vs # N x hidden_size  \n",
    "    \n",
    "    # a vectorized way of computing **target-side self-attention**\n",
    "    # we need to implement some masking to avoid cheating!\n",
    "    def smart_masked_attn(self, qs, ks, vs):\n",
    "        max_len = qs.size(0)\n",
    "        mask = torch.tril(torch.ones(max_len, max_len))\n",
    "        scores = qs @ ks.t() # get all UNMASKED dot products at once, max_len X max_len\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        scores = F.softmax(scores, dim=1)\n",
    "        return scores @ vs\n",
    "\n",
    "    \n",
    "    def forward(self, inputs, decoder_inputs):\n",
    "        \n",
    "        batch_size, max_len = inputs.size()\n",
    "\n",
    "        positions = torch.arange(0, inputs.size(1))\n",
    "        pos_embeds = self.pos_embeds(positions)\n",
    "        \n",
    "        # we'll just consider this the output of our encoder\n",
    "        # of course in a real transformer this would be computed\n",
    "        # through multiple self attention blocks\n",
    "        e_embeds = self.char_embeds(inputs).squeeze(0)\n",
    "        e_embeds = e_embeds + pos_embeds\n",
    "        e_keys = self.key(e_embeds)\n",
    "        e_values = self.value(e_embeds)\n",
    "        \n",
    "        # we'll use the same weights to project decoder embeddings to q,k,v\n",
    "        d_embeds = self.char_embeds(decoder_inputs).squeeze(0)\n",
    "        d_embeds = d_embeds + pos_embeds\n",
    "        d_queries = self.query(d_embeds)\n",
    "        d_keys = self.key(d_embeds)\n",
    "        d_values = self.value(d_embeds)\n",
    "\n",
    "        # compute target side self attention\n",
    "        fast_decoder_states = self.smart_masked_attn(d_queries, d_keys, d_values)\n",
    "\n",
    "        # source attention, queries come from decoder, keys/values from encoder\n",
    "        source_attn = self.smart_unmasked_attn(fast_decoder_states, e_keys, e_values)\n",
    "        \n",
    "        # combine decoder self attention w/ source attention\n",
    "        source_attn = source_attn + fast_decoder_states\n",
    "\n",
    "        # now do prediction over decoder states (reshape to 2d first)\n",
    "        source_attn = source_attn.transpose(0, 1).contiguous().view(-1, self.hidden_size)\n",
    "        decoder_preds = self.cls(source_attn)\n",
    "        decoder_preds = F.log_softmax(decoder_preds, dim=1)\n",
    "\n",
    "        return decoder_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc408ff6",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce8cb377",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(net):\n",
    "\n",
    "    # set some hyperparameters for training the network\n",
    "    idx_to_w = dict((v,k) for (k,v) in vocab.items())\n",
    "    loss_fn = nn.NLLLoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
    "    num_epochs = 10\n",
    "    \n",
    "    # okay, let's train the network!\n",
    "    for ep in range(num_epochs):\n",
    "        ep_loss = 0.\n",
    "\n",
    "        for start in range(0, len(train_inputs)):\n",
    "            e_in_batch = train_encoder_input[start].unsqueeze(0)\n",
    "            d_in_batch = train_decoder_input[start].unsqueeze(0)\n",
    "            d_targ_batch = train_targets[start].unsqueeze(0)\n",
    "            \n",
    "            preds = net(e_in_batch, d_in_batch)\n",
    "            batch_loss = loss_fn(preds, d_targ_batch.view(-1))\n",
    "            ep_loss += batch_loss\n",
    "\n",
    "            # compute gradients\n",
    "            optimizer.zero_grad() # reset the gradients from the last batch\n",
    "            batch_loss.backward() # does backprop!!!\n",
    "            optimizer.step() # updates parameters using gradients\n",
    "\n",
    "        print('epoch %d, loss %f\\n' % (ep, ep_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2160890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 9.770307\n",
      "\n",
      "epoch 1, loss 5.881337\n",
      "\n",
      "epoch 2, loss 4.329915\n",
      "\n",
      "epoch 3, loss 3.894290\n",
      "\n",
      "epoch 4, loss 2.730859\n",
      "\n",
      "epoch 5, loss 1.853093\n",
      "\n",
      "epoch 6, loss 1.146887\n",
      "\n",
      "epoch 7, loss 0.653309\n",
      "\n",
      "epoch 8, loss 0.368380\n",
      "\n",
      "epoch 9, loss 0.222095\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# build the network\n",
    "net = Seq2Seq(32, 64, len(vocab))\n",
    "training_loop(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cf0ce7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
